{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Kubernetes Workshop for Golog AG This workshop is a collection of materials that can be used to teach Kubernetes to beginners. The goal is to provide a quick overview of the most important concepts and tools. It's also a good starting point for self-study. The infrastructure is provided by Stepping Stone AG . Also there are multiple part of this workshop aligned with the infratructure provided by Stepping Stone AG . Goals Learn the basics of Kubernetes Learn the basics of Helm Get an overview of the Stepping Stone infrastructure Get an outlook of GitOps principles within the Stepping Stone infrastructure Prerequisites A computer with a web browser and Terminal (Linux, macOS, Windows) Local admin rights","title":"Home"},{"location":"#kubernetes-workshop-for-golog-ag","text":"This workshop is a collection of materials that can be used to teach Kubernetes to beginners. The goal is to provide a quick overview of the most important concepts and tools. It's also a good starting point for self-study. The infrastructure is provided by Stepping Stone AG . Also there are multiple part of this workshop aligned with the infratructure provided by Stepping Stone AG .","title":"Kubernetes Workshop for Golog AG"},{"location":"#goals","text":"Learn the basics of Kubernetes Learn the basics of Helm Get an overview of the Stepping Stone infrastructure Get an outlook of GitOps principles within the Stepping Stone infrastructure","title":" Goals"},{"location":"#prerequisites","text":"A computer with a web browser and Terminal (Linux, macOS, Windows) Local admin rights","title":" Prerequisites"},{"location":"gitops/","text":"GitOps What is GitOps? GitOps is a way to do Kubernetes application delivery. It works by using Git as a single source of truth for Kubernetes resources and everything else. With GitOps, deployments to Kubernetes are just a git push . This workflow allows for much easier auditing of configurations, as all changes are in source control. Why GitOps? The main benefits of GitOps are: Easy auditing : All changes are in source control. Easy rollbacks : Rollbacks are just a git revert . Easy onboarding : Onboarding new team members is as simple as giving them access to the repository. Easy CI/CD : CI/CD pipelines can be set up to automatically deploy changes to the cluster. Easy to scale : GitOps scales well with the number of applications and teams. Easy to understand : GitOps is a simple workflow that is easy to understand and explain. Easy to automate : GitOps can be fully automated with tools like Flux . Flux Flux is a tool for keeping Kubernetes clusters in sync with sources of configuration (like Git repositories). Flux works by using a Git repository as a single source of truth for Kubernetes resources. Flux monitors the repository and applies any changes to the cluster automatically. Argo CD Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. Argo CD works by using a Git repository as a single source of truth for Kubernetes resources. Argo CD monitors the repository and applies any changes to the cluster automatically. GitLab GitLab is a complete DevOps platform, delivered as a single application. It includes built-in support for GitOps workflows. You can use Gitlab CI/CD to deploy your applications to Kubernetes or to validate your Kubernetes manifests. GitLab CI/CD GitLab CI/CD is a built-in continuous integration and continuous delivery tool that can be used to deploy your applications to Kubernetes or to validate your Kubernetes manifests. GitLab Kubernetes Agent GitLab Kubernetes Agent is a tool that can help you deploy your applications to Kubernetes. It works by using a Git repository as a single source of truth for Kubernetes resources. The agent monitors the repository and applies any changes to the cluster automatically. It is also very useful if you want to run a CI/CD pipeline against your Kubernetes cluster. The agent handels the authentication for you, so you don't have to worry about it. Examples gitlab-ci.yml : stages : - deploy deploy : stage : deploy image : gitlab/gitlab-runner:latest script : - kubectl apply -f kubernetes/manifests only : - master kubernetes/manifests/deployment.yaml : apiVersion : apps/v1 kind : Deployment metadata : name : my-app spec : replicas : 1 selector : matchLabels : app : my-app template : metadata : labels : app : my-app spec : containers : - name : my-app image : my-app:latest You can also combine the Gitlab CI with Argo CD or Flux. This way you can use the Gitlab CI to validate your Kubernetes manifests and then use Argo CD or Flux to deploy them. This is a very powerful combination.","title":"GitOps"},{"location":"gitops/#gitops","text":"","title":"GitOps"},{"location":"gitops/#what-is-gitops","text":"GitOps is a way to do Kubernetes application delivery. It works by using Git as a single source of truth for Kubernetes resources and everything else. With GitOps, deployments to Kubernetes are just a git push . This workflow allows for much easier auditing of configurations, as all changes are in source control.","title":"What is GitOps?"},{"location":"gitops/#why-gitops","text":"The main benefits of GitOps are: Easy auditing : All changes are in source control. Easy rollbacks : Rollbacks are just a git revert . Easy onboarding : Onboarding new team members is as simple as giving them access to the repository. Easy CI/CD : CI/CD pipelines can be set up to automatically deploy changes to the cluster. Easy to scale : GitOps scales well with the number of applications and teams. Easy to understand : GitOps is a simple workflow that is easy to understand and explain. Easy to automate : GitOps can be fully automated with tools like Flux .","title":"Why GitOps?"},{"location":"gitops/#flux","text":"Flux is a tool for keeping Kubernetes clusters in sync with sources of configuration (like Git repositories). Flux works by using a Git repository as a single source of truth for Kubernetes resources. Flux monitors the repository and applies any changes to the cluster automatically.","title":"Flux"},{"location":"gitops/#argo-cd","text":"Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. Argo CD works by using a Git repository as a single source of truth for Kubernetes resources. Argo CD monitors the repository and applies any changes to the cluster automatically.","title":"Argo CD"},{"location":"gitops/#gitlab","text":"GitLab is a complete DevOps platform, delivered as a single application. It includes built-in support for GitOps workflows. You can use Gitlab CI/CD to deploy your applications to Kubernetes or to validate your Kubernetes manifests.","title":"GitLab"},{"location":"gitops/#gitlab-cicd","text":"GitLab CI/CD is a built-in continuous integration and continuous delivery tool that can be used to deploy your applications to Kubernetes or to validate your Kubernetes manifests.","title":"GitLab CI/CD"},{"location":"gitops/#gitlab-kubernetes-agent","text":"GitLab Kubernetes Agent is a tool that can help you deploy your applications to Kubernetes. It works by using a Git repository as a single source of truth for Kubernetes resources. The agent monitors the repository and applies any changes to the cluster automatically. It is also very useful if you want to run a CI/CD pipeline against your Kubernetes cluster. The agent handels the authentication for you, so you don't have to worry about it.","title":"GitLab Kubernetes Agent"},{"location":"gitops/#examples","text":"gitlab-ci.yml : stages : - deploy deploy : stage : deploy image : gitlab/gitlab-runner:latest script : - kubectl apply -f kubernetes/manifests only : - master kubernetes/manifests/deployment.yaml : apiVersion : apps/v1 kind : Deployment metadata : name : my-app spec : replicas : 1 selector : matchLabels : app : my-app template : metadata : labels : app : my-app spec : containers : - name : my-app image : my-app:latest You can also combine the Gitlab CI with Argo CD or Flux. This way you can use the Gitlab CI to validate your Kubernetes manifests and then use Argo CD or Flux to deploy them. This is a very powerful combination.","title":"Examples"},{"location":"kubernetes-basics/database-connection/","text":"Database Connection Environment Variables We are going to use some environment variables in this tutorial. Please make sure you have set them correctly. # check if the environment variables are set if not set them export NAMESPACE = <namespace> echo $NAMESPACE export URL = ${ NAMESPACE } .k8s.golog.ch echo $URL You've now created a Deployment, exposed it as a Service, and scaled it up. But how do you know if it's working? How do you know if your application is actually connecting to the database? In this module, you'll learn how to connect to your application and run some simple commands to verify that it's working. For this tutorial you'll have to take a look at the stepping stone postgresql documentation. Task 1 : Connect Test Webserver to Database We've already created a Deployment. Now we will create another Deployment with a simple webserver that connects to the database. Because the application needs credentials to connect to the database, we will use a Secret to store the credentials. Database Credentials If there is no access.yaml file at your home dir on the db server, create a new user and new database. Note the username and password of the new user. Create a file called db-secret.yaml with the following content (make sure to replace the placeholders with your own values): # get secrets from wiki apiVersion : v1 kind : Secret metadata : name : db-secret stringData : DB_USER : \"<username>\" DB_PASSWORD : \"<password>\" DB_NAME : \"<db name>\" DB_HOST : \"<db host>\" DB_PORT : \"5432\" DB_SSLMODE : \"<disable/enable>\" Note You can generate the yaml file using the following command: kubectl create secret generic db-secret \\ --from-literal = DB_USER = \"<username>\" \\ --from-literal = DB_PASSWORD = \"<password>\" \\ --from-literal = DB_NAME = \"<db name>\" \\ --from-literal = DB_HOST = \"<db host>\" \\ --from-literal = DB_PORT = \"5432\" \\ --from-literal = DB_SSLMODE = \"<disable/enable>\" \\ --dry-run = client -o yaml > db-secret.yaml You will then find the values base64 encoded in the yaml file. You can decode them using the following command: echo -n \"<base64 encoded value>\" | base64 -d Create a file called db-deployment.yaml with the following content: apiVersion : apps/v1 kind : Deployment metadata : labels : app : test-postgresql-webserver name : test-postgresql-webserver spec : replicas : 1 selector : matchLabels : app : test-postgresql-webserver template : metadata : labels : app : test-postgresql-webserver spec : containers : - image : ghcr.io/natrongmbh/kubernetes-workshop-golog-test-postgresql-webserver:latest name : test-postgresql-webserver resources : requests : cpu : 10m memory : 16Mi limits : cpu : 20m memory : 32Mi envFrom : - secretRef : name : db-secret Also create a file called db-service.yaml with the following content: apiVersion : v1 kind : Service metadata : name : test-postgresql-webserver spec : selector : app : test-postgresql-webserver ports : - protocol : TCP port : 8080 targetPort : 8080 type : NodePort Create a file called db-ingress.yaml with the following content: kubectl create --dry-run = client --namespace $NAMESPACE -o yaml -f - <<EOF >> db-ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: test-postgresql-webserver annotations: kubernetes.io/ingress.class: nginx kubernetes.io/tls-acme: \"true\" cert-manager.io/cluster-issuer: letsencrypt-prod nginx.ingress.kubernetes.io/add-base-url: \"true\" spec: tls: - hosts: - $URL secretName: ${NAMESPACE}-tls rules: - host: $URL http: paths: - path: / pathType: Prefix backend: service: name: test-postgresql-webserver port: number: 8080 EOF Finally, create the Deployment and Service: kubectl apply -f db-secret.yaml --namespace $NAMESPACE kubectl apply -f db-deployment.yaml --namespace $NAMESPACE kubectl apply -f db-service.yaml --namespace $NAMESPACE Before you create the Ingress, you need to delete the old Ingress: kubectl delete ingress <ingress name> --namespace $NAMESPACE Create the Ingress: kubectl apply -f db-ingress.yaml --namespace $NAMESPACE Check if the pod is running and also check the logs: kubectl get pods --namespace $NAMESPACE kubectl logs <pod name> --namespace $NAMESPACE","title":"Database Connection"},{"location":"kubernetes-basics/database-connection/#database-connection","text":"Environment Variables We are going to use some environment variables in this tutorial. Please make sure you have set them correctly. # check if the environment variables are set if not set them export NAMESPACE = <namespace> echo $NAMESPACE export URL = ${ NAMESPACE } .k8s.golog.ch echo $URL You've now created a Deployment, exposed it as a Service, and scaled it up. But how do you know if it's working? How do you know if your application is actually connecting to the database? In this module, you'll learn how to connect to your application and run some simple commands to verify that it's working. For this tutorial you'll have to take a look at the stepping stone postgresql documentation.","title":"Database Connection"},{"location":"kubernetes-basics/database-connection/#task-1-connect-test-webserver-to-database","text":"We've already created a Deployment. Now we will create another Deployment with a simple webserver that connects to the database. Because the application needs credentials to connect to the database, we will use a Secret to store the credentials. Database Credentials If there is no access.yaml file at your home dir on the db server, create a new user and new database. Note the username and password of the new user. Create a file called db-secret.yaml with the following content (make sure to replace the placeholders with your own values): # get secrets from wiki apiVersion : v1 kind : Secret metadata : name : db-secret stringData : DB_USER : \"<username>\" DB_PASSWORD : \"<password>\" DB_NAME : \"<db name>\" DB_HOST : \"<db host>\" DB_PORT : \"5432\" DB_SSLMODE : \"<disable/enable>\" Note You can generate the yaml file using the following command: kubectl create secret generic db-secret \\ --from-literal = DB_USER = \"<username>\" \\ --from-literal = DB_PASSWORD = \"<password>\" \\ --from-literal = DB_NAME = \"<db name>\" \\ --from-literal = DB_HOST = \"<db host>\" \\ --from-literal = DB_PORT = \"5432\" \\ --from-literal = DB_SSLMODE = \"<disable/enable>\" \\ --dry-run = client -o yaml > db-secret.yaml You will then find the values base64 encoded in the yaml file. You can decode them using the following command: echo -n \"<base64 encoded value>\" | base64 -d Create a file called db-deployment.yaml with the following content: apiVersion : apps/v1 kind : Deployment metadata : labels : app : test-postgresql-webserver name : test-postgresql-webserver spec : replicas : 1 selector : matchLabels : app : test-postgresql-webserver template : metadata : labels : app : test-postgresql-webserver spec : containers : - image : ghcr.io/natrongmbh/kubernetes-workshop-golog-test-postgresql-webserver:latest name : test-postgresql-webserver resources : requests : cpu : 10m memory : 16Mi limits : cpu : 20m memory : 32Mi envFrom : - secretRef : name : db-secret Also create a file called db-service.yaml with the following content: apiVersion : v1 kind : Service metadata : name : test-postgresql-webserver spec : selector : app : test-postgresql-webserver ports : - protocol : TCP port : 8080 targetPort : 8080 type : NodePort Create a file called db-ingress.yaml with the following content: kubectl create --dry-run = client --namespace $NAMESPACE -o yaml -f - <<EOF >> db-ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: test-postgresql-webserver annotations: kubernetes.io/ingress.class: nginx kubernetes.io/tls-acme: \"true\" cert-manager.io/cluster-issuer: letsencrypt-prod nginx.ingress.kubernetes.io/add-base-url: \"true\" spec: tls: - hosts: - $URL secretName: ${NAMESPACE}-tls rules: - host: $URL http: paths: - path: / pathType: Prefix backend: service: name: test-postgresql-webserver port: number: 8080 EOF Finally, create the Deployment and Service: kubectl apply -f db-secret.yaml --namespace $NAMESPACE kubectl apply -f db-deployment.yaml --namespace $NAMESPACE kubectl apply -f db-service.yaml --namespace $NAMESPACE Before you create the Ingress, you need to delete the old Ingress: kubectl delete ingress <ingress name> --namespace $NAMESPACE Create the Ingress: kubectl apply -f db-ingress.yaml --namespace $NAMESPACE Check if the pod is running and also check the logs: kubectl get pods --namespace $NAMESPACE kubectl logs <pod name> --namespace $NAMESPACE","title":" Task 1: Connect Test Webserver to Database"},{"location":"kubernetes-basics/deploying-containers/","text":"Deploying Containers Environment Variables We are going to use some environment variables in this tutorial. Please make sure you have set them correctly. # check if the environment variables are set if not set them export NAMESPACE = <namespace> echo $NAMESPACE In this tutorial, we are going to deploy our first container image and look at the concepts of Pods, Services, and Deployments. Task 1 : Start and stop a single Pod After we\u2019ve familiarized ourselves with the platform, we are going to have a look at deploying a pre-built container image or any other public container registry. First, we are going to directly start a new Pod. For this we have to define our Kubernetes Pod resource definition. Create a new file pod.yaml with the following content: apiVersion : v1 kind : Pod metadata : name : test-webserver spec : containers : - image : ghcr.io/natrongmbh/kubernetes-workshop-golog-test-webserver:latest imagePullPolicy : Always name : test-webserver resources : limits : cpu : 20m memory : 32Mi requests : cpu : 10m memory : 16Mi Now we can apply this with: kubectl apply -f pod.yaml --namespace $NAMESPACE The output should be: pod/test-webserver created Use kubectl get pods --namespace $NAMESPACE in order to show the running Pod: kubectl get pods --namespace $NAMESPACE Which gives you an output similar to this: NAME READY STATUS RESTARTS AGE test-webserver 1 /1 Running 0 2m Now we can delete the Pod with: kubectl delete pod test-webserver --namespace $NAMESPACE Task 2 : Create a Deployment In some use cases it can make sense to start a single Pod. But this has its downsides and is not really a common practice. Let\u2019s look at another concept which is tightly coupled with the Pod: the so-called Deployment . A Deployment ensures that a Pod is monitored and checks that the number of running Pods corresponds to the number of requested Pods. To create a new Deployment we first define our Deployment in a new file deployment.yaml with the following content: apiVersion : apps/v1 kind : Deployment metadata : labels : app : test-webserver name : test-webserver spec : replicas : 1 selector : matchLabels : app : test-webserver template : metadata : labels : app : test-webserver spec : containers : - image : ghcr.io/natrongmbh/kubernetes-workshop-golog-test-webserver:latest name : test-webserver resources : requests : cpu : 10m memory : 16Mi limits : cpu : 20m memory : 32Mi And with this we create our Deployment inside our already created namespace: kubectl apply -f deployment.yaml --namespace $NAMESPACE The output should be: deployment.apps/test-webserver created Kubernetes creates the defined and necessary resources, pulls the container image (in this case from ghcr.io) and deploys the Pod. Use the command kubectl get with the -w parameter in order to get the requested resources and afterward watch for changes. Note The kubectl get -w command will never end unless you terminate it with CTRL-c . kubectl get pods -w --namespace $NAMESPACE Note Instead of using the -w parameter you can also use the watch command which should be available on most Linux distributions: watch kubectl get pods --namespace $NAMESPACE This process can last for some time depending on your internet connection and if the image is already available locally. Note If you want to create your own container images and use them with Kubernetes, you definitely should have a look at these best practices and apply them. This image creation guide may be for OpenShift, however it also applies to Kubernetes and other container platforms. Creating Kubernetes resources There are two fundamentally different ways to create Kubernetes resources. You\u2019ve already seen one way: Writing the resource\u2019s definition in YAML (or JSON) and then applying it on the cluster using kubectl apply . The other variant is to use helper commands. These are more straightforward: You don\u2019t have to copy a YAML definition from somewhere else and then adapt it. However, the result is the same. The helper commands just simplify the process of creating the YAML definitions. As an example, let\u2019s look at creating above deployment, this time using a helper command instead. If you already created the Deployment using above YAML definition, you don\u2019t have to execute this command: kubectl create deployment test-webserver --image = ghcr.io/natrongmbh/kubernetes-workshop-golog-test-webserver:latest --namespace $NAMESPACE It\u2019s important to know that these helper commands exist. However, in a world where GitOps concepts have an ever-increasing presence, the idea is not to constantly create these resources with helper commands. Instead, we save the resources\u2019 YAML definitions in a Git repository and leave the creation and management of those resources to a tool. Task 3 : Viewing the created resources Display the created Deployment using the following command: kubectl get deployments --namespace $NAMESPACE A Deployment defines the following facts: Update strategy: How application updates should be executed and how the Pods are exchanged Containers Which image should be deployed Environment configuration for Pods ImagePullPolicy The number of Pods/Replicas that should be deployed By using the -o (or --output ) parameter we get a lot more information about the deployment itself. You can choose between YAML and JSON formatting by indicating -o yaml or -o json . In this training we are going to use YAML, but please feel free to replace yaml with json if you prefer. kubectl get deployments test-webserver -o yaml --namespace $NAMESPACE After the image has been pulled, Kubernetes deploys a Pod according to the Deployment: kubectl get pods --namespace $NAMESPACE which gives you an output similar to this: NAME READY STATUS RESTARTS AGE test-webserver-7f7b9b9b4-2j2xg 1 /1 Running 0 2m The Deployment defines that one replica should be deployed \u2014 which is running as we can see in the output. This Pod is not yet reachable from outside the cluster.","title":"Deploying Containers"},{"location":"kubernetes-basics/deploying-containers/#deploying-containers","text":"Environment Variables We are going to use some environment variables in this tutorial. Please make sure you have set them correctly. # check if the environment variables are set if not set them export NAMESPACE = <namespace> echo $NAMESPACE In this tutorial, we are going to deploy our first container image and look at the concepts of Pods, Services, and Deployments.","title":"Deploying Containers"},{"location":"kubernetes-basics/deploying-containers/#task-1-start-and-stop-a-single-pod","text":"After we\u2019ve familiarized ourselves with the platform, we are going to have a look at deploying a pre-built container image or any other public container registry. First, we are going to directly start a new Pod. For this we have to define our Kubernetes Pod resource definition. Create a new file pod.yaml with the following content: apiVersion : v1 kind : Pod metadata : name : test-webserver spec : containers : - image : ghcr.io/natrongmbh/kubernetes-workshop-golog-test-webserver:latest imagePullPolicy : Always name : test-webserver resources : limits : cpu : 20m memory : 32Mi requests : cpu : 10m memory : 16Mi Now we can apply this with: kubectl apply -f pod.yaml --namespace $NAMESPACE The output should be: pod/test-webserver created Use kubectl get pods --namespace $NAMESPACE in order to show the running Pod: kubectl get pods --namespace $NAMESPACE Which gives you an output similar to this: NAME READY STATUS RESTARTS AGE test-webserver 1 /1 Running 0 2m Now we can delete the Pod with: kubectl delete pod test-webserver --namespace $NAMESPACE","title":" Task 1: Start and stop a single Pod"},{"location":"kubernetes-basics/deploying-containers/#task-2-create-a-deployment","text":"In some use cases it can make sense to start a single Pod. But this has its downsides and is not really a common practice. Let\u2019s look at another concept which is tightly coupled with the Pod: the so-called Deployment . A Deployment ensures that a Pod is monitored and checks that the number of running Pods corresponds to the number of requested Pods. To create a new Deployment we first define our Deployment in a new file deployment.yaml with the following content: apiVersion : apps/v1 kind : Deployment metadata : labels : app : test-webserver name : test-webserver spec : replicas : 1 selector : matchLabels : app : test-webserver template : metadata : labels : app : test-webserver spec : containers : - image : ghcr.io/natrongmbh/kubernetes-workshop-golog-test-webserver:latest name : test-webserver resources : requests : cpu : 10m memory : 16Mi limits : cpu : 20m memory : 32Mi And with this we create our Deployment inside our already created namespace: kubectl apply -f deployment.yaml --namespace $NAMESPACE The output should be: deployment.apps/test-webserver created Kubernetes creates the defined and necessary resources, pulls the container image (in this case from ghcr.io) and deploys the Pod. Use the command kubectl get with the -w parameter in order to get the requested resources and afterward watch for changes. Note The kubectl get -w command will never end unless you terminate it with CTRL-c . kubectl get pods -w --namespace $NAMESPACE Note Instead of using the -w parameter you can also use the watch command which should be available on most Linux distributions: watch kubectl get pods --namespace $NAMESPACE This process can last for some time depending on your internet connection and if the image is already available locally. Note If you want to create your own container images and use them with Kubernetes, you definitely should have a look at these best practices and apply them. This image creation guide may be for OpenShift, however it also applies to Kubernetes and other container platforms.","title":" Task 2: Create a Deployment"},{"location":"kubernetes-basics/deploying-containers/#creating-kubernetes-resources","text":"There are two fundamentally different ways to create Kubernetes resources. You\u2019ve already seen one way: Writing the resource\u2019s definition in YAML (or JSON) and then applying it on the cluster using kubectl apply . The other variant is to use helper commands. These are more straightforward: You don\u2019t have to copy a YAML definition from somewhere else and then adapt it. However, the result is the same. The helper commands just simplify the process of creating the YAML definitions. As an example, let\u2019s look at creating above deployment, this time using a helper command instead. If you already created the Deployment using above YAML definition, you don\u2019t have to execute this command: kubectl create deployment test-webserver --image = ghcr.io/natrongmbh/kubernetes-workshop-golog-test-webserver:latest --namespace $NAMESPACE It\u2019s important to know that these helper commands exist. However, in a world where GitOps concepts have an ever-increasing presence, the idea is not to constantly create these resources with helper commands. Instead, we save the resources\u2019 YAML definitions in a Git repository and leave the creation and management of those resources to a tool.","title":"Creating Kubernetes resources"},{"location":"kubernetes-basics/deploying-containers/#task-3-viewing-the-created-resources","text":"Display the created Deployment using the following command: kubectl get deployments --namespace $NAMESPACE A Deployment defines the following facts: Update strategy: How application updates should be executed and how the Pods are exchanged Containers Which image should be deployed Environment configuration for Pods ImagePullPolicy The number of Pods/Replicas that should be deployed By using the -o (or --output ) parameter we get a lot more information about the deployment itself. You can choose between YAML and JSON formatting by indicating -o yaml or -o json . In this training we are going to use YAML, but please feel free to replace yaml with json if you prefer. kubectl get deployments test-webserver -o yaml --namespace $NAMESPACE After the image has been pulled, Kubernetes deploys a Pod according to the Deployment: kubectl get pods --namespace $NAMESPACE which gives you an output similar to this: NAME READY STATUS RESTARTS AGE test-webserver-7f7b9b9b4-2j2xg 1 /1 Running 0 2m The Deployment defines that one replica should be deployed \u2014 which is running as we can see in the output. This Pod is not yet reachable from outside the cluster.","title":" Task 3: Viewing the created resources"},{"location":"kubernetes-basics/exposing-services/","text":"Exposing Services Environment Variables We are going to use some environment variables in this tutorial. Please make sure you have set them correctly. # check if the environment variables are set if not set them export NAMESPACE = <namespace> echo $NAMESPACE export URL = ${ NAMESPACE } .k8s.golog.ch echo $URL In this module, you'll learn how to expose an application to the outside world. Task 1 : Create a NodePort Service with an Ingress The command kubectl apply -f deployment.yaml from the last tutorial creates a Deployment but no Service. A Kubernetes Service is an abstract way to expose an application running on a set of Pods as a network service. For some parts of your application (for example, frontends) you may want to expose a Service to an external IP address which is outside your cluster. Kubernetes ServiceTypes allow you to specify what kind of Service you want. The default is ClusterIP . Type values and their behaviors are: ClusterIP : Exposes the Service on a cluster-internal IP. Choosing this value only makes the Service reachable from within the cluster. This is the default ServiceType. NodePort : Exposes the Service on each Node\u2019s IP at a static port (the NodePort). A ClusterIP Service, to which the NodePort Service routes, is automatically created. You\u2019ll be able to contact the NodePort Service from outside the cluster, by requesting : . LoadBalancer : Exposes the Service externally using a cloud provider\u2019s load balancer. NodePort and ClusterIP Services, to which the external load balancer routes, are automatically created. ExternalName : Maps the Service to the contents of the externalName field (e.g. foo.bar.example.com), by returning a CNAME record with its value. No proxying of any kind is set up. You can also use Ingress to expose your Service. Ingress is not a Service type, but it acts as the entry point for your cluster. Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on the Ingress resource. An Ingress may be configured to give Services externally reachable URLs, load balance traffic, terminate SSL / TLS, and offer name-based virtual hosting. An Ingress controller is responsible for fulfilling the route, usually with a load balancer, though it may also configure your edge router or additional frontends to help handle the traffic. In order to create an Ingress, we first need to create a Service of type ClusterIP . We\u2019re going to do this with the command kubectl expose : kubectl expose deployment/test-webserver --name = test-webserver --port = 8080 --target-port = 8080 --type = NodePort --namespace $NAMESPACE Let\u2019s have a more detailed look at our Service: kubectl get service test-webserver --namespace $NAMESPACE The output should look like this: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE test-webserver NodePort 10 .97.53.32 <none> 8080 :32329/TCP 4s Note Service IP (CLUSTER-IP) addresses stay the same for the duration of the Service\u2019s lifespan. By executing the following command: kubectl get service test-webserver -o yaml --namespace $NAMESPACE You get additional information: apiVersion : v1 kind : Service metadata : labels : app : test-webserver name : test-webserver namespace : test-ns resourceVersion : \"4270474\" spec : clusterIP : 10.97.53.32 clusterIPs : - 10.97.53.32 externalTrafficPolicy : Cluster internalTrafficPolicy : Cluster ipFamilies : - IPv4 ipFamilyPolicy : SingleStack ports : - nodePort : 32329 port : 8080 protocol : TCP targetPort : 8080 selector : app : test-webserver sessionAffinity : None type : NodePort status : loadBalancer : {} The Service\u2019s selector defines which Pods are being used as Endpoints. This happens based on labels. Look at the configuration of Service and Pod in order to find out what maps to what: kubectl get service test-webserver -o yaml --namespace $NAMESPACE ... selector : app : test-webserver ... With the following command you get details from the Pod: Note First, get all Pod names from your namespace with ( kubectl get pods --namespace $NAMESPACE ) and then replace in the following command. If you have installed and configured the bash completion, you can also press the TAB key for autocompletion of the Pod\u2019s name. export POD_NAME = $( kubectl get pods --namespace $NAMESPACE -l \"app=test-webserver\" -o jsonpath = \"{.items[0].metadata.name}\" ) kubectl get pod $POD_NAME -o yaml --namespace $NAMESPACE Let\u2019s have a look at the label section of the Pod and verify that the Service selector matches the Pod\u2019s labels: ... labels : app : test-webserver ... This link between Service and Pod can also be displayed in an easier fashion with the kubectl describe command: kubectl describe service test-webserver --namespace $NAMESPACE Name: test-webserver Namespace: test-ns Labels: app=test-webserver Annotations: <none> Selector: app=test-webserver Type: NodePort IP Family Policy: SingleStack IP Families: IPv4 IP: 10.97.53.32 IPs: 10.97.53.32 Port: <unset> 8080/TCP TargetPort: 8080/TCP NodePort: <unset> 32329/TCP Endpoints: <none> Session Affinity: None External Traffic Policy: Cluster Events: <none> The Endpoints show the IP addresses of all currently matched Pods. With the NodePort Service ready, we can now create the Ingress resource. In order to create the Ingress resource, we first need to create the file ingress.yaml and change the host entry to match your environment: kubectl create --dry-run = client --namespace $NAMESPACE -o yaml -f - <<EOF >> ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: test-webserver annotations: kubernetes.io/ingress.class: nginx kubernetes.io/tls-acme: \"true\" cert-manager.io/cluster-issuer: letsencrypt-prod nginx.ingress.kubernetes.io/add-base-url: \"true\" spec: tls: - hosts: - $URL secretName: ${NAMESPACE}-tls rules: - host: $URL http: paths: - path: / pathType: Prefix backend: service: name: test-webserver port: number: 8080 EOF As you see in the resource definition at spec.rules[0].http.paths[0].backend.service.name we use the previously created test-webserver NodePort Service. Let\u2019s create the Ingress resource with: kubectl apply -f ingress.yaml --namespace $NAMESPACE Get the hostname of the Ingress resource: kubectl get ingress test-webserver --namespace $NAMESPACE Afterwards, we are able to access our freshly created Ingress at https://<namespace>.k8s.golog.ch Task 2 : For fast learners Have a closer look at the resources created in your namespace $NAMESPACE with the following commands and try to understand them: kubectl describe namespace $NAMESPACE kubectl get all --namespace $NAMESPACE kubectl describe <resource> <name> --namespace $NAMESPACE kubectl get <resource> <name> -o yaml --namespace $NAMESPACE","title":"Exposing Services"},{"location":"kubernetes-basics/exposing-services/#exposing-services","text":"Environment Variables We are going to use some environment variables in this tutorial. Please make sure you have set them correctly. # check if the environment variables are set if not set them export NAMESPACE = <namespace> echo $NAMESPACE export URL = ${ NAMESPACE } .k8s.golog.ch echo $URL In this module, you'll learn how to expose an application to the outside world.","title":"Exposing Services"},{"location":"kubernetes-basics/exposing-services/#task-1-create-a-nodeport-service-with-an-ingress","text":"The command kubectl apply -f deployment.yaml from the last tutorial creates a Deployment but no Service. A Kubernetes Service is an abstract way to expose an application running on a set of Pods as a network service. For some parts of your application (for example, frontends) you may want to expose a Service to an external IP address which is outside your cluster. Kubernetes ServiceTypes allow you to specify what kind of Service you want. The default is ClusterIP . Type values and their behaviors are: ClusterIP : Exposes the Service on a cluster-internal IP. Choosing this value only makes the Service reachable from within the cluster. This is the default ServiceType. NodePort : Exposes the Service on each Node\u2019s IP at a static port (the NodePort). A ClusterIP Service, to which the NodePort Service routes, is automatically created. You\u2019ll be able to contact the NodePort Service from outside the cluster, by requesting : . LoadBalancer : Exposes the Service externally using a cloud provider\u2019s load balancer. NodePort and ClusterIP Services, to which the external load balancer routes, are automatically created. ExternalName : Maps the Service to the contents of the externalName field (e.g. foo.bar.example.com), by returning a CNAME record with its value. No proxying of any kind is set up. You can also use Ingress to expose your Service. Ingress is not a Service type, but it acts as the entry point for your cluster. Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on the Ingress resource. An Ingress may be configured to give Services externally reachable URLs, load balance traffic, terminate SSL / TLS, and offer name-based virtual hosting. An Ingress controller is responsible for fulfilling the route, usually with a load balancer, though it may also configure your edge router or additional frontends to help handle the traffic. In order to create an Ingress, we first need to create a Service of type ClusterIP . We\u2019re going to do this with the command kubectl expose : kubectl expose deployment/test-webserver --name = test-webserver --port = 8080 --target-port = 8080 --type = NodePort --namespace $NAMESPACE Let\u2019s have a more detailed look at our Service: kubectl get service test-webserver --namespace $NAMESPACE The output should look like this: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE test-webserver NodePort 10 .97.53.32 <none> 8080 :32329/TCP 4s Note Service IP (CLUSTER-IP) addresses stay the same for the duration of the Service\u2019s lifespan. By executing the following command: kubectl get service test-webserver -o yaml --namespace $NAMESPACE You get additional information: apiVersion : v1 kind : Service metadata : labels : app : test-webserver name : test-webserver namespace : test-ns resourceVersion : \"4270474\" spec : clusterIP : 10.97.53.32 clusterIPs : - 10.97.53.32 externalTrafficPolicy : Cluster internalTrafficPolicy : Cluster ipFamilies : - IPv4 ipFamilyPolicy : SingleStack ports : - nodePort : 32329 port : 8080 protocol : TCP targetPort : 8080 selector : app : test-webserver sessionAffinity : None type : NodePort status : loadBalancer : {} The Service\u2019s selector defines which Pods are being used as Endpoints. This happens based on labels. Look at the configuration of Service and Pod in order to find out what maps to what: kubectl get service test-webserver -o yaml --namespace $NAMESPACE ... selector : app : test-webserver ... With the following command you get details from the Pod: Note First, get all Pod names from your namespace with ( kubectl get pods --namespace $NAMESPACE ) and then replace in the following command. If you have installed and configured the bash completion, you can also press the TAB key for autocompletion of the Pod\u2019s name. export POD_NAME = $( kubectl get pods --namespace $NAMESPACE -l \"app=test-webserver\" -o jsonpath = \"{.items[0].metadata.name}\" ) kubectl get pod $POD_NAME -o yaml --namespace $NAMESPACE Let\u2019s have a look at the label section of the Pod and verify that the Service selector matches the Pod\u2019s labels: ... labels : app : test-webserver ... This link between Service and Pod can also be displayed in an easier fashion with the kubectl describe command: kubectl describe service test-webserver --namespace $NAMESPACE Name: test-webserver Namespace: test-ns Labels: app=test-webserver Annotations: <none> Selector: app=test-webserver Type: NodePort IP Family Policy: SingleStack IP Families: IPv4 IP: 10.97.53.32 IPs: 10.97.53.32 Port: <unset> 8080/TCP TargetPort: 8080/TCP NodePort: <unset> 32329/TCP Endpoints: <none> Session Affinity: None External Traffic Policy: Cluster Events: <none> The Endpoints show the IP addresses of all currently matched Pods. With the NodePort Service ready, we can now create the Ingress resource. In order to create the Ingress resource, we first need to create the file ingress.yaml and change the host entry to match your environment: kubectl create --dry-run = client --namespace $NAMESPACE -o yaml -f - <<EOF >> ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: test-webserver annotations: kubernetes.io/ingress.class: nginx kubernetes.io/tls-acme: \"true\" cert-manager.io/cluster-issuer: letsencrypt-prod nginx.ingress.kubernetes.io/add-base-url: \"true\" spec: tls: - hosts: - $URL secretName: ${NAMESPACE}-tls rules: - host: $URL http: paths: - path: / pathType: Prefix backend: service: name: test-webserver port: number: 8080 EOF As you see in the resource definition at spec.rules[0].http.paths[0].backend.service.name we use the previously created test-webserver NodePort Service. Let\u2019s create the Ingress resource with: kubectl apply -f ingress.yaml --namespace $NAMESPACE Get the hostname of the Ingress resource: kubectl get ingress test-webserver --namespace $NAMESPACE Afterwards, we are able to access our freshly created Ingress at https://<namespace>.k8s.golog.ch","title":" Task 1: Create a NodePort Service with an Ingress"},{"location":"kubernetes-basics/exposing-services/#task-2-for-fast-learners","text":"Have a closer look at the resources created in your namespace $NAMESPACE with the following commands and try to understand them: kubectl describe namespace $NAMESPACE kubectl get all --namespace $NAMESPACE kubectl describe <resource> <name> --namespace $NAMESPACE kubectl get <resource> <name> -o yaml --namespace $NAMESPACE","title":" Task 2: For fast learners"},{"location":"kubernetes-basics/first-steps/","text":"First Steps In this tutorial, we will interact with the Kubernetes cluster for the first time. Warning Please make sure you completed Setup before you continue with this tutorial. Login To login to the stepping stone cluster head over to the Stepping Stone Wiki . There you will find the login information for the cluster. Also you can copy the kubeconfig file from the wiki to your local machine. (e.g. ~/.kube/config ) Note You can rename the context in the kubeconfig file to stepping-stone to make it easier to use. There are also some cli tools which improve the observability of the cluster context. (e.g. oh-my-zsh ) Warning For this tutorial you can also use a local minikube cluster. But some parts of the tutorial will not work as expected. (e.g. the ingress, storage, etc.) Namespaces As a first step on the cluster, we are going to create a new Namespace. A Namespace is a logical design used in Kubernetes to organize and separate your applications, Deployments, Pods, Ingresses, Services, etc. on a top-level basis. Take a look at the Kubernetes docs . Authorized users inside a namespace are able to manage those resources. Namespace names have to be unique in your cluster. Task 1 : Create a new Namespace Create a new namespace in the tutorial environment. The kubectl help output can help you figure out the right command. Note Please choose an identifying name for your Namespace, e.g. your initials or name as a prefix . We are going to use <namespace> as a placeholder for your created Namespace. Solution kubectl create namespace <namespace> # export it as an environment variable for later use (will be mentioned in the tutorial) export NAMESPACE = <namespace> Note By using the following command, you can switch into another Namespace instead of specifying it for each kubectl command. Linux/MacOS: kubectl config set-context $( kubectl config current-context ) --namespace <namespace> Windows: kubectl config current-context SET KUBE_CONTEXT = [Insert output of the upper command] kubectl config set-context % KUBE_CONTEXT % - -namespace < namespace > Some prefer to explicitly select the Namespace for each kubectl command by adding --namespace <namespace> or -n <namespace> . Others prefer helper tools like kubens .","title":"First Steps"},{"location":"kubernetes-basics/first-steps/#first-steps","text":"In this tutorial, we will interact with the Kubernetes cluster for the first time. Warning Please make sure you completed Setup before you continue with this tutorial.","title":"First Steps"},{"location":"kubernetes-basics/first-steps/#login","text":"To login to the stepping stone cluster head over to the Stepping Stone Wiki . There you will find the login information for the cluster. Also you can copy the kubeconfig file from the wiki to your local machine. (e.g. ~/.kube/config ) Note You can rename the context in the kubeconfig file to stepping-stone to make it easier to use. There are also some cli tools which improve the observability of the cluster context. (e.g. oh-my-zsh ) Warning For this tutorial you can also use a local minikube cluster. But some parts of the tutorial will not work as expected. (e.g. the ingress, storage, etc.)","title":"Login"},{"location":"kubernetes-basics/first-steps/#namespaces","text":"As a first step on the cluster, we are going to create a new Namespace. A Namespace is a logical design used in Kubernetes to organize and separate your applications, Deployments, Pods, Ingresses, Services, etc. on a top-level basis. Take a look at the Kubernetes docs . Authorized users inside a namespace are able to manage those resources. Namespace names have to be unique in your cluster.","title":"Namespaces"},{"location":"kubernetes-basics/first-steps/#task-1-create-a-new-namespace","text":"Create a new namespace in the tutorial environment. The kubectl help output can help you figure out the right command. Note Please choose an identifying name for your Namespace, e.g. your initials or name as a prefix . We are going to use <namespace> as a placeholder for your created Namespace. Solution kubectl create namespace <namespace> # export it as an environment variable for later use (will be mentioned in the tutorial) export NAMESPACE = <namespace> Note By using the following command, you can switch into another Namespace instead of specifying it for each kubectl command. Linux/MacOS: kubectl config set-context $( kubectl config current-context ) --namespace <namespace> Windows: kubectl config current-context SET KUBE_CONTEXT = [Insert output of the upper command] kubectl config set-context % KUBE_CONTEXT % - -namespace < namespace > Some prefer to explicitly select the Namespace for each kubectl command by adding --namespace <namespace> or -n <namespace> . Others prefer helper tools like kubens .","title":" Task 1: Create a new Namespace"},{"location":"kubernetes-basics/scaling-applications/","text":"Scaling Applications Environment Variables We are going to use some environment variables in this tutorial. Please make sure you have set them correctly. # check if the environment variables are set if not set them export NAMESPACE = <namespace> echo $NAMESPACE export URL = ${ NAMESPACE } .k8s.golog.ch echo $URL In this tutorial, we are going to show you how to scale applications on Kubernetes. Furthermore, we show you how Kubernetes makes sure that the number of requested Pods is up and running and how an application can tell the platform that it is ready to receive requests. Note This tutorial is based on previous tutorials. If you haven\u2019t done so, please complete the following tutorials: Deploying Containers Exposing Services Task 1 : Scale the test-webserver application Create a new Deployment in your Namespace. So again, lets use the Deployment deployment.yaml which we created in the previous tutorial Deploying Containers . Make sure that everything is up and running by using kubectl get pods --namespace $NAMESPACE . If we want to scale our example application, we have to tell the Deployment that we want to have three running replicas instead of one. Let\u2019s have a closer look at the existing ReplicaSet: kubectl get replicasets --namespace $NAMESPACE Which will give you an output similar to this: NAME DESIRED CURRENT READY AGE test-webserver-6564f9788b 1 1 1 54s Or for even more details: export REPLICASET = $( kubectl get replicasets --namespace $NAMESPACE -o jsonpath = \"{.items[0].metadata.name}\" ) kubectl get replicaset $REPLICASET -o yaml --namespace $NAMESPACE The ReplicaSet shows how many instances of a Pod are desired, current and ready. Now we scale our application to three replicas: kubectl scale deployment test-webserver --replicas = 3 --namespace $NAMESPACE Check the number of desired, current and ready replicas: kubectl get replicasets --namespace $NAMESPACE NAME DESIRED CURRENT READY AGE test-webserver-6564f9788b 3 3 3 3m23s Look at how many Pods there are: kubectl get pods --namespace $NAMESPACE Which gives you an output similar to this: NAME READY STATUS RESTARTS AGE test-webserver-6564f9788b-8np9n 1 /1 Running 0 3m54s test-webserver-6564f9788b-8tzt7 1 /1 Running 0 2m2s test-webserver-6564f9788b-msfvz 1 /1 Running 0 2m2s Note Kubernetes even supports autoscaling . Scaling of Pods is fast as Kubernetes simply creates new containers. You can check the availability of your Service while you scale the number of replicas up and down in your browser: https://test.k8s.golog.ch/ . Now, execute the corresponding loop command for your operating system in another console. Linux/MacOS while true ; do sleep 1 ; curl -s \"https:// ${ URL } /pod/\" ; date \"+ TIME: %H:%M:%S,%3N\" ; done Windows while ( 1 ) { Start-Sleep -s 1 Invoke-RestMethod https ://< namespace >. k8s . golog . ch / pod / Get-Date -Uformat \"+ TIME: %H:%M:%S,%3N\" } Scale from 3 replicas to 1. The output shows which Pod is still alive and is responding to requests. solution kubectl scale deployment test-webserver --replicas = 1 --namespace $NAMESPACE The requests get distributed amongst the three Pods. As soon as you scale down to one Pod, there should be only one remaining Pod that responds. Let\u2019s make another test: What happens if you start a new Deployment while our request generator is still running? kubectl rollout restart deployment test-webserver --namespace $NAMESPACE During a short period of time, you will see that we won\u2019t get any response from the Service: test-webserver-6564f9788b-8np9n TIME: 15:13:40,249 test-webserver-6564f9788b-8np9n TIME: 15:13:41,499 test-webserver-6564f9788b-8np9n TIME: 15:13:42,719 test-webserver-6564f9788b-8np9n TIME: 15:13:43,945 test-webserver-6564f9788b-8np9n TIME: 15:13:45,190 # no response test-webserver-5f8bf9b644-5ltlh TIME: 15:13:51,422 test-webserver-5f8bf9b644-5ltlh TIME: 15:13:52,635 test-webserver-5f8bf9b644-5ltlh TIME: 15:13:53,854 test-webserver-5f8bf9b644-5ltlh TIME: 15:13:55,078 test-webserver-5f8bf9b644-5ltlh TIME: 15:13:56,322 test-webserver-5f8bf9b644-5ltlh TIME: 15:13:57,548 test-webserver-5f8bf9b644-5ltlh TIME: 15:13:58,759 In our example, we use a very lightweight Pod. If we had used a more heavyweight Pod that needed a longer time to respond to requests, we would of course see a larger gap. An example for this would be a Java application with a startup time of 30 seconds: test-spring-boot-2-73aln TIME: 16:48:25,251 test-spring-boot-2-73aln TIME: 16:48:26,305 test-spring-boot-2-73aln TIME: 16:48:27,400 test-spring-boot-2-73aln TIME: 16:48:28,463 test-spring-boot-2-73aln TIME: 16:48:29,507 <html><body><h1>503 Service Unavailable</h1> No server is available to handle this request. </body></html> TIME: 16:48:33,562 <html><body><h1>503 Service Unavailable</h1> No server is available to handle this request. </body></html> TIME: 16:48:34,601 ... test-spring-boot-3-tjdkj TIME: 16:49:20,114 test-spring-boot-3-tjdkj TIME: 16:49:21,181 test-spring-boot-3-tjdkj TIME: 16:49:22,231 It is even possible that the Service gets down, and the routing layer responds with the status code 503 as can be seen in the example output above. In the following chapter we are going to look at how a Service can be configured to be highly available. Uninterruptible Deployments The rolling update strategy makes it possible to deploy Pods without interruption. The rolling update strategy means that the new version of an application gets deployed and started. As soon as the application says it is ready, Kubernetes forwards requests to the new instead of the old version of the Pod, and the old Pod gets terminated. Additionally, container health checks help Kubernetes to precisely determine what state the application is in. Basically, there are two different kinds of checks that can be implemented: Liveness probes are used to find out if an application is still running Readiness probes tell us if the application is ready to receive requests (which is especially relevant for the above-mentioned rolling updates) These probes can be implemented as HTTP checks, container execution checks (the execution of a command or script inside a container) or TCP socket checks. In our example, we want the application to tell Kubernetes that it is ready for requests with an appropriate readiness probe. Our example application has a health check context named health: https:///<namespace>.k8s.golog.ch/health Task 2 : Availability during deployment In our deployment configuration inside the rolling update strategy section, we define that our application always has to be available during an update: maxUnavailable: 0 You can directly edit the deployment (or any resource) with: kubectl edit deployment test-webserver --namespace $NAMESPACE Note If you\u2019re not comfortable with vi then you can switch to another editor by setting the environment variable EDITOR or KUBE_EDITOR , e.g. export KUBE_EDITOR=nano . Look for the following section and change the value for maxUnavailable to 0: ... spec : strategy : rollingUpdate : maxSurge : 25% maxUnavailable : 0 type : RollingUpdate ... Now insert the readiness probe at .spec.template.spec.containers above the resources: {} line: ... spec : template : spec : containers : - name : test-webserver image : gologch/test-webserver:1.0.0 # start to copy here readinessProbe : httpGet : path : /health port : 8080 initialDelaySeconds : 5 periodSeconds : 5 # stop to copy here resources : {} ... We are now going to verify that a redeployment of the application does not lead to an interruption. Set up the loop again to periodically check the application\u2019s response (you don\u2019t have to set the $URL variable again if it is still defined): Linux/MacOS while true ; do sleep 1 ; curl -s \"https:// $URL /pod/\" ; date \"+ TIME: %H:%M:%S,%3N\" ; done Windows while ( 1 ) { Start-Sleep -s 1 Invoke-RestMethod https :// test . k8s . golog . ch / pod / Get-Date -Uformat \"+ TIME: %H:%M:%S,%3N\" } Start a new deployment by editing it (the so-called ConfigChange trigger creates the new Deployment automatically): kubectl patch deployment test-webserver --patch \"{\\\"spec\\\":{\\\"template\\\":{\\\"metadata\\\":{\\\"labels\\\":{\\\"date\\\":\\\"`date +'%s'`\\\"}}}}}\" --namespace $NAMESPACE Self-healing Via the Replicaset we told Kubernetes how many replicas we want. So what happens if we simply delete a Pod? Look for a running Pod (status RUNNING ) that you can bear to kill via kubectl get pods . Show all Pods and watch for changes: kubectl get pods -w --namespace $NAMESPACE Now delete a Pod (in another terminal) with the following command: kubectl delete pod <pod-name> --namespace $NAMESPACE You should see that Kubernetes immediately starts a new Pod to replace the deleted one.","title":"Scaling Applications"},{"location":"kubernetes-basics/scaling-applications/#scaling-applications","text":"Environment Variables We are going to use some environment variables in this tutorial. Please make sure you have set them correctly. # check if the environment variables are set if not set them export NAMESPACE = <namespace> echo $NAMESPACE export URL = ${ NAMESPACE } .k8s.golog.ch echo $URL In this tutorial, we are going to show you how to scale applications on Kubernetes. Furthermore, we show you how Kubernetes makes sure that the number of requested Pods is up and running and how an application can tell the platform that it is ready to receive requests. Note This tutorial is based on previous tutorials. If you haven\u2019t done so, please complete the following tutorials: Deploying Containers Exposing Services","title":"Scaling Applications"},{"location":"kubernetes-basics/scaling-applications/#task-1-scale-the-test-webserver-application","text":"Create a new Deployment in your Namespace. So again, lets use the Deployment deployment.yaml which we created in the previous tutorial Deploying Containers . Make sure that everything is up and running by using kubectl get pods --namespace $NAMESPACE . If we want to scale our example application, we have to tell the Deployment that we want to have three running replicas instead of one. Let\u2019s have a closer look at the existing ReplicaSet: kubectl get replicasets --namespace $NAMESPACE Which will give you an output similar to this: NAME DESIRED CURRENT READY AGE test-webserver-6564f9788b 1 1 1 54s Or for even more details: export REPLICASET = $( kubectl get replicasets --namespace $NAMESPACE -o jsonpath = \"{.items[0].metadata.name}\" ) kubectl get replicaset $REPLICASET -o yaml --namespace $NAMESPACE The ReplicaSet shows how many instances of a Pod are desired, current and ready. Now we scale our application to three replicas: kubectl scale deployment test-webserver --replicas = 3 --namespace $NAMESPACE Check the number of desired, current and ready replicas: kubectl get replicasets --namespace $NAMESPACE NAME DESIRED CURRENT READY AGE test-webserver-6564f9788b 3 3 3 3m23s Look at how many Pods there are: kubectl get pods --namespace $NAMESPACE Which gives you an output similar to this: NAME READY STATUS RESTARTS AGE test-webserver-6564f9788b-8np9n 1 /1 Running 0 3m54s test-webserver-6564f9788b-8tzt7 1 /1 Running 0 2m2s test-webserver-6564f9788b-msfvz 1 /1 Running 0 2m2s Note Kubernetes even supports autoscaling . Scaling of Pods is fast as Kubernetes simply creates new containers. You can check the availability of your Service while you scale the number of replicas up and down in your browser: https://test.k8s.golog.ch/ . Now, execute the corresponding loop command for your operating system in another console. Linux/MacOS while true ; do sleep 1 ; curl -s \"https:// ${ URL } /pod/\" ; date \"+ TIME: %H:%M:%S,%3N\" ; done Windows while ( 1 ) { Start-Sleep -s 1 Invoke-RestMethod https ://< namespace >. k8s . golog . ch / pod / Get-Date -Uformat \"+ TIME: %H:%M:%S,%3N\" } Scale from 3 replicas to 1. The output shows which Pod is still alive and is responding to requests. solution kubectl scale deployment test-webserver --replicas = 1 --namespace $NAMESPACE The requests get distributed amongst the three Pods. As soon as you scale down to one Pod, there should be only one remaining Pod that responds. Let\u2019s make another test: What happens if you start a new Deployment while our request generator is still running? kubectl rollout restart deployment test-webserver --namespace $NAMESPACE During a short period of time, you will see that we won\u2019t get any response from the Service: test-webserver-6564f9788b-8np9n TIME: 15:13:40,249 test-webserver-6564f9788b-8np9n TIME: 15:13:41,499 test-webserver-6564f9788b-8np9n TIME: 15:13:42,719 test-webserver-6564f9788b-8np9n TIME: 15:13:43,945 test-webserver-6564f9788b-8np9n TIME: 15:13:45,190 # no response test-webserver-5f8bf9b644-5ltlh TIME: 15:13:51,422 test-webserver-5f8bf9b644-5ltlh TIME: 15:13:52,635 test-webserver-5f8bf9b644-5ltlh TIME: 15:13:53,854 test-webserver-5f8bf9b644-5ltlh TIME: 15:13:55,078 test-webserver-5f8bf9b644-5ltlh TIME: 15:13:56,322 test-webserver-5f8bf9b644-5ltlh TIME: 15:13:57,548 test-webserver-5f8bf9b644-5ltlh TIME: 15:13:58,759 In our example, we use a very lightweight Pod. If we had used a more heavyweight Pod that needed a longer time to respond to requests, we would of course see a larger gap. An example for this would be a Java application with a startup time of 30 seconds: test-spring-boot-2-73aln TIME: 16:48:25,251 test-spring-boot-2-73aln TIME: 16:48:26,305 test-spring-boot-2-73aln TIME: 16:48:27,400 test-spring-boot-2-73aln TIME: 16:48:28,463 test-spring-boot-2-73aln TIME: 16:48:29,507 <html><body><h1>503 Service Unavailable</h1> No server is available to handle this request. </body></html> TIME: 16:48:33,562 <html><body><h1>503 Service Unavailable</h1> No server is available to handle this request. </body></html> TIME: 16:48:34,601 ... test-spring-boot-3-tjdkj TIME: 16:49:20,114 test-spring-boot-3-tjdkj TIME: 16:49:21,181 test-spring-boot-3-tjdkj TIME: 16:49:22,231 It is even possible that the Service gets down, and the routing layer responds with the status code 503 as can be seen in the example output above. In the following chapter we are going to look at how a Service can be configured to be highly available.","title":" Task 1: Scale the test-webserver application"},{"location":"kubernetes-basics/scaling-applications/#uninterruptible-deployments","text":"The rolling update strategy makes it possible to deploy Pods without interruption. The rolling update strategy means that the new version of an application gets deployed and started. As soon as the application says it is ready, Kubernetes forwards requests to the new instead of the old version of the Pod, and the old Pod gets terminated. Additionally, container health checks help Kubernetes to precisely determine what state the application is in. Basically, there are two different kinds of checks that can be implemented: Liveness probes are used to find out if an application is still running Readiness probes tell us if the application is ready to receive requests (which is especially relevant for the above-mentioned rolling updates) These probes can be implemented as HTTP checks, container execution checks (the execution of a command or script inside a container) or TCP socket checks. In our example, we want the application to tell Kubernetes that it is ready for requests with an appropriate readiness probe. Our example application has a health check context named health: https:///<namespace>.k8s.golog.ch/health","title":"Uninterruptible Deployments"},{"location":"kubernetes-basics/scaling-applications/#task-2-availability-during-deployment","text":"In our deployment configuration inside the rolling update strategy section, we define that our application always has to be available during an update: maxUnavailable: 0 You can directly edit the deployment (or any resource) with: kubectl edit deployment test-webserver --namespace $NAMESPACE Note If you\u2019re not comfortable with vi then you can switch to another editor by setting the environment variable EDITOR or KUBE_EDITOR , e.g. export KUBE_EDITOR=nano . Look for the following section and change the value for maxUnavailable to 0: ... spec : strategy : rollingUpdate : maxSurge : 25% maxUnavailable : 0 type : RollingUpdate ... Now insert the readiness probe at .spec.template.spec.containers above the resources: {} line: ... spec : template : spec : containers : - name : test-webserver image : gologch/test-webserver:1.0.0 # start to copy here readinessProbe : httpGet : path : /health port : 8080 initialDelaySeconds : 5 periodSeconds : 5 # stop to copy here resources : {} ... We are now going to verify that a redeployment of the application does not lead to an interruption. Set up the loop again to periodically check the application\u2019s response (you don\u2019t have to set the $URL variable again if it is still defined): Linux/MacOS while true ; do sleep 1 ; curl -s \"https:// $URL /pod/\" ; date \"+ TIME: %H:%M:%S,%3N\" ; done Windows while ( 1 ) { Start-Sleep -s 1 Invoke-RestMethod https :// test . k8s . golog . ch / pod / Get-Date -Uformat \"+ TIME: %H:%M:%S,%3N\" } Start a new deployment by editing it (the so-called ConfigChange trigger creates the new Deployment automatically): kubectl patch deployment test-webserver --patch \"{\\\"spec\\\":{\\\"template\\\":{\\\"metadata\\\":{\\\"labels\\\":{\\\"date\\\":\\\"`date +'%s'`\\\"}}}}}\" --namespace $NAMESPACE","title":" Task 2: Availability during deployment"},{"location":"kubernetes-basics/scaling-applications/#self-healing","text":"Via the Replicaset we told Kubernetes how many replicas we want. So what happens if we simply delete a Pod? Look for a running Pod (status RUNNING ) that you can bear to kill via kubectl get pods . Show all Pods and watch for changes: kubectl get pods -w --namespace $NAMESPACE Now delete a Pod (in another terminal) with the following command: kubectl delete pod <pod-name> --namespace $NAMESPACE You should see that Kubernetes immediately starts a new Pod to replace the deleted one.","title":"Self-healing"},{"location":"kubernetes-basics/storage/","text":"Storage Environment Variables We are going to use some environment variables in this tutorial. Please make sure you have set them correctly. # check if the environment variables are set if not set them export NAMESPACE = <namespace> echo $NAMESPACE export URL = ${ NAMESPACE } .k8s.golog.ch echo $URL By default, data in containers is not persistent as was the case e.g. in Database Connection . This means that data that was written in a container is lost as soon as it does not exist anymore. We want to prevent this from happening. One possible solution to this problem is to use persistent storage. Request storage Attaching persistent storage to a Pod happens in two steps. The first step includes the creation of a so-called PersistentVolumeClaim (PVC) in our namespace. This claim defines amongst other things what size we would like to get. The PersistentVolumeClaim only represents a request but not the storage itself. It is automatically going to be bound to a PersistentVolume by Kubernetes, one that has at least the requested size. If only volumes exist that have a bigger size than was requested, one of these volumes is going to be used. The claim will automatically be updated with the new size. If there are only smaller volumes available, the claim cannot be fulfilled as long as no volume the exact same or larger size is created. On the stepping stone cluster we have a NFS service that provides storage to the cluster. You can find the details of the service in the stepping stone documentation . NFS Storage Class For the next steps you need to deploy a Storage Class Provider, which handles the PVC with the NFS Server. A solid service is the following Project: NFS Subdir External Provisioner Note The following steps are only necessary if it was not already deployed. Also make sure you know how to use helm For a simple helm deployment you can execute the following commands: helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/ helm repo update Create a dedicated namespace: kubectl create namespace nfs-provisioner Create the Helm deployment: helm install nfs-subdir-external-provisioner --namespace nfs-provisioner \\ nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\ --set nfs.server = 192 .168.16.17 \\ --set nfs.path = /var/data/share \\ --set storageClass.name = nfs \\ --set storageClass.onDelete = true Task 1 : Create a PersistentVolumeClaim and attach it to the Pod For this tutorial we will create a new deployment with a simple webserver, which serves static files. We want to store the static files on a persistent volume. Details For further information read the NFS based persistent storage documentation. Then we need to create a PersistentVolumeClaim . A persistent volume claim (PVC) specifies the desired access mode and storage capacity. Currently, based on only these two attributes, a PVC is bound to a single PV. Once a PV is bound to a PVC, that PV is essentially tied to the PVC\u2019s project and cannot be bound to by another PVC. There is a one-to-one mapping of PVs and PVCs. However, multiple pods in the same project can use the same PVC. So we create a new file called nfs-pvc.yaml with the following content: apiVersion : v1 kind : PersistentVolumeClaim metadata : name : webserver-pvc spec : accessModes : - ReadWriteOnce # ReadWriteMany is also possible for NFS resources : requests : storage : 1Gi storageClassName : nfs Note The storageClassName must match the name of the Storage Class Provider. In our case it is nfs . You can check the name of the Storage Class Provider with the following command: kubectl get storageclass Afterwards, create a file called nfs-deployment.yaml for a classic nginx webserver and try to attach the persistent volume claim at the mount path /usr/share/nginx/html to the Pod. solution apiVersion : apps/v1 kind : Deployment metadata : labels : app : nfs-webserver name : nfs-webserver spec : replicas : 1 selector : matchLabels : app : nfs-webserver template : metadata : labels : app : nfs-webserver spec : containers : - image : nginx name : nfs-webserver resources : requests : cpu : 10m memory : 16Mi limits : cpu : 20m memory : 32Mi volumeMounts : - mountPath : /usr/share/nginx/html name : webserver volumes : - name : webserver persistentVolumeClaim : claimName : webserver-pvc Also create the responsible nfs-service.yaml and nfs-ingress.yaml file to expose the webserver to the outside world. solution nfs-service.yaml : # nfs-service.yaml apiVersion : v1 kind : Service metadata : name : nfs-webserver spec : type : NodePort ports : - port : 80 targetPort : 80 selector : app : nfs-webserver nfs-ingress.yaml : kubectl create --dry-run = client --namespace $NAMESPACE -o yaml -f - <<EOF >> nfs-ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: nfs-webserver annotations: kubernetes.io/ingress.class: nginx kubernetes.io/tls-acme: \"true\" cert-manager.io/cluster-issuer: letsencrypt-prod nginx.ingress.kubernetes.io/add-base-url: \"true\" spec: tls: - hosts: - $URL secretName: ${NAMESPACE}-tls rules: - host: $URL http: paths: - path: / pathType: Prefix backend: service: name: nfs-webserver port: number: 80 EOF Ingress Before you can deploy the Ingress, make sure you delete the old one. Otherwise you will get an error message. kubectl get ingress --namespace $NAMESPACE kubectl delete ingress <ingress> --namespace $NAMESPACE Now you can deploy everything in order to test it. kubectl apply -f nfs-pvc.yaml --namespace $NAMESPACE kubectl apply -f nfs-deployment.yaml --namespace $NAMESPACE kubectl apply -f nfs-service.yaml --namespace $NAMESPACE kubectl apply -f nfs-ingress.yaml --namespace $NAMESPACE Describe the Persistent Volume Claim to see if it was created successfully. Task 2 : Adding a file to the persistent volume When we now visit the webserver, we will see that we get a 403 Forbidden error. This is because the directory is empty and we need to add a file to it. We can do this by opening a shell in the Pod and adding a file to the directory. There are multiple ways to do this. One way is to use the kubectl exec command. This command allows us to execute a command in a running container. We can use this to open a shell in the container. kubectl exec -it nfs-webserver-<pod-id> --namespace $NAMESPACE -- /bin/bash Because there is no editor installed in the container, we simply use echo to create a file. echo \"<h1>Hello World</h1>\" > /usr/share/nginx/html/index.html Note You can also execute the echo command directly in the kubectl exec command. kubectl exec -it nfs-webserver-<pod-id> --namespace $NAMESPACE -- /bin/bash -c \"echo '<h1>Hello World</h1>' > /usr/share/nginx/html/index.html\" Now we can visit the webserver and see that the file was created successfully. Another way to do this is to use the kubectl cp command. This command allows us to copy files from and to a container. We can use this to copy a file from our local machine to the container. First, we need to create a index.html file on our local machine. echo \"<h1>Hello World</h1>\" > index.html Then we can copy the file to the container. kubectl cp index.html nfs-webserver-<pod-id>:/usr/share/nginx/html/index.html --namespace $NAMESPACE Now we can visit the webserver and see that the file was created successfully. Task 3 : Deleting the Pod Now we can delete the Pod and see that the file is still there. This is because the file is stored on the NFS server and not in the Pod. export POD_NAME = $( kubectl get pods --namespace $NAMESPACE -l app = nfs-webserver -o jsonpath = \"{.items[0].metadata.name}\" ) kubectl delete pod $POD_NAME --namespace $NAMESPACE Now we can visit the webserver and see that the file is still there.","title":"Storage"},{"location":"kubernetes-basics/storage/#storage","text":"Environment Variables We are going to use some environment variables in this tutorial. Please make sure you have set them correctly. # check if the environment variables are set if not set them export NAMESPACE = <namespace> echo $NAMESPACE export URL = ${ NAMESPACE } .k8s.golog.ch echo $URL By default, data in containers is not persistent as was the case e.g. in Database Connection . This means that data that was written in a container is lost as soon as it does not exist anymore. We want to prevent this from happening. One possible solution to this problem is to use persistent storage.","title":"Storage"},{"location":"kubernetes-basics/storage/#request-storage","text":"Attaching persistent storage to a Pod happens in two steps. The first step includes the creation of a so-called PersistentVolumeClaim (PVC) in our namespace. This claim defines amongst other things what size we would like to get. The PersistentVolumeClaim only represents a request but not the storage itself. It is automatically going to be bound to a PersistentVolume by Kubernetes, one that has at least the requested size. If only volumes exist that have a bigger size than was requested, one of these volumes is going to be used. The claim will automatically be updated with the new size. If there are only smaller volumes available, the claim cannot be fulfilled as long as no volume the exact same or larger size is created. On the stepping stone cluster we have a NFS service that provides storage to the cluster. You can find the details of the service in the stepping stone documentation .","title":"Request storage"},{"location":"kubernetes-basics/storage/#nfs-storage-class","text":"For the next steps you need to deploy a Storage Class Provider, which handles the PVC with the NFS Server. A solid service is the following Project: NFS Subdir External Provisioner Note The following steps are only necessary if it was not already deployed. Also make sure you know how to use helm For a simple helm deployment you can execute the following commands: helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/ helm repo update Create a dedicated namespace: kubectl create namespace nfs-provisioner Create the Helm deployment: helm install nfs-subdir-external-provisioner --namespace nfs-provisioner \\ nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\ --set nfs.server = 192 .168.16.17 \\ --set nfs.path = /var/data/share \\ --set storageClass.name = nfs \\ --set storageClass.onDelete = true","title":"NFS Storage Class"},{"location":"kubernetes-basics/storage/#task-1-create-a-persistentvolumeclaim-and-attach-it-to-the-pod","text":"For this tutorial we will create a new deployment with a simple webserver, which serves static files. We want to store the static files on a persistent volume. Details For further information read the NFS based persistent storage documentation. Then we need to create a PersistentVolumeClaim . A persistent volume claim (PVC) specifies the desired access mode and storage capacity. Currently, based on only these two attributes, a PVC is bound to a single PV. Once a PV is bound to a PVC, that PV is essentially tied to the PVC\u2019s project and cannot be bound to by another PVC. There is a one-to-one mapping of PVs and PVCs. However, multiple pods in the same project can use the same PVC. So we create a new file called nfs-pvc.yaml with the following content: apiVersion : v1 kind : PersistentVolumeClaim metadata : name : webserver-pvc spec : accessModes : - ReadWriteOnce # ReadWriteMany is also possible for NFS resources : requests : storage : 1Gi storageClassName : nfs Note The storageClassName must match the name of the Storage Class Provider. In our case it is nfs . You can check the name of the Storage Class Provider with the following command: kubectl get storageclass Afterwards, create a file called nfs-deployment.yaml for a classic nginx webserver and try to attach the persistent volume claim at the mount path /usr/share/nginx/html to the Pod. solution apiVersion : apps/v1 kind : Deployment metadata : labels : app : nfs-webserver name : nfs-webserver spec : replicas : 1 selector : matchLabels : app : nfs-webserver template : metadata : labels : app : nfs-webserver spec : containers : - image : nginx name : nfs-webserver resources : requests : cpu : 10m memory : 16Mi limits : cpu : 20m memory : 32Mi volumeMounts : - mountPath : /usr/share/nginx/html name : webserver volumes : - name : webserver persistentVolumeClaim : claimName : webserver-pvc Also create the responsible nfs-service.yaml and nfs-ingress.yaml file to expose the webserver to the outside world. solution nfs-service.yaml : # nfs-service.yaml apiVersion : v1 kind : Service metadata : name : nfs-webserver spec : type : NodePort ports : - port : 80 targetPort : 80 selector : app : nfs-webserver nfs-ingress.yaml : kubectl create --dry-run = client --namespace $NAMESPACE -o yaml -f - <<EOF >> nfs-ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: nfs-webserver annotations: kubernetes.io/ingress.class: nginx kubernetes.io/tls-acme: \"true\" cert-manager.io/cluster-issuer: letsencrypt-prod nginx.ingress.kubernetes.io/add-base-url: \"true\" spec: tls: - hosts: - $URL secretName: ${NAMESPACE}-tls rules: - host: $URL http: paths: - path: / pathType: Prefix backend: service: name: nfs-webserver port: number: 80 EOF Ingress Before you can deploy the Ingress, make sure you delete the old one. Otherwise you will get an error message. kubectl get ingress --namespace $NAMESPACE kubectl delete ingress <ingress> --namespace $NAMESPACE Now you can deploy everything in order to test it. kubectl apply -f nfs-pvc.yaml --namespace $NAMESPACE kubectl apply -f nfs-deployment.yaml --namespace $NAMESPACE kubectl apply -f nfs-service.yaml --namespace $NAMESPACE kubectl apply -f nfs-ingress.yaml --namespace $NAMESPACE Describe the Persistent Volume Claim to see if it was created successfully.","title":" Task 1: Create a PersistentVolumeClaim and attach it to the Pod"},{"location":"kubernetes-basics/storage/#task-2-adding-a-file-to-the-persistent-volume","text":"When we now visit the webserver, we will see that we get a 403 Forbidden error. This is because the directory is empty and we need to add a file to it. We can do this by opening a shell in the Pod and adding a file to the directory. There are multiple ways to do this. One way is to use the kubectl exec command. This command allows us to execute a command in a running container. We can use this to open a shell in the container. kubectl exec -it nfs-webserver-<pod-id> --namespace $NAMESPACE -- /bin/bash Because there is no editor installed in the container, we simply use echo to create a file. echo \"<h1>Hello World</h1>\" > /usr/share/nginx/html/index.html Note You can also execute the echo command directly in the kubectl exec command. kubectl exec -it nfs-webserver-<pod-id> --namespace $NAMESPACE -- /bin/bash -c \"echo '<h1>Hello World</h1>' > /usr/share/nginx/html/index.html\" Now we can visit the webserver and see that the file was created successfully. Another way to do this is to use the kubectl cp command. This command allows us to copy files from and to a container. We can use this to copy a file from our local machine to the container. First, we need to create a index.html file on our local machine. echo \"<h1>Hello World</h1>\" > index.html Then we can copy the file to the container. kubectl cp index.html nfs-webserver-<pod-id>:/usr/share/nginx/html/index.html --namespace $NAMESPACE Now we can visit the webserver and see that the file was created successfully.","title":" Task 2: Adding a file to the persistent volume"},{"location":"kubernetes-basics/storage/#task-3-deleting-the-pod","text":"Now we can delete the Pod and see that the file is still there. This is because the file is stored on the NFS server and not in the Pod. export POD_NAME = $( kubectl get pods --namespace $NAMESPACE -l app = nfs-webserver -o jsonpath = \"{.items[0].metadata.name}\" ) kubectl delete pod $POD_NAME --namespace $NAMESPACE Now we can visit the webserver and see that the file is still there.","title":" Task 3: Deleting the Pod"},{"location":"kubernetes-basics/troubleshooting/","text":"Troubleshooting Environment Variables We are going to use some environment variables in this tutorial. Please make sure you have set them correctly. # check if the environment variables are set if not set them export NAMESPACE = <namespace> echo $NAMESPACE This tutorial will help you troubleshoot your application and show you some tools that can make troubleshooting easier. Logging into a container Running containers should be treated as immutable infrastructure and should therefore not be modified. However, there are some use cases in which you have to log into your running container. Debugging and analyzing is one example for this. Task 1 : Shell into Pod With Kubernetes you can open a remote shell into a Pod without installing SSH by using the command kubectl exec . The command can also be used to execute any command in a Pod. With the parameter -it you can leave an open connection. Note On Windows, you can use Git Bash and winpty . Choose a Pod with kubectl get pods --namespace $NAMESPACE and execute the following command: kubectl exec -it <pod> --namespace $NAMESPACE -- /bin/sh Note If Bash is available in the Pod you can fallback to \u2013 /bin/bash instead of \u2013 /bin/sh . You now have a running shell session inside the container in which you can execute every binary available, e.g.: ~@<pod>:/# ls -la /usr/local/bin/ total 6308 drwxr-xr-x 1 root root 16 Nov 19 13 :43 . drwxr-xr-x 1 root root 17 Aug 9 08 :47 .. -rwxr-xr-x 1 root root 6456761 Nov 19 13 :43 go With exit or CTRL+d you can leave the container and close the connection: ~@<pod>:/# exit Task 2 : Single commands Single commands inside a container can also be executed with kubectl exec : kubectl exec <pod> --namespace $NAMESPACE -- env Watching log files Log files of a Pod can be shown with the following command: kubectl logs <pod> --namespace $NAMESPACE The parameter -f allows you to follow the log file (same as tail -f ). With this, log files are streamed and new entries are shown immediately. When a Pod is in state CrashLoopBackOff it means that although multiple attempts have been made, no container inside the Pod could be started successfully. Now even though no container might be running at the moment the kubectl log s command is executed, there is a way to view the logs the application might have generated. This is achieved using the -p or --previous parameter: kubectl logs <pod> --namespace $NAMESPACE -p Task 3 : Port forwarding Kubernetes allows you to forward arbitrary ports to your development workstation. This allows you to access admin consoles, databases, etc., even when they are not exposed externally. Port forwarding is handled by the Kubernetes control plane nodes and therefore tunneled from the client via HTTPS. This allows you to access the Kubernetes platform even when there are restrictive firewalls or proxies between your workstation and Kubernetes. Get the name of the Pod: kubectl get pods --namespace $NAMESPACE Then execute the port forwarding command using the Pod\u2019s name: kubectl port-forward <pod> 8080 :8080 --namespace $NAMESPACE Note Use the additional parameter --address <IP address> (where <IP address> refers to a NIC\u2019s IP address from your local workstation) if you want to access the forwarded port from outside your own local workstation. The output of the command should look like this: Forwarding from 127.0.0.1:8080 -> 8080 Forwarding from [::1]:8080 -> 8080 Don\u2019t forget to change the Pod name to your own installation. If configured, you can use auto-completion. The application is now available with the following link: http://localhost:8080/ . Or try a curl command: curl http://localhost:8080/ With the same concept you can access databases from your local workstation or connect your local development environment via remote debugging to your application in the Pod. This documentation page offers some more details about port forwarding. Note The kubectl port-forward process runs as long as it is not terminated by the user. So when done, stop it with CTRL-c . Events Kubernetes maintains an event log with high-level information on what\u2019s going on in the cluster. It\u2019s possible that everything looks okay at first but somehow something seems stuck. Make sure to have a look at the events because they can give you more information if something is not working as expected. Use the following command to list the events in chronological order: kubectl get events --sort-by = .metadata.creationTimestamp --namespace $NAMESPACE Dry-run To help verify changes, you can use the optional kubectl flag --dry-run=client -o yaml to see the rendered YAML definition of your Kubernetes objects, without sending it to the API. The following kubectl subcommands support this flag (non-final list): apply create expose patch replace run set For example, we can use the --dry-run=client flag to create a template for our a Nginx deployment: kubectl create deployment nginx --image = nginx --dry-run = client -o yaml The result is the following YAML output: apiVersion : apps/v1 kind : Deployment metadata : creationTimestamp : null labels : app : nginx name : nginx spec : replicas : 1 selector : matchLabels : app : nginx strategy : {} template : metadata : creationTimestamp : null labels : app : nginx spec : containers : - image : nginx name : nginx resources : {} status : {} kubectl API requests If you want to see the HTTP requests kubectl sends to the Kubernetes API in detail, you can use the optional flag --v=10 . For example, to see the API request for creating a namespace: kubectl create namespace test --v = 10 The result is the following output: I1119 15:47:52.822841 25474 loader.go:372] Config loaded from file: /home/nte-jla/.kube/config I1119 15:47:52.824692 25474 request.go:1073] Request Body: {\"kind\":\"Namespace\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"test2\",\"creationTimestamp\":null},\"spec\":{},\"status\":{}} I1119 15:47:52.824891 25474 round_trippers.go:466] curl -v -XPOST -H \"User-Agent: kubectl/v1.24.3 (linux/amd64) kubernetes/aef86a9\" -H \"Authorization: Bearer <masked>\" -H \"Accept: application/json, */*\" -H \"Content-Type: application/json\" 'https://gog-pro-lbaas-01.os.stoney-cloud.com:6443/api/v1/namespaces?fieldManager=kubectl-create&fieldValidation=Strict' I1119 15:47:52.829841 25474 round_trippers.go:495] HTTP Trace: DNS Lookup for gog-pro-lbaas-01.os.stoney-cloud.com resolved to [{185.85.126.71 }] I1119 15:47:52.831672 25474 round_trippers.go:510] HTTP Trace: Dial to tcp:185.85.126.71:6443 succeed I1119 15:47:52.878279 25474 round_trippers.go:553] POST https://gog-pro-lbaas-01.os.stoney-cloud.com:6443/api/v1/namespaces?fieldManager=kubectl-create&fieldValidation=Strict 201 Created in 53 milliseconds I1119 15:47:52.878340 25474 round_trippers.go:570] HTTP Statistics: DNSLookup 4 ms Dial 1 ms TLSHandshake 17 ms ServerProcessing 28 ms Duration 53 ms I1119 15:47:52.878369 25474 round_trippers.go:577] Response Headers: I1119 15:47:52.878399 25474 round_trippers.go:580] Content-Type: application/json I1119 15:47:52.878428 25474 round_trippers.go:580] X-Kubernetes-Pf-Flowschema-Uid: 5bdf6f47-b545-478e-89e3-56cee0a9bfa1 I1119 15:47:52.878455 25474 round_trippers.go:580] X-Kubernetes-Pf-Prioritylevel-Uid: f75dacbb-8f1d-4a76-8234-a205d24e39ea I1119 15:47:52.878481 25474 round_trippers.go:580] Content-Length: 520 I1119 15:47:52.878507 25474 round_trippers.go:580] Date: Sat, 19 Nov 2022 14:47:52 GMT I1119 15:47:52.878533 25474 round_trippers.go:580] Audit-Id: 8cfa2adb-b9eb-49fc-94d2-7e1fe192c4c2 I1119 15:47:52.878559 25474 round_trippers.go:580] Cache-Control: no-cache, private I1119 15:47:52.878648 25474 request.go:1073] Response Body: {\"kind\":\"Namespace\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"test2\",\"uid\":\"252882cf-a7db-4269-84c0-271381fab4d1\",\"resourceVersion\":\"4295281\",\"creationTimestamp\":\"2022-11-19T14:47:52Z\",\"labels\":{\"kubernetes.io/metadata.name\":\"test2\"},\"managedFields\":[{\"manager\":\"kubectl-create\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2022-11-19T14:47:52Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:kubernetes.io/metadata.name\":{}}}}}]},\"spec\":{\"finalizers\":[\"kubernetes\"]},\"status\":{\"phase\":\"Active\"}} namespace/test2 created As you can see, the output conveniently contains the corresponding curl commands which we could use in our own code, tools, pipelines etc. Note If you created the deployment to see the output, you can delete it again as it\u2019s not used anywhere else (which is also the reason why the replicas are set to 0 ): kubectl delete deployment nginx","title":"Troubleshooting"},{"location":"kubernetes-basics/troubleshooting/#troubleshooting","text":"Environment Variables We are going to use some environment variables in this tutorial. Please make sure you have set them correctly. # check if the environment variables are set if not set them export NAMESPACE = <namespace> echo $NAMESPACE This tutorial will help you troubleshoot your application and show you some tools that can make troubleshooting easier.","title":"Troubleshooting"},{"location":"kubernetes-basics/troubleshooting/#logging-into-a-container","text":"Running containers should be treated as immutable infrastructure and should therefore not be modified. However, there are some use cases in which you have to log into your running container. Debugging and analyzing is one example for this.","title":"Logging into a container"},{"location":"kubernetes-basics/troubleshooting/#task-1-shell-into-pod","text":"With Kubernetes you can open a remote shell into a Pod without installing SSH by using the command kubectl exec . The command can also be used to execute any command in a Pod. With the parameter -it you can leave an open connection. Note On Windows, you can use Git Bash and winpty . Choose a Pod with kubectl get pods --namespace $NAMESPACE and execute the following command: kubectl exec -it <pod> --namespace $NAMESPACE -- /bin/sh Note If Bash is available in the Pod you can fallback to \u2013 /bin/bash instead of \u2013 /bin/sh . You now have a running shell session inside the container in which you can execute every binary available, e.g.: ~@<pod>:/# ls -la /usr/local/bin/ total 6308 drwxr-xr-x 1 root root 16 Nov 19 13 :43 . drwxr-xr-x 1 root root 17 Aug 9 08 :47 .. -rwxr-xr-x 1 root root 6456761 Nov 19 13 :43 go With exit or CTRL+d you can leave the container and close the connection: ~@<pod>:/# exit","title":" Task 1: Shell into Pod"},{"location":"kubernetes-basics/troubleshooting/#task-2-single-commands","text":"Single commands inside a container can also be executed with kubectl exec : kubectl exec <pod> --namespace $NAMESPACE -- env","title":" Task 2: Single commands"},{"location":"kubernetes-basics/troubleshooting/#watching-log-files","text":"Log files of a Pod can be shown with the following command: kubectl logs <pod> --namespace $NAMESPACE The parameter -f allows you to follow the log file (same as tail -f ). With this, log files are streamed and new entries are shown immediately. When a Pod is in state CrashLoopBackOff it means that although multiple attempts have been made, no container inside the Pod could be started successfully. Now even though no container might be running at the moment the kubectl log s command is executed, there is a way to view the logs the application might have generated. This is achieved using the -p or --previous parameter: kubectl logs <pod> --namespace $NAMESPACE -p","title":"Watching log files"},{"location":"kubernetes-basics/troubleshooting/#task-3-port-forwarding","text":"Kubernetes allows you to forward arbitrary ports to your development workstation. This allows you to access admin consoles, databases, etc., even when they are not exposed externally. Port forwarding is handled by the Kubernetes control plane nodes and therefore tunneled from the client via HTTPS. This allows you to access the Kubernetes platform even when there are restrictive firewalls or proxies between your workstation and Kubernetes. Get the name of the Pod: kubectl get pods --namespace $NAMESPACE Then execute the port forwarding command using the Pod\u2019s name: kubectl port-forward <pod> 8080 :8080 --namespace $NAMESPACE Note Use the additional parameter --address <IP address> (where <IP address> refers to a NIC\u2019s IP address from your local workstation) if you want to access the forwarded port from outside your own local workstation. The output of the command should look like this: Forwarding from 127.0.0.1:8080 -> 8080 Forwarding from [::1]:8080 -> 8080 Don\u2019t forget to change the Pod name to your own installation. If configured, you can use auto-completion. The application is now available with the following link: http://localhost:8080/ . Or try a curl command: curl http://localhost:8080/ With the same concept you can access databases from your local workstation or connect your local development environment via remote debugging to your application in the Pod. This documentation page offers some more details about port forwarding. Note The kubectl port-forward process runs as long as it is not terminated by the user. So when done, stop it with CTRL-c .","title":" Task 3: Port forwarding"},{"location":"kubernetes-basics/troubleshooting/#events","text":"Kubernetes maintains an event log with high-level information on what\u2019s going on in the cluster. It\u2019s possible that everything looks okay at first but somehow something seems stuck. Make sure to have a look at the events because they can give you more information if something is not working as expected. Use the following command to list the events in chronological order: kubectl get events --sort-by = .metadata.creationTimestamp --namespace $NAMESPACE","title":"Events"},{"location":"kubernetes-basics/troubleshooting/#dry-run","text":"To help verify changes, you can use the optional kubectl flag --dry-run=client -o yaml to see the rendered YAML definition of your Kubernetes objects, without sending it to the API. The following kubectl subcommands support this flag (non-final list): apply create expose patch replace run set For example, we can use the --dry-run=client flag to create a template for our a Nginx deployment: kubectl create deployment nginx --image = nginx --dry-run = client -o yaml The result is the following YAML output: apiVersion : apps/v1 kind : Deployment metadata : creationTimestamp : null labels : app : nginx name : nginx spec : replicas : 1 selector : matchLabels : app : nginx strategy : {} template : metadata : creationTimestamp : null labels : app : nginx spec : containers : - image : nginx name : nginx resources : {} status : {}","title":"Dry-run"},{"location":"kubernetes-basics/troubleshooting/#kubectl-api-requests","text":"If you want to see the HTTP requests kubectl sends to the Kubernetes API in detail, you can use the optional flag --v=10 . For example, to see the API request for creating a namespace: kubectl create namespace test --v = 10 The result is the following output: I1119 15:47:52.822841 25474 loader.go:372] Config loaded from file: /home/nte-jla/.kube/config I1119 15:47:52.824692 25474 request.go:1073] Request Body: {\"kind\":\"Namespace\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"test2\",\"creationTimestamp\":null},\"spec\":{},\"status\":{}} I1119 15:47:52.824891 25474 round_trippers.go:466] curl -v -XPOST -H \"User-Agent: kubectl/v1.24.3 (linux/amd64) kubernetes/aef86a9\" -H \"Authorization: Bearer <masked>\" -H \"Accept: application/json, */*\" -H \"Content-Type: application/json\" 'https://gog-pro-lbaas-01.os.stoney-cloud.com:6443/api/v1/namespaces?fieldManager=kubectl-create&fieldValidation=Strict' I1119 15:47:52.829841 25474 round_trippers.go:495] HTTP Trace: DNS Lookup for gog-pro-lbaas-01.os.stoney-cloud.com resolved to [{185.85.126.71 }] I1119 15:47:52.831672 25474 round_trippers.go:510] HTTP Trace: Dial to tcp:185.85.126.71:6443 succeed I1119 15:47:52.878279 25474 round_trippers.go:553] POST https://gog-pro-lbaas-01.os.stoney-cloud.com:6443/api/v1/namespaces?fieldManager=kubectl-create&fieldValidation=Strict 201 Created in 53 milliseconds I1119 15:47:52.878340 25474 round_trippers.go:570] HTTP Statistics: DNSLookup 4 ms Dial 1 ms TLSHandshake 17 ms ServerProcessing 28 ms Duration 53 ms I1119 15:47:52.878369 25474 round_trippers.go:577] Response Headers: I1119 15:47:52.878399 25474 round_trippers.go:580] Content-Type: application/json I1119 15:47:52.878428 25474 round_trippers.go:580] X-Kubernetes-Pf-Flowschema-Uid: 5bdf6f47-b545-478e-89e3-56cee0a9bfa1 I1119 15:47:52.878455 25474 round_trippers.go:580] X-Kubernetes-Pf-Prioritylevel-Uid: f75dacbb-8f1d-4a76-8234-a205d24e39ea I1119 15:47:52.878481 25474 round_trippers.go:580] Content-Length: 520 I1119 15:47:52.878507 25474 round_trippers.go:580] Date: Sat, 19 Nov 2022 14:47:52 GMT I1119 15:47:52.878533 25474 round_trippers.go:580] Audit-Id: 8cfa2adb-b9eb-49fc-94d2-7e1fe192c4c2 I1119 15:47:52.878559 25474 round_trippers.go:580] Cache-Control: no-cache, private I1119 15:47:52.878648 25474 request.go:1073] Response Body: {\"kind\":\"Namespace\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"test2\",\"uid\":\"252882cf-a7db-4269-84c0-271381fab4d1\",\"resourceVersion\":\"4295281\",\"creationTimestamp\":\"2022-11-19T14:47:52Z\",\"labels\":{\"kubernetes.io/metadata.name\":\"test2\"},\"managedFields\":[{\"manager\":\"kubectl-create\",\"operation\":\"Update\",\"apiVersion\":\"v1\",\"time\":\"2022-11-19T14:47:52Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\".\":{},\"f:kubernetes.io/metadata.name\":{}}}}}]},\"spec\":{\"finalizers\":[\"kubernetes\"]},\"status\":{\"phase\":\"Active\"}} namespace/test2 created As you can see, the output conveniently contains the corresponding curl commands which we could use in our own code, tools, pipelines etc. Note If you created the deployment to see the output, you can delete it again as it\u2019s not used anywhere else (which is also the reason why the replicas are set to 0 ): kubectl delete deployment nginx","title":"kubectl API requests"},{"location":"kubernetes-basics/advanced-concepts/","text":"Advanced Concepts Kubernetes does not only know Pods, Deployments, Services, etc. There are various other kinds of resources. In the next few labs, we are going to have a look at some of them.","title":"Advanced Concepts"},{"location":"kubernetes-basics/advanced-concepts/#advanced-concepts","text":"Kubernetes does not only know Pods, Deployments, Services, etc. There are various other kinds of resources. In the next few labs, we are going to have a look at some of them.","title":"Advanced Concepts"},{"location":"kubernetes-basics/advanced-concepts/configmaps/","text":"Configmaps Environment Variables We are going to use some environment variables in this tutorial. Please make sure you have set them correctly. # check if the environment variables are set if not set them export NAMESPACE = <namespace> echo $NAMESPACE Similar to environment variables, ConfigsMaps allow you to separate the configuration for an application from the image. Pods can access those variables at runtime which allows maximum portability for applications running in containers. In this lab, you will learn how to create and use ConfigMaps. A ConfigMap can be created using the kubectl create configmap command as follows: kubectl create configmap <name> <data-source> --namespace $NAMESPACE Where the <data-source> can be a file, directory, or command line input. Task 1 : Create a ConfigMap for Java properties A classic example for ConfigMaps are properties files of Java applications which can\u2019t be configured with environment variables. First, create a file called java.properties with the following content: JAVA_OPTS=-Xmx512m key=value key2=value2 Now you can create a ConfigMap based on that file: kubectl create configmap javaconfiguration --from-file = ./java.properties --namespace $NAMESPACE Verify that the ConfigMap was created successfully: kubectl get configmaps --namespace $NAMESPACE The output should look like this: NAME DATA AGE javaconfiguration 1 2m Have a look at its content: kubectl get configmap javaconfiguration -o yaml --namespace $NAMESPACE The output should look like this: apiVersion: v1 kind: ConfigMap metadata: name: javaconfiguration data: java.properties: | JAVA_OPTS=-Xmx512m key=value key2=value2 Task 2 : Create a Pod that uses the ConfigMap Next, we want to make a ConfigMap accessible for a container. There are basically the following possibilities to achieve this : ConfigMap properties as environment variables in a Deployment Command line arguments via environment variables Mounted as volumes in the container In this example, we want the file to be mounted as a volume inside the container. Basically, a Deployment has to be extended with the following config: ... volumeMounts : - mountPath : /etc/config name : config-volume ... volumes : - configMap : defaultMode : 420 name : javaconfiguration name : config-volume ... The volumeMounts section defines the mount point inside the container. The volumes section defines the volume that should be mounted. The name property of the volume has to match the name property of the volumeMounts section. Create a file called java-deployment.yaml with the following content: apiVersion : apps/v1 kind : Deployment metadata : labels : app : spring-boot-example name : spring-boot-example spec : progressDeadlineSeconds : 600 replicas : 1 revisionHistoryLimit : 10 selector : matchLabels : app : spring-boot-example strategy : rollingUpdate : maxSurge : 25% maxUnavailable : 25% type : RollingUpdate template : metadata : labels : app : spring-boot-example spec : containers : - image : appuio/example-spring-boot imagePullPolicy : Always name : example-spring-boot resources : limits : cpu : 1 memory : 768Mi requests : cpu : 20m memory : 32Mi terminationMessagePath : /dev/termination-log terminationMessagePolicy : File volumeMounts : - mountPath : /etc/config name : config-volume dnsPolicy : ClusterFirst restartPolicy : Always schedulerName : default-scheduler securityContext : {} terminationGracePeriodSeconds : 30 volumes : - configMap : defaultMode : 420 name : javaconfiguration name : config-volume Deploy it: kubectl apply -f java-deployment.yaml --namespace $NAMESPACE This means that the container should now be able to access the ConfigMap\u2019s content in /etc/config/java.properties . Let\u2019s check: export POD_NAME = $( kubectl get pods --namespace $NAMESPACE -l \"app=spring-boot-example\" -o jsonpath = \"{.items[0].metadata.name}\" ) kubectl exec -it $POD_NAME --namespace $NAMESPACE -- cat /etc/config/java.properties Note On Windows, you can use Git Bash with winpty kubectl exec -it <pod> --namespace $NAMESPACE -- cat //etc/config/java.properties . The output should look like this: JAVA_OPTS=-Xmx512m key=value key2=value2 Like this, the property file can be read and used by the application inside the container. The image stays portable to other environments. Task 3 : Create a ConfigMap for environment variables In the previous task, we created a ConfigMap for a Java properties file. Now we want to create a ConfigMap for environment variables. Use a ConfigMap to define the environment variables JAVA_OPTS and JAVA_TOOL_OPTIONS . You can refer to the official documentation for more information. solutions kubectl create configmap javaenv --from-literal = JAVA_OPTS = -Xmx512m --from-literal = JAVA_TOOL_OPTIONS = -Dfile.encoding = UTF-8 --namespace $NAMESPACE Update the java-deployment.yaml file with the following content: apiVersion : apps/v1 kind : Deployment metadata : labels : app : spring-boot-example name : spring-boot-example spec : progressDeadlineSeconds : 600 replicas : 1 revisionHistoryLimit : 10 selector : matchLabels : app : spring-boot-example strategy : rollingUpdate : maxSurge : 25% maxUnavailable : 25% type : RollingUpdate template : metadata : labels : app : spring-boot-example spec : containers : - image : appuio/example-spring-boot imagePullPolicy : Always name : example-spring-boot resources : limits : cpu : 1 memory : 768Mi requests : cpu : 20m memory : 32Mi terminationMessagePath : /dev/termination-log terminationMessagePolicy : File volumeMounts : - mountPath : /etc/config name : config-volume envFrom : - configMapRef : name : javaenv dnsPolicy : ClusterFirst restartPolicy : Always schedulerName : default-scheduler securityContext : {} terminationGracePeriodSeconds : 30 volumes : - configMap : defaultMode : 420 name : javaconfiguration name : config-volume Make sure you delete the old deployment first: kubectl delete deployment spring-boot-example --namespace $NAMESPACE Then, create the new deployment: kubectl create -f java-deployment.yaml --namespace $NAMESPACE Check the environment variables of the container: export POD_NAME = $( kubectl get pods --namespace $NAMESPACE -l \"app=spring-boot-example\" -o jsonpath = \"{.items[0].metadata.name}\" ) kubectl exec -it $POD_NAME --namespace $NAMESPACE -- env The output should look like this: ... JAVA_OPTS=-Xmx512m JAVA_TOOL_OPTIONS=-Dfile.encoding=UTF-8 ...","title":"ConfigMaps"},{"location":"kubernetes-basics/advanced-concepts/configmaps/#configmaps","text":"Environment Variables We are going to use some environment variables in this tutorial. Please make sure you have set them correctly. # check if the environment variables are set if not set them export NAMESPACE = <namespace> echo $NAMESPACE Similar to environment variables, ConfigsMaps allow you to separate the configuration for an application from the image. Pods can access those variables at runtime which allows maximum portability for applications running in containers. In this lab, you will learn how to create and use ConfigMaps. A ConfigMap can be created using the kubectl create configmap command as follows: kubectl create configmap <name> <data-source> --namespace $NAMESPACE Where the <data-source> can be a file, directory, or command line input.","title":"Configmaps"},{"location":"kubernetes-basics/advanced-concepts/configmaps/#task-1-create-a-configmap-for-java-properties","text":"A classic example for ConfigMaps are properties files of Java applications which can\u2019t be configured with environment variables. First, create a file called java.properties with the following content: JAVA_OPTS=-Xmx512m key=value key2=value2 Now you can create a ConfigMap based on that file: kubectl create configmap javaconfiguration --from-file = ./java.properties --namespace $NAMESPACE Verify that the ConfigMap was created successfully: kubectl get configmaps --namespace $NAMESPACE The output should look like this: NAME DATA AGE javaconfiguration 1 2m Have a look at its content: kubectl get configmap javaconfiguration -o yaml --namespace $NAMESPACE The output should look like this: apiVersion: v1 kind: ConfigMap metadata: name: javaconfiguration data: java.properties: | JAVA_OPTS=-Xmx512m key=value key2=value2","title":" Task 1: Create a ConfigMap for Java properties"},{"location":"kubernetes-basics/advanced-concepts/configmaps/#task-2-create-a-pod-that-uses-the-configmap","text":"Next, we want to make a ConfigMap accessible for a container. There are basically the following possibilities to achieve this : ConfigMap properties as environment variables in a Deployment Command line arguments via environment variables Mounted as volumes in the container In this example, we want the file to be mounted as a volume inside the container. Basically, a Deployment has to be extended with the following config: ... volumeMounts : - mountPath : /etc/config name : config-volume ... volumes : - configMap : defaultMode : 420 name : javaconfiguration name : config-volume ... The volumeMounts section defines the mount point inside the container. The volumes section defines the volume that should be mounted. The name property of the volume has to match the name property of the volumeMounts section. Create a file called java-deployment.yaml with the following content: apiVersion : apps/v1 kind : Deployment metadata : labels : app : spring-boot-example name : spring-boot-example spec : progressDeadlineSeconds : 600 replicas : 1 revisionHistoryLimit : 10 selector : matchLabels : app : spring-boot-example strategy : rollingUpdate : maxSurge : 25% maxUnavailable : 25% type : RollingUpdate template : metadata : labels : app : spring-boot-example spec : containers : - image : appuio/example-spring-boot imagePullPolicy : Always name : example-spring-boot resources : limits : cpu : 1 memory : 768Mi requests : cpu : 20m memory : 32Mi terminationMessagePath : /dev/termination-log terminationMessagePolicy : File volumeMounts : - mountPath : /etc/config name : config-volume dnsPolicy : ClusterFirst restartPolicy : Always schedulerName : default-scheduler securityContext : {} terminationGracePeriodSeconds : 30 volumes : - configMap : defaultMode : 420 name : javaconfiguration name : config-volume Deploy it: kubectl apply -f java-deployment.yaml --namespace $NAMESPACE This means that the container should now be able to access the ConfigMap\u2019s content in /etc/config/java.properties . Let\u2019s check: export POD_NAME = $( kubectl get pods --namespace $NAMESPACE -l \"app=spring-boot-example\" -o jsonpath = \"{.items[0].metadata.name}\" ) kubectl exec -it $POD_NAME --namespace $NAMESPACE -- cat /etc/config/java.properties Note On Windows, you can use Git Bash with winpty kubectl exec -it <pod> --namespace $NAMESPACE -- cat //etc/config/java.properties . The output should look like this: JAVA_OPTS=-Xmx512m key=value key2=value2 Like this, the property file can be read and used by the application inside the container. The image stays portable to other environments.","title":" Task 2: Create a Pod that uses the ConfigMap"},{"location":"kubernetes-basics/advanced-concepts/configmaps/#task-3-create-a-configmap-for-environment-variables","text":"In the previous task, we created a ConfigMap for a Java properties file. Now we want to create a ConfigMap for environment variables. Use a ConfigMap to define the environment variables JAVA_OPTS and JAVA_TOOL_OPTIONS . You can refer to the official documentation for more information. solutions kubectl create configmap javaenv --from-literal = JAVA_OPTS = -Xmx512m --from-literal = JAVA_TOOL_OPTIONS = -Dfile.encoding = UTF-8 --namespace $NAMESPACE Update the java-deployment.yaml file with the following content: apiVersion : apps/v1 kind : Deployment metadata : labels : app : spring-boot-example name : spring-boot-example spec : progressDeadlineSeconds : 600 replicas : 1 revisionHistoryLimit : 10 selector : matchLabels : app : spring-boot-example strategy : rollingUpdate : maxSurge : 25% maxUnavailable : 25% type : RollingUpdate template : metadata : labels : app : spring-boot-example spec : containers : - image : appuio/example-spring-boot imagePullPolicy : Always name : example-spring-boot resources : limits : cpu : 1 memory : 768Mi requests : cpu : 20m memory : 32Mi terminationMessagePath : /dev/termination-log terminationMessagePolicy : File volumeMounts : - mountPath : /etc/config name : config-volume envFrom : - configMapRef : name : javaenv dnsPolicy : ClusterFirst restartPolicy : Always schedulerName : default-scheduler securityContext : {} terminationGracePeriodSeconds : 30 volumes : - configMap : defaultMode : 420 name : javaconfiguration name : config-volume Make sure you delete the old deployment first: kubectl delete deployment spring-boot-example --namespace $NAMESPACE Then, create the new deployment: kubectl create -f java-deployment.yaml --namespace $NAMESPACE Check the environment variables of the container: export POD_NAME = $( kubectl get pods --namespace $NAMESPACE -l \"app=spring-boot-example\" -o jsonpath = \"{.items[0].metadata.name}\" ) kubectl exec -it $POD_NAME --namespace $NAMESPACE -- env The output should look like this: ... JAVA_OPTS=-Xmx512m JAVA_TOOL_OPTIONS=-Dfile.encoding=UTF-8 ...","title":" Task 3: Create a ConfigMap for environment variables"},{"location":"kubernetes-basics/advanced-concepts/daemonsets/","text":"DaemonSets A DaemonSet is almost identical to a normal Deployment. The difference is that it makes sure that exactly one Pod is running on every (or some specified) Node. When a new Node is added, the DaemonSet automatically deploys a Pod on the new Node if its selector matches. When the DaemonSet is deleted, all related Pods are deleted. One obvious use case for a DaemonSet is some kind of agent or daemon to e.g. grab logs from each Node of the cluster (e.g., Fluentd, Logstash or a Splunk forwarder). More information about DaemonSet can be found in the Kubernetes DaemonSet Documentation .","title":"DaemonSets"},{"location":"kubernetes-basics/advanced-concepts/daemonsets/#daemonsets","text":"A DaemonSet is almost identical to a normal Deployment. The difference is that it makes sure that exactly one Pod is running on every (or some specified) Node. When a new Node is added, the DaemonSet automatically deploys a Pod on the new Node if its selector matches. When the DaemonSet is deleted, all related Pods are deleted. One obvious use case for a DaemonSet is some kind of agent or daemon to e.g. grab logs from each Node of the cluster (e.g., Fluentd, Logstash or a Splunk forwarder). More information about DaemonSet can be found in the Kubernetes DaemonSet Documentation .","title":"DaemonSets"},{"location":"kubernetes-basics/advanced-concepts/init-containers/","text":"Init Containers Environment Variables We are going to use some environment variables in this tutorial. Please make sure you have set them correctly. # check if the environment variables are set if not set them export NAMESPACE = <namespace> echo $NAMESPACE A Pod can have multiple containers running apps within it, but it can also have one or more init containers, which are run before the app container is started. Init containers are exactly like regular containers, except: Init containers always run to completion. Each init container must complete successfully before the next one starts. Check Init Containers from the Kubernetes documentation for more details. Task 1 : Create an init container We want to create a Pod that runs an init container that creates a file and then runs a container that reads the file. Create a file called init-container.yaml with the following content: apiVersion : v1 kind : Pod metadata : name : init-demo spec : containers : - name : nginx image : nginx resources : requests : cpu : 10m memory : 16Mi limits : cpu : 20m memory : 32Mi volumeMounts : - name : workdir mountPath : /usr/share/nginx/html initContainers : - name : init image : busybox command : - 'sh' - '-c' - 'echo \"<h1>Hello World</h1>\" > /work-dir/index.html' volumeMounts : - name : workdir mountPath : /work-dir volumes : - name : workdir emptyDir : {} Apply the file: kubectl apply -f init-container.yaml --namespace $NAMESPACE Check the status of the Pod: kubectl get pod init-demo --namespace $NAMESPACE The output should look like this: NAME READY STATUS RESTARTS AGE init-demo 0 /1 Init:0/1 0 1m The Pod is in the Init:0/1 state, which means that the init container is running. Check the /usr/share/nginx/html/index.html file in the nginx container: kubectl exec -it init-demo --namespace $NAMESPACE -- cat /usr/share/nginx/html/index.html The output should look like this: Defaulted container \"nginx\" out of: nginx, init (init) <h1>Hello World</h1> for fast learners Try to expose the Pod with a Service and an Ingress. Task 2 : Create a Init Container that checks database connectivity We want to create a Deployment that runs an init container that checks database connectivity and then starts the main container. Create a file called init-deployment.yaml with the following content: apiVersion : apps/v1 kind : Deployment metadata : labels : app : init-deployment name : init-deployment spec : replicas : 1 selector : matchLabels : app : init-deployment template : metadata : labels : app : init-deployment spec : initContainers : - name : pg-isready image : postgres:14.5 command : - 'sh' - '-c' - | until pg_isready -h <postgresql host> -p 5432; do echo \"waiting for database to start\" sleep 2 done - 'echo' - 'Database is ready' containers : - image : ghcr.io/natrongmbh/kubernetes-workshop-golog-test-postgresql-webserver:latest name : init-deployment resources : requests : cpu : 10m memory : 16Mi limits : cpu : 20m memory : 32Mi envFrom : - secretRef : name : db-secret Note Make sure to replace <postgresql host> with the host of your PostgreSQL database. Apply the file: kubectl apply -f init-deployment.yaml --namespace $NAMESPACE Check the status of the Deployment: kubectl get deployment init-deployment --namespace $NAMESPACE The output should look like this: NAME READY STATUS RESTARTS AGE init-deployment-5567dc778c-ns27h 0/1 Init:0/1 0 3s The Deployment is in the Init:0/1 state, which means that the init container is running. Check the logs of the init container: export POD_NAME = $( kubectl get pods --namespace $NAMESPACE -l \"app=init-deployment\" -o jsonpath = \"{.items[0].metadata.name}\" ) kubectl logs $POD_NAME -c pg-isready --namespace $NAMESPACE","title":"Init Containers"},{"location":"kubernetes-basics/advanced-concepts/init-containers/#init-containers","text":"Environment Variables We are going to use some environment variables in this tutorial. Please make sure you have set them correctly. # check if the environment variables are set if not set them export NAMESPACE = <namespace> echo $NAMESPACE A Pod can have multiple containers running apps within it, but it can also have one or more init containers, which are run before the app container is started. Init containers are exactly like regular containers, except: Init containers always run to completion. Each init container must complete successfully before the next one starts. Check Init Containers from the Kubernetes documentation for more details.","title":"Init Containers"},{"location":"kubernetes-basics/advanced-concepts/init-containers/#task-1-create-an-init-container","text":"We want to create a Pod that runs an init container that creates a file and then runs a container that reads the file. Create a file called init-container.yaml with the following content: apiVersion : v1 kind : Pod metadata : name : init-demo spec : containers : - name : nginx image : nginx resources : requests : cpu : 10m memory : 16Mi limits : cpu : 20m memory : 32Mi volumeMounts : - name : workdir mountPath : /usr/share/nginx/html initContainers : - name : init image : busybox command : - 'sh' - '-c' - 'echo \"<h1>Hello World</h1>\" > /work-dir/index.html' volumeMounts : - name : workdir mountPath : /work-dir volumes : - name : workdir emptyDir : {} Apply the file: kubectl apply -f init-container.yaml --namespace $NAMESPACE Check the status of the Pod: kubectl get pod init-demo --namespace $NAMESPACE The output should look like this: NAME READY STATUS RESTARTS AGE init-demo 0 /1 Init:0/1 0 1m The Pod is in the Init:0/1 state, which means that the init container is running. Check the /usr/share/nginx/html/index.html file in the nginx container: kubectl exec -it init-demo --namespace $NAMESPACE -- cat /usr/share/nginx/html/index.html The output should look like this: Defaulted container \"nginx\" out of: nginx, init (init) <h1>Hello World</h1> for fast learners Try to expose the Pod with a Service and an Ingress.","title":" Task 1: Create an init container"},{"location":"kubernetes-basics/advanced-concepts/init-containers/#task-2-create-a-init-container-that-checks-database-connectivity","text":"We want to create a Deployment that runs an init container that checks database connectivity and then starts the main container. Create a file called init-deployment.yaml with the following content: apiVersion : apps/v1 kind : Deployment metadata : labels : app : init-deployment name : init-deployment spec : replicas : 1 selector : matchLabels : app : init-deployment template : metadata : labels : app : init-deployment spec : initContainers : - name : pg-isready image : postgres:14.5 command : - 'sh' - '-c' - | until pg_isready -h <postgresql host> -p 5432; do echo \"waiting for database to start\" sleep 2 done - 'echo' - 'Database is ready' containers : - image : ghcr.io/natrongmbh/kubernetes-workshop-golog-test-postgresql-webserver:latest name : init-deployment resources : requests : cpu : 10m memory : 16Mi limits : cpu : 20m memory : 32Mi envFrom : - secretRef : name : db-secret Note Make sure to replace <postgresql host> with the host of your PostgreSQL database. Apply the file: kubectl apply -f init-deployment.yaml --namespace $NAMESPACE Check the status of the Deployment: kubectl get deployment init-deployment --namespace $NAMESPACE The output should look like this: NAME READY STATUS RESTARTS AGE init-deployment-5567dc778c-ns27h 0/1 Init:0/1 0 3s The Deployment is in the Init:0/1 state, which means that the init container is running. Check the logs of the init container: export POD_NAME = $( kubectl get pods --namespace $NAMESPACE -l \"app=init-deployment\" -o jsonpath = \"{.items[0].metadata.name}\" ) kubectl logs $POD_NAME -c pg-isready --namespace $NAMESPACE","title":" Task 2: Create a Init Container that checks database connectivity"},{"location":"kubernetes-basics/advanced-concepts/jobs-and-cronjobs/","text":"Jobs and CronJobs Environment Variables We are going to use some environment variables in this tutorial. Please make sure you have set them correctly. # check if the environment variables are set if not set them export NAMESPACE = <namespace> echo $NAMESPACE Jobs are different from normal Deployments: Jobs execute a time-constrained operation and report the result as soon as they are finished; think of a batch job. To achieve this, a Job creates a Pod and runs a defined command. A Job isn\u2019t limited to creating a single Pod, it can also create multiple Pods. When a Job is deleted, the Pods started (and stopped) by the Job are also deleted. For example, a Job is used to ensure that a Pod is run until its completion. If a Pod fails, for example because of a Node error, the Job starts a new one. A Job can also be used to start multiple Pods in parallel. More detailed information can be retrieved from the Kubernetes documentation . Task 1 : Create a Job for a database dump We want to create a Job that creates a postgresql database dump and stores it in a file. The Job should run once and then terminate. Create a file called job.yaml with the following content: apiVersion : batch/v1 kind : Job metadata : name : pg-dump spec : template : spec : containers : - name : pg-dump image : postgres:14.5 command : - 'bash' - '-eo' - 'pipefail' - '-c' - > trap \"echo Backup failed; exit 0\" ERR; FILENAME=backup-$(date +%Y-%m-%d_%H-%M-%S).sql.gz; echo \"creating .pgpass file\"; echo \"$POSTGRES_HOST:$POSTGRES_PORT:$POSTGRES_DB:$POSTGRES_USER:$POSTGRES_PASSWORD\" > ~/.pgpass; chmod 0600 ~/.pgpass; echo \"creating dump\"; pg_dump -h $POSTGRES_HOST -p $POSTGRES_PORT -U $POSTGRES_USER $POSTGRES_DB | gzip > $FILENAME; echo \"\"; echo \"Backup created: $FILENAME\"; du -h $FILENAME; env : - name : POSTGRES_USER value : \"<postgresql user>\" - name : POSTGRES_HOST value : \"<postgresql host>\" - name : POSTGRES_PORT value : \"5432\" - name : POSTGRES_DB value : \"<postgresql database>\" - name : POSTGRES_PASSWORD valueFrom : secretKeyRef : name : job-secret key : POSTGRES_PASSWORD resources : limits : cpu : 40m memory : 64Mi requests : cpu : 10m memory : 32Mi restartPolicy : Never The Job uses the postgres:14.5 image to create a database dump. The database password credentials are stored in a secret called job-secret . The Job is configured to run only once and then terminate. Create the secret file job-secret.yaml with the following content: apiVersion : v1 kind : Secret metadata : name : job-secret stringData : POSTGRES_PASSWORD : \"<postgresql password>\" Execute the following commands to create the Job and the secret: kubectl apply -f job-secret.yaml --namespace $NAMESPACE kubectl apply -f job.yaml --namespace $NAMESPACE Task 2 : Check the Job status Check the status of the Job: kubectl get jobs --namespace $NAMESPACE The output should look like this: NAME COMPLETIONS DURATION AGE pg-dump 1/1 19s 4m47 The Job is completed after 19 seconds. The Job created a Pod and executed the command defined in the command section of the Job. The output of the command is stored in the Job status. Check the Job pod logs: kubectl logs -f jobs/pg-dump --namespace $NAMESPACE The output should look like this: creating .pgpass file creating dump Backup created: backup-2022-11-21_09-38-50.sql.gz 4.0K backup-2022-11-21_09-38-50.sql.gz The Job created a database dump and stored it in a file called backup-2022-11-21_09-38-50.sql.gz . To show all Pods belonging to a Job in a human-readable format, the following command can be used: kubectl get pods --selector = job-name = pg-dump --output = go-template = '{{range .items}}{{.metadata.name}}{{end}}' --namespace $NAMESPACE Task 3 : Create a CronJob A CronJob is nothing else than a resource which creates a Job at a defined time, which in turn starts (as we saw in the previous section) a Pod to run a command. Typical use cases are cleanup Jobs, which tidy up old data for a running Pod, or a Job to regularly create and save a database dump as we just did during this tutorial. The CronJob\u2019s definition will remind you of the Deployment\u2019s structure, or really any other control resource. There\u2019s most importantly the schedule specification in cron schedule format , some more things you could define and then the Job\u2019s definition itself that is going to be created by the CronJob. Try to create a CronJob that runs every hour at minute 0 and creates a database dump. The CronJob should be named pg-dump-cronjob and the Job should be named pg-dump-job . The Job should be created in the same namespace as the CronJob. solution apiVersion : batch/v1 kind : CronJob metadata : name : pg-dump-cronjob spec : schedule : \"0 * * * *\" jobTemplate : spec : template : spec : containers : - name : pg-dump image : postgres:14.5 command : - 'bash' - '-eo' - 'pipefail' - '-c' - > trap \"echo Backup failed; exit 0\" ERR; FILENAME=backup-$(date +%Y-%m-%d_%H-%M-%S).sql.gz; echo \"creating .pgpass file\"; echo \"$POSTGRES_HOST:$POSTGRES_PORT:$POSTGRES_DB:$POSTGRES_USER:$POSTGRES_PASSWORD\" > ~/.pgpass; chmod 0600 ~/.pgpass; echo \"creating dump\"; pg_dump -h $POSTGRES_HOST -p $POSTGRES_PORT -U $POSTGRES_USER $POSTGRES_DB | gzip > $FILENAME; echo \"\"; echo \"Backup created: $FILENAME\"; du -h $FILENAME; env : - name : POSTGRES_USER value : \"<postgresql user>\" - name : POSTGRES_HOST value : \"<postgresql host>\" - name : POSTGRES_PORT value : \"5432\" - name : POSTGRES_DB value : \"<postgresql database>\" - name : POSTGRES_PASSWORD valueFrom : secretKeyRef : name : job-secret key : POSTGRES_PASSWORD resources : limits : cpu : 40m memory : 64Mi requests : cpu : 10m memory : 32Mi restartPolicy : Never Further information can be found in the Kubernetes CronJob documentation .","title":"Jobs and CronJobs"},{"location":"kubernetes-basics/advanced-concepts/jobs-and-cronjobs/#jobs-and-cronjobs","text":"Environment Variables We are going to use some environment variables in this tutorial. Please make sure you have set them correctly. # check if the environment variables are set if not set them export NAMESPACE = <namespace> echo $NAMESPACE Jobs are different from normal Deployments: Jobs execute a time-constrained operation and report the result as soon as they are finished; think of a batch job. To achieve this, a Job creates a Pod and runs a defined command. A Job isn\u2019t limited to creating a single Pod, it can also create multiple Pods. When a Job is deleted, the Pods started (and stopped) by the Job are also deleted. For example, a Job is used to ensure that a Pod is run until its completion. If a Pod fails, for example because of a Node error, the Job starts a new one. A Job can also be used to start multiple Pods in parallel. More detailed information can be retrieved from the Kubernetes documentation .","title":"Jobs and CronJobs"},{"location":"kubernetes-basics/advanced-concepts/jobs-and-cronjobs/#task-1-create-a-job-for-a-database-dump","text":"We want to create a Job that creates a postgresql database dump and stores it in a file. The Job should run once and then terminate. Create a file called job.yaml with the following content: apiVersion : batch/v1 kind : Job metadata : name : pg-dump spec : template : spec : containers : - name : pg-dump image : postgres:14.5 command : - 'bash' - '-eo' - 'pipefail' - '-c' - > trap \"echo Backup failed; exit 0\" ERR; FILENAME=backup-$(date +%Y-%m-%d_%H-%M-%S).sql.gz; echo \"creating .pgpass file\"; echo \"$POSTGRES_HOST:$POSTGRES_PORT:$POSTGRES_DB:$POSTGRES_USER:$POSTGRES_PASSWORD\" > ~/.pgpass; chmod 0600 ~/.pgpass; echo \"creating dump\"; pg_dump -h $POSTGRES_HOST -p $POSTGRES_PORT -U $POSTGRES_USER $POSTGRES_DB | gzip > $FILENAME; echo \"\"; echo \"Backup created: $FILENAME\"; du -h $FILENAME; env : - name : POSTGRES_USER value : \"<postgresql user>\" - name : POSTGRES_HOST value : \"<postgresql host>\" - name : POSTGRES_PORT value : \"5432\" - name : POSTGRES_DB value : \"<postgresql database>\" - name : POSTGRES_PASSWORD valueFrom : secretKeyRef : name : job-secret key : POSTGRES_PASSWORD resources : limits : cpu : 40m memory : 64Mi requests : cpu : 10m memory : 32Mi restartPolicy : Never The Job uses the postgres:14.5 image to create a database dump. The database password credentials are stored in a secret called job-secret . The Job is configured to run only once and then terminate. Create the secret file job-secret.yaml with the following content: apiVersion : v1 kind : Secret metadata : name : job-secret stringData : POSTGRES_PASSWORD : \"<postgresql password>\" Execute the following commands to create the Job and the secret: kubectl apply -f job-secret.yaml --namespace $NAMESPACE kubectl apply -f job.yaml --namespace $NAMESPACE","title":" Task 1: Create a Job for a database dump"},{"location":"kubernetes-basics/advanced-concepts/jobs-and-cronjobs/#task-2-check-the-job-status","text":"Check the status of the Job: kubectl get jobs --namespace $NAMESPACE The output should look like this: NAME COMPLETIONS DURATION AGE pg-dump 1/1 19s 4m47 The Job is completed after 19 seconds. The Job created a Pod and executed the command defined in the command section of the Job. The output of the command is stored in the Job status. Check the Job pod logs: kubectl logs -f jobs/pg-dump --namespace $NAMESPACE The output should look like this: creating .pgpass file creating dump Backup created: backup-2022-11-21_09-38-50.sql.gz 4.0K backup-2022-11-21_09-38-50.sql.gz The Job created a database dump and stored it in a file called backup-2022-11-21_09-38-50.sql.gz . To show all Pods belonging to a Job in a human-readable format, the following command can be used: kubectl get pods --selector = job-name = pg-dump --output = go-template = '{{range .items}}{{.metadata.name}}{{end}}' --namespace $NAMESPACE","title":" Task 2: Check the Job status"},{"location":"kubernetes-basics/advanced-concepts/jobs-and-cronjobs/#task-3-create-a-cronjob","text":"A CronJob is nothing else than a resource which creates a Job at a defined time, which in turn starts (as we saw in the previous section) a Pod to run a command. Typical use cases are cleanup Jobs, which tidy up old data for a running Pod, or a Job to regularly create and save a database dump as we just did during this tutorial. The CronJob\u2019s definition will remind you of the Deployment\u2019s structure, or really any other control resource. There\u2019s most importantly the schedule specification in cron schedule format , some more things you could define and then the Job\u2019s definition itself that is going to be created by the CronJob. Try to create a CronJob that runs every hour at minute 0 and creates a database dump. The CronJob should be named pg-dump-cronjob and the Job should be named pg-dump-job . The Job should be created in the same namespace as the CronJob. solution apiVersion : batch/v1 kind : CronJob metadata : name : pg-dump-cronjob spec : schedule : \"0 * * * *\" jobTemplate : spec : template : spec : containers : - name : pg-dump image : postgres:14.5 command : - 'bash' - '-eo' - 'pipefail' - '-c' - > trap \"echo Backup failed; exit 0\" ERR; FILENAME=backup-$(date +%Y-%m-%d_%H-%M-%S).sql.gz; echo \"creating .pgpass file\"; echo \"$POSTGRES_HOST:$POSTGRES_PORT:$POSTGRES_DB:$POSTGRES_USER:$POSTGRES_PASSWORD\" > ~/.pgpass; chmod 0600 ~/.pgpass; echo \"creating dump\"; pg_dump -h $POSTGRES_HOST -p $POSTGRES_PORT -U $POSTGRES_USER $POSTGRES_DB | gzip > $FILENAME; echo \"\"; echo \"Backup created: $FILENAME\"; du -h $FILENAME; env : - name : POSTGRES_USER value : \"<postgresql user>\" - name : POSTGRES_HOST value : \"<postgresql host>\" - name : POSTGRES_PORT value : \"5432\" - name : POSTGRES_DB value : \"<postgresql database>\" - name : POSTGRES_PASSWORD valueFrom : secretKeyRef : name : job-secret key : POSTGRES_PASSWORD resources : limits : cpu : 40m memory : 64Mi requests : cpu : 10m memory : 32Mi restartPolicy : Never Further information can be found in the Kubernetes CronJob documentation .","title":" Task 3: Create a CronJob"},{"location":"kubernetes-basics/advanced-concepts/resource-quotas/","text":"ResourceQuotas and LimitRanges In this tutorial, we are going to look at ResourceQuotas and LimitRanges. As Kubernetes users, we are most certainly going to encounter the limiting effects that ResourceQuotas and LimitRanges impose. Warning For this lab to work it is vital that you create and use the namespace <username>-quota ! ResourceQuotas ResourceQuotas among other things limit the amount of resources Pods can use in a Namespace. They can also be used to limit the total number of a certain resource type in a Namespace. In more detail, there are these kinds of quotas: Compute ResourceQuotas can be used to limit the amount of memory and CPU Storage ResourceQuotas can be used to limit the total amount of storage and the number of PersistentVolumeClaims, generally or specific to a StorageClass Object count quotas can be used to limit the number of a certain resource type such as Services, Pods or Secrets Defining ResourceQuotas makes sense when the cluster administrators want to have better control over consumed resources. A typical use case are public offerings where users pay for a certain guaranteed amount of resources which must not be exceeded. In order to check for defined quotas in your Namespace, simply see if there are any of type ResourceQuota: kubectl get resourcequota --namespace <namespace> To show in detail what kinds of limits the quota imposes: kubectl describe resourcequota <quota-name> --namespace <namespace> For more details, have look at Kubernetes\u2019 documentation about resource quotas . Requests and limits As we\u2019ve already seen, compute ResourceQuotas limit the amount of memory and CPU we can use in a Namespace. Only defining a ResourceQuota, however is not going to have an effect on Pods that don\u2019t define the amount of resources they want to use. This is where the concept of limits and requests comes into play. Limits and requests on a Pod, or rather on a container in a Pod, define how much memory and CPU this container wants to consume at least (request) and at most (limit). Requests mean that the container will be guaranteed to get at least this amount of resources, limits represent the upper boundary which cannot be crossed. Defining these values helps Kubernetes in determining on which Node to schedule the Pod because it knows how many resources should be available for it. Note Containers using more CPU time than what their limit allows will be throttled. Containers using more memory than what they are allowed to use will be killed. Defining limits and requests on a Pod that has one container looks like this: apiVersion : v1 kind : Pod metadata : name : lr-demo namespace : lr-example spec : containers : - name : lr-demo-ctr image : docker.io/nginxinc/nginx-unprivileged:1.18-alpine resources : limits : memory : \"200Mi\" cpu : \"700m\" requests : memory : \"200Mi\" cpu : \"700m\" You can see the familiar binary unit \u201cMi\u201d is used for the memory value. Other binary (\u201cGi\u201d, \u201cKi\u201d, \u2026) or decimal units (\u201cM\u201d, \u201cG\u201d, \u201cK\u201d, \u2026) can be used as well. The CPU value is denoted as \u201cm\u201d. \u201cm\u201d stands for millicpu or sometimes also referred to as millicores where \" 1000m \" is equal to one core/vCPU/hyperthread. Quality of service Setting limits and requests on containers has yet another effect: It might change the Pod\u2019s Quality of Service class. There are three such QoS classes: Guaranteed Burstable BestEffort The Guaranteed QoS class is applied to Pods that define both limits and requests for both memory and CPU resources on all their containers. The most important part is that each request has the same value as the limit. Pods that belong to this QoS class will never be killed by the scheduler because of resources running out on a Node. Note If a container only defines its limits, Kubernetes automatically assigns a request that matches the limit. The Burstable QoS class means that limits and requests on a container are set, but they are different. It is enough to define limits and requests on one container of a Pod even though there might be more, and it also only has to define limits and requests on memory or CPU, not necessarily both. The BestEffort QoS class applies to Pods that do not define any limits and requests at all on any containers. As its class name suggests, these are the kinds of Pods that will be killed by the scheduler first if a Node runs out of memory or CPU. As you might have already guessed by now, if there are no BestEffort QoS Pods, the scheduler will begin to kill Pods belonging to the class of Burstable. A Node hosting only Pods of class Guaranteed will (theoretically) never run out of resources. For more examples have a look at the Kubernetes documentation about Quality of Service . LimitRanges As you now know what limits and requests are, we can come back to the statement made above: Quote As we\u2019ve already seen, compute ResourceQuotas limit the amount of memory and CPU we can use in a Namespace. Only defining a ResourceQuota, however is not going to have an effect on Pods that don\u2019t define the amount of resources they want to use. This is where the concept of limits and requests comes into play. So, if a cluster administrator wanted to make sure that every Pod in the cluster counted against the compute ResourceQuota, the administrator would have to have a way of defining some kind of default limits and requests that were applied if none were defined in the containers. This is exactly what LimitRanges are for. Quoting the Kubernetes documentation , LimitRanges can be used to: Enforce minimum and maximum compute resource usage per Pod or container in a Namespace Enforce minimum and maximum storage request per PersistentVolumeClaim in a Namespace Enforce a ratio between request and limit for a resource in a Namespace Set default request/limit for compute resources in a Namespace and automatically inject them to containers at runtime If for example a container did not define any requests or limits and there was a LimitRange defining the default values, these default values would be used when deploying said container. However, as soon as limits or requests were defined, the default values would no longer be applied. The possibility of enforcing minimum and maximum resources and defining ResourceQuotas per Namespace allows for many combinations of resource control. Task 1 : Play with ResourceQuotas and LimitRanges In this task, we will play around with ResourceQuotas and LimitRanges to get a better understanding of how they work. Create a Namespace First, we will create a new Namespace for our experiments: export NAMESPACE = <username>-quota kubectl create namespace $NAMESPACE Create a ResourceQuota Next, we will create a ResourceQuota for our Namespace: kubectl apply -f - <<EOF apiVersion: v1 kind: ResourceQuota metadata: name: lr-quota namespace: $NAMESPACE spec: hard: limits.cpu: \"1\" limits.memory: 1Gi requests.cpu: \"1\" requests.memory: 1Gi EOF Create a LimitRange Now, we will create a LimitRange for our Namespace: kubectl apply -f - <<EOF apiVersion: v1 kind: LimitRange metadata: name: lr-range namespace: $NAMESPACE spec: limits: - default: cpu: 500m memory: 200Mi defaultRequest: cpu: 500m memory: 200Mi type: Container EOF Create a Pod We create a Pod that does uses a lot of resources: kubectl apply -f - <<EOF apiVersion: v1 kind: Pod metadata: name: lr-demo namespace: $NAMESPACE spec: containers: - name: lr-demo-ctr image: bretfisher/stress:2cpu1024m EOF Do you see any problems with the Pod? If not, let\u2019s have a look at the events: kubectl get pods -n $NAMESPACE kubectl get events -n $NAMESPACE See the error message? It says that the Pod is in a state of OOMKilled which stands for \u201cOut of Memory Killed\u201d. This is because the Pod requested more memory than the ResourceQuota allows. The Pod was killed by the scheduler.","title":"ResourceQuotas and Limits"},{"location":"kubernetes-basics/advanced-concepts/resource-quotas/#resourcequotas-and-limitranges","text":"In this tutorial, we are going to look at ResourceQuotas and LimitRanges. As Kubernetes users, we are most certainly going to encounter the limiting effects that ResourceQuotas and LimitRanges impose. Warning For this lab to work it is vital that you create and use the namespace <username>-quota !","title":"ResourceQuotas and LimitRanges"},{"location":"kubernetes-basics/advanced-concepts/resource-quotas/#resourcequotas","text":"ResourceQuotas among other things limit the amount of resources Pods can use in a Namespace. They can also be used to limit the total number of a certain resource type in a Namespace. In more detail, there are these kinds of quotas: Compute ResourceQuotas can be used to limit the amount of memory and CPU Storage ResourceQuotas can be used to limit the total amount of storage and the number of PersistentVolumeClaims, generally or specific to a StorageClass Object count quotas can be used to limit the number of a certain resource type such as Services, Pods or Secrets Defining ResourceQuotas makes sense when the cluster administrators want to have better control over consumed resources. A typical use case are public offerings where users pay for a certain guaranteed amount of resources which must not be exceeded. In order to check for defined quotas in your Namespace, simply see if there are any of type ResourceQuota: kubectl get resourcequota --namespace <namespace> To show in detail what kinds of limits the quota imposes: kubectl describe resourcequota <quota-name> --namespace <namespace> For more details, have look at Kubernetes\u2019 documentation about resource quotas .","title":"ResourceQuotas"},{"location":"kubernetes-basics/advanced-concepts/resource-quotas/#requests-and-limits","text":"As we\u2019ve already seen, compute ResourceQuotas limit the amount of memory and CPU we can use in a Namespace. Only defining a ResourceQuota, however is not going to have an effect on Pods that don\u2019t define the amount of resources they want to use. This is where the concept of limits and requests comes into play. Limits and requests on a Pod, or rather on a container in a Pod, define how much memory and CPU this container wants to consume at least (request) and at most (limit). Requests mean that the container will be guaranteed to get at least this amount of resources, limits represent the upper boundary which cannot be crossed. Defining these values helps Kubernetes in determining on which Node to schedule the Pod because it knows how many resources should be available for it. Note Containers using more CPU time than what their limit allows will be throttled. Containers using more memory than what they are allowed to use will be killed. Defining limits and requests on a Pod that has one container looks like this: apiVersion : v1 kind : Pod metadata : name : lr-demo namespace : lr-example spec : containers : - name : lr-demo-ctr image : docker.io/nginxinc/nginx-unprivileged:1.18-alpine resources : limits : memory : \"200Mi\" cpu : \"700m\" requests : memory : \"200Mi\" cpu : \"700m\" You can see the familiar binary unit \u201cMi\u201d is used for the memory value. Other binary (\u201cGi\u201d, \u201cKi\u201d, \u2026) or decimal units (\u201cM\u201d, \u201cG\u201d, \u201cK\u201d, \u2026) can be used as well. The CPU value is denoted as \u201cm\u201d. \u201cm\u201d stands for millicpu or sometimes also referred to as millicores where \" 1000m \" is equal to one core/vCPU/hyperthread.","title":"Requests and limits"},{"location":"kubernetes-basics/advanced-concepts/resource-quotas/#quality-of-service","text":"Setting limits and requests on containers has yet another effect: It might change the Pod\u2019s Quality of Service class. There are three such QoS classes: Guaranteed Burstable BestEffort The Guaranteed QoS class is applied to Pods that define both limits and requests for both memory and CPU resources on all their containers. The most important part is that each request has the same value as the limit. Pods that belong to this QoS class will never be killed by the scheduler because of resources running out on a Node. Note If a container only defines its limits, Kubernetes automatically assigns a request that matches the limit. The Burstable QoS class means that limits and requests on a container are set, but they are different. It is enough to define limits and requests on one container of a Pod even though there might be more, and it also only has to define limits and requests on memory or CPU, not necessarily both. The BestEffort QoS class applies to Pods that do not define any limits and requests at all on any containers. As its class name suggests, these are the kinds of Pods that will be killed by the scheduler first if a Node runs out of memory or CPU. As you might have already guessed by now, if there are no BestEffort QoS Pods, the scheduler will begin to kill Pods belonging to the class of Burstable. A Node hosting only Pods of class Guaranteed will (theoretically) never run out of resources. For more examples have a look at the Kubernetes documentation about Quality of Service .","title":"Quality of service"},{"location":"kubernetes-basics/advanced-concepts/resource-quotas/#limitranges","text":"As you now know what limits and requests are, we can come back to the statement made above: Quote As we\u2019ve already seen, compute ResourceQuotas limit the amount of memory and CPU we can use in a Namespace. Only defining a ResourceQuota, however is not going to have an effect on Pods that don\u2019t define the amount of resources they want to use. This is where the concept of limits and requests comes into play. So, if a cluster administrator wanted to make sure that every Pod in the cluster counted against the compute ResourceQuota, the administrator would have to have a way of defining some kind of default limits and requests that were applied if none were defined in the containers. This is exactly what LimitRanges are for. Quoting the Kubernetes documentation , LimitRanges can be used to: Enforce minimum and maximum compute resource usage per Pod or container in a Namespace Enforce minimum and maximum storage request per PersistentVolumeClaim in a Namespace Enforce a ratio between request and limit for a resource in a Namespace Set default request/limit for compute resources in a Namespace and automatically inject them to containers at runtime If for example a container did not define any requests or limits and there was a LimitRange defining the default values, these default values would be used when deploying said container. However, as soon as limits or requests were defined, the default values would no longer be applied. The possibility of enforcing minimum and maximum resources and defining ResourceQuotas per Namespace allows for many combinations of resource control.","title":"LimitRanges"},{"location":"kubernetes-basics/advanced-concepts/resource-quotas/#task-1-play-with-resourcequotas-and-limitranges","text":"In this task, we will play around with ResourceQuotas and LimitRanges to get a better understanding of how they work.","title":" Task 1: Play with ResourceQuotas and LimitRanges"},{"location":"kubernetes-basics/advanced-concepts/resource-quotas/#create-a-namespace","text":"First, we will create a new Namespace for our experiments: export NAMESPACE = <username>-quota kubectl create namespace $NAMESPACE","title":"Create a Namespace"},{"location":"kubernetes-basics/advanced-concepts/resource-quotas/#create-a-resourcequota","text":"Next, we will create a ResourceQuota for our Namespace: kubectl apply -f - <<EOF apiVersion: v1 kind: ResourceQuota metadata: name: lr-quota namespace: $NAMESPACE spec: hard: limits.cpu: \"1\" limits.memory: 1Gi requests.cpu: \"1\" requests.memory: 1Gi EOF","title":"Create a ResourceQuota"},{"location":"kubernetes-basics/advanced-concepts/resource-quotas/#create-a-limitrange","text":"Now, we will create a LimitRange for our Namespace: kubectl apply -f - <<EOF apiVersion: v1 kind: LimitRange metadata: name: lr-range namespace: $NAMESPACE spec: limits: - default: cpu: 500m memory: 200Mi defaultRequest: cpu: 500m memory: 200Mi type: Container EOF","title":"Create a LimitRange"},{"location":"kubernetes-basics/advanced-concepts/resource-quotas/#create-a-pod","text":"We create a Pod that does uses a lot of resources: kubectl apply -f - <<EOF apiVersion: v1 kind: Pod metadata: name: lr-demo namespace: $NAMESPACE spec: containers: - name: lr-demo-ctr image: bretfisher/stress:2cpu1024m EOF Do you see any problems with the Pod? If not, let\u2019s have a look at the events: kubectl get pods -n $NAMESPACE kubectl get events -n $NAMESPACE See the error message? It says that the Pod is in a state of OOMKilled which stands for \u201cOut of Memory Killed\u201d. This is because the Pod requested more memory than the ResourceQuota allows. The Pod was killed by the scheduler.","title":"Create a Pod"},{"location":"kubernetes-basics/advanced-concepts/sidecars/","text":"Sidecar Containers Environment Variables We are going to use some environment variables in this tutorial. Please make sure you have set them correctly. # check if the environment variables are set if not set them export NAMESPACE = <namespace> echo $NAMESPACE Let\u2019s first have another look at the Pod\u2019s description on the Kubernetes documentation page : Quote A Pod (as in a pod of whales or pea pod) is a group of one or more containers (such as Docker containers), with shared storage/network, and a specification for how to run the containers. A Pod\u2019s contents are always co-located and co-scheduled, and run in a shared context. A Pod models an application-specific \u201clogical host\u201d - it contains one or more application containers which are relatively tightly coupled \u2014 in a pre-container world, being executed on the same physical or virtual machine would mean being executed on the same logical host. The shared context of a Pod is a set of Linux namespaces, cgroups, and potentially other facets of isolation - the same things that isolate a Docker container. Within a Pod\u2019s context, the individual applications may have further sub-isolations applied. A sidecar container is a utility container in the Pod. Its purpose is to support the main container. It is important to note that the standalone sidecar container does not serve any purpose, it must be paired with one or more main containers. Generally, sidecar containers are reusable and can be paired with numerous types of main containers. In a sidecar pattern, the functionality of the main container is extended or enhanced by a sidecar container without strong coupling between the two. Although it is always possible to build sidecar container functionality into the main container, there are several benefits with this pattern: Different resource profiles, i.e. independent resource accounting and allocation Clear separation of concerns at packaging level, i.e. no strong coupling between containers Reusability, i.e., sidecar containers can be paired with numerous \u201cmain\u201d containers Failure containment boundary, making it possible for the overall system to degrade gracefully Independent testing, packaging, upgrade, deployment and if necessary rollback Task 1 : Create a sidecar container We want to create a Pod that runs a sidecar container that reads a log file and prints it to stdout. The main container writes a log file every seconds. Create a file called sidecar-pod.yaml with the following content: apiVersion : v1 kind : Pod metadata : name : sidecar-demo spec : initContainers : - name : init image : busybox command : - 'sh' - '-c' - 'echo \"init\" >> /work-dir/test.log' volumeMounts : - name : workdir mountPath : /work-dir containers : - name : logwriter image : busybox command : - 'sh' - '-c' - 'while true; do printf \"%s %s\\n\" \"$(date)\" >> /work-dir/test.log; sleep 1; done' resources : requests : cpu : 10m memory : 16Mi limits : cpu : 20m memory : 32Mi volumeMounts : - name : workdir mountPath : /work-dir - name : logreader image : busybox command : - 'sh' - '-c' - 'while true; do cat /work-dir/test.log; sleep 1; done' resources : requests : cpu : 10m memory : 16Mi limits : cpu : 20m memory : 32Mi volumeMounts : - name : workdir mountPath : /work-dir volumes : - name : workdir emptyDir : {} Apply the file: kubectl apply -f sidecar-pod.yaml --namespace $NAMESPACE Check the logs of the logreader container: kubectl logs sidecar-demo logreader --namespace $NAMESPACE --follow You should see the log file being printed to stdout every second. Note The --follow flag tells kubectl to keep the connection open and stream the logs.","title":"Sidecars"},{"location":"kubernetes-basics/advanced-concepts/sidecars/#sidecar-containers","text":"Environment Variables We are going to use some environment variables in this tutorial. Please make sure you have set them correctly. # check if the environment variables are set if not set them export NAMESPACE = <namespace> echo $NAMESPACE Let\u2019s first have another look at the Pod\u2019s description on the Kubernetes documentation page : Quote A Pod (as in a pod of whales or pea pod) is a group of one or more containers (such as Docker containers), with shared storage/network, and a specification for how to run the containers. A Pod\u2019s contents are always co-located and co-scheduled, and run in a shared context. A Pod models an application-specific \u201clogical host\u201d - it contains one or more application containers which are relatively tightly coupled \u2014 in a pre-container world, being executed on the same physical or virtual machine would mean being executed on the same logical host. The shared context of a Pod is a set of Linux namespaces, cgroups, and potentially other facets of isolation - the same things that isolate a Docker container. Within a Pod\u2019s context, the individual applications may have further sub-isolations applied. A sidecar container is a utility container in the Pod. Its purpose is to support the main container. It is important to note that the standalone sidecar container does not serve any purpose, it must be paired with one or more main containers. Generally, sidecar containers are reusable and can be paired with numerous types of main containers. In a sidecar pattern, the functionality of the main container is extended or enhanced by a sidecar container without strong coupling between the two. Although it is always possible to build sidecar container functionality into the main container, there are several benefits with this pattern: Different resource profiles, i.e. independent resource accounting and allocation Clear separation of concerns at packaging level, i.e. no strong coupling between containers Reusability, i.e., sidecar containers can be paired with numerous \u201cmain\u201d containers Failure containment boundary, making it possible for the overall system to degrade gracefully Independent testing, packaging, upgrade, deployment and if necessary rollback","title":"Sidecar Containers"},{"location":"kubernetes-basics/advanced-concepts/sidecars/#task-1-create-a-sidecar-container","text":"We want to create a Pod that runs a sidecar container that reads a log file and prints it to stdout. The main container writes a log file every seconds. Create a file called sidecar-pod.yaml with the following content: apiVersion : v1 kind : Pod metadata : name : sidecar-demo spec : initContainers : - name : init image : busybox command : - 'sh' - '-c' - 'echo \"init\" >> /work-dir/test.log' volumeMounts : - name : workdir mountPath : /work-dir containers : - name : logwriter image : busybox command : - 'sh' - '-c' - 'while true; do printf \"%s %s\\n\" \"$(date)\" >> /work-dir/test.log; sleep 1; done' resources : requests : cpu : 10m memory : 16Mi limits : cpu : 20m memory : 32Mi volumeMounts : - name : workdir mountPath : /work-dir - name : logreader image : busybox command : - 'sh' - '-c' - 'while true; do cat /work-dir/test.log; sleep 1; done' resources : requests : cpu : 10m memory : 16Mi limits : cpu : 20m memory : 32Mi volumeMounts : - name : workdir mountPath : /work-dir volumes : - name : workdir emptyDir : {} Apply the file: kubectl apply -f sidecar-pod.yaml --namespace $NAMESPACE Check the logs of the logreader container: kubectl logs sidecar-demo logreader --namespace $NAMESPACE --follow You should see the log file being printed to stdout every second. Note The --follow flag tells kubectl to keep the connection open and stream the logs.","title":" Task 1: Create a sidecar container"},{"location":"kubernetes-basics/advanced-concepts/statefulsets/","text":"StatefulSets Environment Variables We are going to use some environment variables in this tutorial. Please make sure you have set them correctly. # check if the environment variables are set if not set them export NAMESPACE = <namespace> echo $NAMESPACE Stateless applications or applications with a stateful backend can be described as Deployments. However, sometimes your application has to be stateful. Examples would be an application that needs a static, non-changing hostname every time it starts or a clustered application with a strict start/stop order of its services (e.g. RabbitMQ). These features are offered by StatefulSets. Note This tutorial does not depend on the previous ones. Consistent hostnames While in normal Deployments a hash-based name of the Pods (also represented as the hostname inside the Pod) is generated, StatefulSets create Pods with preconfigured names. An example of a RabbitMQ cluster with three instances (Pods) could look like this: rabbitmq-0 rabbitmq-1 rabbitmq-2 Scaling Scaling is handled differently in StatefulSets. When scaling up from 3 to 5 replicas in a Deployment, two additional Pods are started at the same time (based on the configuration). Using a StatefulSet, scaling is done serially: Let\u2019s use our RabbitMQ example again: The StatefulSet is scaled up using: kubectl scale deployment rabbitmq --replicas=5 --namespace $NAMESPACE rabbitmq-3 is started As soon as Pod rabbitmq-3 is in Ready state the same procedure starts for rabbitmq-4 When scaling down, the order is inverted. The highest-numbered Pod will be stopped first. As soon as it has finished terminating the now highest-numbered Pod is stopped. This procedure is repeated as long as the desired number of replicas has not been reached. Update procedure During an update of an application with a StatefulSet the highest-numbered Pod will be the first to be updated and only after a successful start the next Pod follows. Highest-numbered Pod is stopped New Pod (with new image tag) is started If the new Pod successfully starts, the procedure is repeated for the second highest-numbered Pod And so on If the start of a new Pod fails, the update will be interrupted so that the architecture of your application won\u2019t break. Dedicated persistent volumes A very convenient feature is that unlike a Deployment a StatefulSet makes it possible to attach a different, dedicated persistent volume to each of its Pods. This is done using a so-called VolumeClaimTemplate. This spares you from defining identical Deployments with 1 replica each but different volumes. Conclusion The controllable and predictable behavior can be a perfect match for applications such as RabbitMQ or etcd, as you need unique names for such application clusters. Task 1 : Create a StatefulSet Create a file named sts_nginx-cluster.yaml with the following definition of a StatefulSet: apiVersion : apps/v1 kind : StatefulSet metadata : name : nginx-cluster spec : serviceName : \"nginx\" replicas : 1 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginxinc/nginx-unprivileged:1.18-alpine ports : - containerPort : 80 name : nginx resources : limits : cpu : 40m memory : 64Mi requests : cpu : 10m memory : 32Mi Create the StatefulSet: kubectl apply -f sts_nginx-cluster.yaml --namespace $NAMESPACE To watch the pods\u2019 progress, open a second console and execute the watch command: kubectl get pods --selector app = nginx -w --namespace $NAMESPACE Note Friendly reminder that the kubectl get -w command will never end unless you terminate it with CTRL-c . Task 2 : Scale the StatefulSet Scale the StatefulSet up: kubectl scale statefulset nginx-cluster --replicas = 3 --namespace $NAMESPACE You can again watch the pods\u2019 progress like you did in the first task. Task 3 : Update the StatefulSet In order to update the image tag in use in a StatefulSet, you can use the kubectl set image command. Set the StatefulSet\u2019s image tag to latest : kubectl set image statefulset nginx-cluster nginx = docker.io/nginxinc/nginx-unprivileged:latest --namespace $NAMESPACE Task 4 : Rollback Imagine you just realized that switching to the latest image tag was an awful idea (because it is generally not advisable). Rollback the change: kubectl rollout undo statefulset nginx-cluster --namespace $NAMESPACE Task 5 : Cleanup As with every other Kubernetes resource you can delete the StatefulSet with: kubectl delete statefulset nginx-cluster --namespace $NAMESPACE Further information can be found in the Kubernetes\u2019 StatefulSet documentation or this published article .","title":"StatefulSets"},{"location":"kubernetes-basics/advanced-concepts/statefulsets/#statefulsets","text":"Environment Variables We are going to use some environment variables in this tutorial. Please make sure you have set them correctly. # check if the environment variables are set if not set them export NAMESPACE = <namespace> echo $NAMESPACE Stateless applications or applications with a stateful backend can be described as Deployments. However, sometimes your application has to be stateful. Examples would be an application that needs a static, non-changing hostname every time it starts or a clustered application with a strict start/stop order of its services (e.g. RabbitMQ). These features are offered by StatefulSets. Note This tutorial does not depend on the previous ones.","title":"StatefulSets"},{"location":"kubernetes-basics/advanced-concepts/statefulsets/#consistent-hostnames","text":"While in normal Deployments a hash-based name of the Pods (also represented as the hostname inside the Pod) is generated, StatefulSets create Pods with preconfigured names. An example of a RabbitMQ cluster with three instances (Pods) could look like this: rabbitmq-0 rabbitmq-1 rabbitmq-2","title":"Consistent hostnames"},{"location":"kubernetes-basics/advanced-concepts/statefulsets/#scaling","text":"Scaling is handled differently in StatefulSets. When scaling up from 3 to 5 replicas in a Deployment, two additional Pods are started at the same time (based on the configuration). Using a StatefulSet, scaling is done serially: Let\u2019s use our RabbitMQ example again: The StatefulSet is scaled up using: kubectl scale deployment rabbitmq --replicas=5 --namespace $NAMESPACE rabbitmq-3 is started As soon as Pod rabbitmq-3 is in Ready state the same procedure starts for rabbitmq-4 When scaling down, the order is inverted. The highest-numbered Pod will be stopped first. As soon as it has finished terminating the now highest-numbered Pod is stopped. This procedure is repeated as long as the desired number of replicas has not been reached.","title":"Scaling"},{"location":"kubernetes-basics/advanced-concepts/statefulsets/#update-procedure","text":"During an update of an application with a StatefulSet the highest-numbered Pod will be the first to be updated and only after a successful start the next Pod follows. Highest-numbered Pod is stopped New Pod (with new image tag) is started If the new Pod successfully starts, the procedure is repeated for the second highest-numbered Pod And so on If the start of a new Pod fails, the update will be interrupted so that the architecture of your application won\u2019t break.","title":"Update procedure"},{"location":"kubernetes-basics/advanced-concepts/statefulsets/#dedicated-persistent-volumes","text":"A very convenient feature is that unlike a Deployment a StatefulSet makes it possible to attach a different, dedicated persistent volume to each of its Pods. This is done using a so-called VolumeClaimTemplate. This spares you from defining identical Deployments with 1 replica each but different volumes.","title":"Dedicated persistent volumes"},{"location":"kubernetes-basics/advanced-concepts/statefulsets/#conclusion","text":"The controllable and predictable behavior can be a perfect match for applications such as RabbitMQ or etcd, as you need unique names for such application clusters.","title":"Conclusion"},{"location":"kubernetes-basics/advanced-concepts/statefulsets/#task-1-create-a-statefulset","text":"Create a file named sts_nginx-cluster.yaml with the following definition of a StatefulSet: apiVersion : apps/v1 kind : StatefulSet metadata : name : nginx-cluster spec : serviceName : \"nginx\" replicas : 1 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginxinc/nginx-unprivileged:1.18-alpine ports : - containerPort : 80 name : nginx resources : limits : cpu : 40m memory : 64Mi requests : cpu : 10m memory : 32Mi Create the StatefulSet: kubectl apply -f sts_nginx-cluster.yaml --namespace $NAMESPACE To watch the pods\u2019 progress, open a second console and execute the watch command: kubectl get pods --selector app = nginx -w --namespace $NAMESPACE Note Friendly reminder that the kubectl get -w command will never end unless you terminate it with CTRL-c .","title":" Task 1: Create a StatefulSet"},{"location":"kubernetes-basics/advanced-concepts/statefulsets/#task-2-scale-the-statefulset","text":"Scale the StatefulSet up: kubectl scale statefulset nginx-cluster --replicas = 3 --namespace $NAMESPACE You can again watch the pods\u2019 progress like you did in the first task.","title":" Task 2: Scale the StatefulSet"},{"location":"kubernetes-basics/advanced-concepts/statefulsets/#task-3-update-the-statefulset","text":"In order to update the image tag in use in a StatefulSet, you can use the kubectl set image command. Set the StatefulSet\u2019s image tag to latest : kubectl set image statefulset nginx-cluster nginx = docker.io/nginxinc/nginx-unprivileged:latest --namespace $NAMESPACE","title":" Task 3: Update the StatefulSet"},{"location":"kubernetes-basics/advanced-concepts/statefulsets/#task-4-rollback","text":"Imagine you just realized that switching to the latest image tag was an awful idea (because it is generally not advisable). Rollback the change: kubectl rollout undo statefulset nginx-cluster --namespace $NAMESPACE","title":" Task 4: Rollback"},{"location":"kubernetes-basics/advanced-concepts/statefulsets/#task-5-cleanup","text":"As with every other Kubernetes resource you can delete the StatefulSet with: kubectl delete statefulset nginx-cluster --namespace $NAMESPACE Further information can be found in the Kubernetes\u2019 StatefulSet documentation or this published article .","title":" Task 5: Cleanup"},{"location":"kubernetes-basics/deployment-strategies/","text":"Deployment Strategies You learned some of the basics of Kubernetes in the previous modules. With this knowledge, you can deploy an application to a cluster. In this module, you can play with different deployment strategies. Resources Deployment Strategies Examples Task 1 : Apply deployment strategies Apply some deployment strategies to the application which we deployed in the previous module. for example: Scaling Applications","title":"Deployment Strategies"},{"location":"kubernetes-basics/deployment-strategies/#deployment-strategies","text":"You learned some of the basics of Kubernetes in the previous modules. With this knowledge, you can deploy an application to a cluster. In this module, you can play with different deployment strategies. Resources Deployment Strategies Examples","title":"Deployment Strategies"},{"location":"kubernetes-basics/deployment-strategies/#task-1-apply-deployment-strategies","text":"Apply some deployment strategies to the application which we deployed in the previous module. for example: Scaling Applications","title":" Task 1: Apply deployment strategies"},{"location":"kubernetes-basics/helm/","text":"Helm Helm is a Cloud Native Foundation project to define, install and manage applications in Kubernetes. Helm is a Package Manager for Kubernetes package multiple K8s resources into a single logical deployment unit \u2026 but it\u2019s not just a Package Manager Helm is a Deployment Management for Kubernetes do a repeatable deployment manage dependencies: reuse and share manage multiple configurations update, rollback and test application deployments Overview Ok, let\u2019s start with Helm. First, you have to understand the following 3 Helm concepts: Chart , Repository and Release . A Chart is a Helm package. It contains all of the resource definitions necessary to run an application, tool, or service inside of a Kubernetes cluster. Think of it like the Kubernetes equivalent of a Homebrew formula, an Apt dpkg, or a Yum RPM file. A Repository is the place where charts can be collected and shared. It\u2019s like Perl\u2019s CPAN archive or the Fedora Package Database, but for Kubernetes packages. A Release is an instance of a chart running in a Kubernetes cluster. One chart can often be installed many times in the same cluster. Each time it is installed, a new release is created. Consider a MySQL chart. If you want two databases running in your cluster, you can install that chart twice. Each one will have its own release, which will in turn have its own release name. With these concepts in mind, we can now explain Helm like this: Quote Helm installs charts into Kubernetes, creating a new release for each installation. To find new charts, you can search Helm chart repositories. Installation This guide shows you how to install the helm CLI tool. helm can be installed either from source or from pre-built binary releases. We are going to use the pre-built releases. helm binaries can be found on Helm\u2019s release page for the usual variety of operating systems. Task 1 : Install CLI Install the CLI for your Operating System Verify the installation To verify, run the following command and check if Version is what you expected: helm version The output is similar to this: version.BuildInfo{Version:\"v3.10.0\", GitCommit:\"ce66412a723e4d89555dc67217607c6579ffcb21\", GitTreeState:\"clean\", GoVersion:\"go1.19.1\"} From here on you should be able to run the client.","title":"Helm"},{"location":"kubernetes-basics/helm/#helm","text":"Helm is a Cloud Native Foundation project to define, install and manage applications in Kubernetes. Helm is a Package Manager for Kubernetes package multiple K8s resources into a single logical deployment unit \u2026 but it\u2019s not just a Package Manager Helm is a Deployment Management for Kubernetes do a repeatable deployment manage dependencies: reuse and share manage multiple configurations update, rollback and test application deployments","title":"Helm"},{"location":"kubernetes-basics/helm/#overview","text":"Ok, let\u2019s start with Helm. First, you have to understand the following 3 Helm concepts: Chart , Repository and Release . A Chart is a Helm package. It contains all of the resource definitions necessary to run an application, tool, or service inside of a Kubernetes cluster. Think of it like the Kubernetes equivalent of a Homebrew formula, an Apt dpkg, or a Yum RPM file. A Repository is the place where charts can be collected and shared. It\u2019s like Perl\u2019s CPAN archive or the Fedora Package Database, but for Kubernetes packages. A Release is an instance of a chart running in a Kubernetes cluster. One chart can often be installed many times in the same cluster. Each time it is installed, a new release is created. Consider a MySQL chart. If you want two databases running in your cluster, you can install that chart twice. Each one will have its own release, which will in turn have its own release name. With these concepts in mind, we can now explain Helm like this: Quote Helm installs charts into Kubernetes, creating a new release for each installation. To find new charts, you can search Helm chart repositories.","title":"Overview"},{"location":"kubernetes-basics/helm/#installation","text":"This guide shows you how to install the helm CLI tool. helm can be installed either from source or from pre-built binary releases. We are going to use the pre-built releases. helm binaries can be found on Helm\u2019s release page for the usual variety of operating systems.","title":"Installation"},{"location":"kubernetes-basics/helm/#task-1-install-cli","text":"Install the CLI for your Operating System","title":" Task 1: Install CLI"},{"location":"kubernetes-basics/helm/#verify-the-installation","text":"To verify, run the following command and check if Version is what you expected: helm version The output is similar to this: version.BuildInfo{Version:\"v3.10.0\", GitCommit:\"ce66412a723e4d89555dc67217607c6579ffcb21\", GitTreeState:\"clean\", GoVersion:\"go1.19.1\"} From here on you should be able to run the client.","title":"Verify the installation"},{"location":"kubernetes-basics/helm/helm-charts/","text":"Helm Charts Environment Variables We are going to use some environment variables in this tutorial. Please make sure you have set them correctly. # check if the environment variables are set if not set them export NAMESPACE = <namespace> echo $NAMESPACE export URL = ${ NAMESPACE } .k8s.golog.ch echo $URL In this tutorial we are going to create our very first Helm chart and deploy it. Task 1 : Create a Chart First, let\u2019s create our chart. Open your favorite terminal and make sure you\u2019re in the workspace for this lab, e.g. cd ~/<workspace-kubernetes-training> : helm create mychart You will now find a mychart directory with the newly created chart. It already is a valid and fully functional chart which deploys an nginx instance. Have a look at the generated files and their content. For an explanation of the files, visit the Helm Developer Documentation . In a later section you\u2019ll find all the information about Helm templates. Task 2 : Install the Chart Before actually deploying our generated chart, we can check the (to be) generated Kubernetes resources with the following command: helm install --dry-run --debug --namespace $NAMESPACE myfirstrelease ./mychart Finally, the following command creates a new release and deploys the application: helm install --namespace $NAMESPACE myfirstrelease ./mychart With kubectl get pods --namespace $NAMESPACE you should see a new Pod: NAME READY STATUS RESTARTS AGE myfirstrelease-mychart-4d5956b75-nd8jd 1/1 Running 0 2m21s You can list the newly created Helm release with the following command: helm list --namespace $NAMESPACE Task 3 : Upgrade the Chart Our freshly deployed nginx is not yet accessible from outside the Kubernetes cluster. To expose it, we have to make sure a so called ingress resource will be deployed as well. Also make sure the application is accessible via TLS. A look into the file templates/ingress.yaml reveals that the rendering of the ingress and its values is configurable through values( values.yaml ): {{ - if .Values.ingress.enabled - }} {{ - $fullName : = include \"mychart.fullname\" . - }} {{ - $svcPort : = .Values.service.port - }} {{ - if and .Values.ingress.className (not (semverCompare \">=1.18-0\" .Capabilities.KubeVersion.GitVersion)) }} {{ - if not (hasKey .Values.ingress.annotations \"kubernetes.io/ingress.class\") }} {{ - $_ : = set .Values.ingress.annotations \"kubernetes.io/ingress.class\" .Values.ingress.className }} {{ - end }} {{ - end }} {{ - if semverCompare \">=1.19-0\" .Capabilities.KubeVersion.GitVersion - }} apiVersion : networking.k8s.io/v1 {{ - else if semverCompare \">=1.14-0\" .Capabilities.KubeVersion.GitVersion - }} apiVersion : networking.k8s.io/v1beta1 {{ - else - }} apiVersion : extensions/v1beta1 {{ - end }} kind : Ingress metadata : name : {{ $fullName }} labels : {{ - include \"mychart.labels\" . | nindent 4 }} {{ - with .Values.ingress.annotations }} annotations : {{ - toYaml . | nindent 4 }} {{ - end }} spec : {{ - if and .Values.ingress.className (semverCompare \">=1.18-0\" .Capabilities.KubeVersion.GitVersion) }} ingressClassName : {{ .Values.ingress.className }} {{ - end }} {{ - if .Values.ingress.tls }} tls : {{ - range .Values.ingress.tls }} - hosts : {{ - range .hosts }} - {{ . | quote }} {{ - end }} secretName : {{ .secretName }} {{ - end }} {{ - end }} rules : {{ - range .Values.ingress.hosts }} - host : {{ .host | quote }} http : paths : {{ - range .paths }} - path : {{ .path }} {{ - if and .pathType (semverCompare \">=1.18-0\" $.Capabilities.KubeVersion.GitVersion) }} pathType : {{ .pathType }} {{ - end }} backend : {{ - if semverCompare \">=1.19-0\" $.Capabilities.KubeVersion.GitVersion }} service : name : {{ $fullName }} port : number : {{ $svcPort }} {{ - else }} serviceName : {{ $fullName }} servicePort : {{ $svcPort }} {{ - end }} {{ - end }} {{ - end }} {{ - end }} Thus, we need to change this value inside our mychart/values.yaml file. This is also where we enable the TLS part: Note Make sure to replace the <url> and <namespace> accordingly. [ ... ] ingress : enabled : true annotations : kubernetes.io/ingress.class : nginx kubernetes.io/tls-acme : \"true\" cert-manager.io/cluster-issuer : letsencrypt-prod nginx.ingress.kubernetes.io/add-base-url : \"true\" hosts : - host : <url> paths : - path : / pathType : ImplementationSpecific tls : - secretName : <namespace>-tls hosts : - <url> [ ... ] Note Make sure to set the proper value as hostname. <namespace> and <url> will be provided by the trainer. Before we can upgrade, make sure to delete the old ingress resource: kubectl get ingress --namespace $NAMESPACE kubectl delete ingress <ingress-name> --namespace $NAMESPACE Apply the change by upgrading our release: helm upgrade --namespace $NAMESPACE myfirstrelease ./mychart Check whether the ingress was successfully deployed by accessing the URL. Task 4 : Overwrite value using commandline param An alternative way to set or overwrite values for charts we want to deploy is the --set name=value parameter. This parameter can be used when installing a chart as well as upgrading. Update the replica count of your nginx Deployment to 2 using --set name=value helm upgrade --namespace $NAMESPACE --set replicaCount = 2 myfirstrelease ./mychart Task 5 : values.yaml Have a look at the values.yaml file in your chart and study all the possible configuration params introduced in a freshly created chart. Task 6 : Uninstall the Chart To remove an application, simply remove the Helm release with the following command: helm uninstall --namespace $NAMESPACE myfirstrelease Do this with our deployed release. With kubectl get pods --namespace $NAMESPACE you should no longer see your application Pod. Further Reading For creating a nice README.md for your chart, have a look at this tool. Also check out the Artifact Hub for finding Helm charts. You can also publish your own charts there.","title":"Helm Charts"},{"location":"kubernetes-basics/helm/helm-charts/#helm-charts","text":"Environment Variables We are going to use some environment variables in this tutorial. Please make sure you have set them correctly. # check if the environment variables are set if not set them export NAMESPACE = <namespace> echo $NAMESPACE export URL = ${ NAMESPACE } .k8s.golog.ch echo $URL In this tutorial we are going to create our very first Helm chart and deploy it.","title":"Helm Charts"},{"location":"kubernetes-basics/helm/helm-charts/#task-1-create-a-chart","text":"First, let\u2019s create our chart. Open your favorite terminal and make sure you\u2019re in the workspace for this lab, e.g. cd ~/<workspace-kubernetes-training> : helm create mychart You will now find a mychart directory with the newly created chart. It already is a valid and fully functional chart which deploys an nginx instance. Have a look at the generated files and their content. For an explanation of the files, visit the Helm Developer Documentation . In a later section you\u2019ll find all the information about Helm templates.","title":" Task 1: Create a Chart"},{"location":"kubernetes-basics/helm/helm-charts/#task-2-install-the-chart","text":"Before actually deploying our generated chart, we can check the (to be) generated Kubernetes resources with the following command: helm install --dry-run --debug --namespace $NAMESPACE myfirstrelease ./mychart Finally, the following command creates a new release and deploys the application: helm install --namespace $NAMESPACE myfirstrelease ./mychart With kubectl get pods --namespace $NAMESPACE you should see a new Pod: NAME READY STATUS RESTARTS AGE myfirstrelease-mychart-4d5956b75-nd8jd 1/1 Running 0 2m21s You can list the newly created Helm release with the following command: helm list --namespace $NAMESPACE","title":" Task 2: Install the Chart"},{"location":"kubernetes-basics/helm/helm-charts/#task-3-upgrade-the-chart","text":"Our freshly deployed nginx is not yet accessible from outside the Kubernetes cluster. To expose it, we have to make sure a so called ingress resource will be deployed as well. Also make sure the application is accessible via TLS. A look into the file templates/ingress.yaml reveals that the rendering of the ingress and its values is configurable through values( values.yaml ): {{ - if .Values.ingress.enabled - }} {{ - $fullName : = include \"mychart.fullname\" . - }} {{ - $svcPort : = .Values.service.port - }} {{ - if and .Values.ingress.className (not (semverCompare \">=1.18-0\" .Capabilities.KubeVersion.GitVersion)) }} {{ - if not (hasKey .Values.ingress.annotations \"kubernetes.io/ingress.class\") }} {{ - $_ : = set .Values.ingress.annotations \"kubernetes.io/ingress.class\" .Values.ingress.className }} {{ - end }} {{ - end }} {{ - if semverCompare \">=1.19-0\" .Capabilities.KubeVersion.GitVersion - }} apiVersion : networking.k8s.io/v1 {{ - else if semverCompare \">=1.14-0\" .Capabilities.KubeVersion.GitVersion - }} apiVersion : networking.k8s.io/v1beta1 {{ - else - }} apiVersion : extensions/v1beta1 {{ - end }} kind : Ingress metadata : name : {{ $fullName }} labels : {{ - include \"mychart.labels\" . | nindent 4 }} {{ - with .Values.ingress.annotations }} annotations : {{ - toYaml . | nindent 4 }} {{ - end }} spec : {{ - if and .Values.ingress.className (semverCompare \">=1.18-0\" .Capabilities.KubeVersion.GitVersion) }} ingressClassName : {{ .Values.ingress.className }} {{ - end }} {{ - if .Values.ingress.tls }} tls : {{ - range .Values.ingress.tls }} - hosts : {{ - range .hosts }} - {{ . | quote }} {{ - end }} secretName : {{ .secretName }} {{ - end }} {{ - end }} rules : {{ - range .Values.ingress.hosts }} - host : {{ .host | quote }} http : paths : {{ - range .paths }} - path : {{ .path }} {{ - if and .pathType (semverCompare \">=1.18-0\" $.Capabilities.KubeVersion.GitVersion) }} pathType : {{ .pathType }} {{ - end }} backend : {{ - if semverCompare \">=1.19-0\" $.Capabilities.KubeVersion.GitVersion }} service : name : {{ $fullName }} port : number : {{ $svcPort }} {{ - else }} serviceName : {{ $fullName }} servicePort : {{ $svcPort }} {{ - end }} {{ - end }} {{ - end }} {{ - end }} Thus, we need to change this value inside our mychart/values.yaml file. This is also where we enable the TLS part: Note Make sure to replace the <url> and <namespace> accordingly. [ ... ] ingress : enabled : true annotations : kubernetes.io/ingress.class : nginx kubernetes.io/tls-acme : \"true\" cert-manager.io/cluster-issuer : letsencrypt-prod nginx.ingress.kubernetes.io/add-base-url : \"true\" hosts : - host : <url> paths : - path : / pathType : ImplementationSpecific tls : - secretName : <namespace>-tls hosts : - <url> [ ... ] Note Make sure to set the proper value as hostname. <namespace> and <url> will be provided by the trainer. Before we can upgrade, make sure to delete the old ingress resource: kubectl get ingress --namespace $NAMESPACE kubectl delete ingress <ingress-name> --namespace $NAMESPACE Apply the change by upgrading our release: helm upgrade --namespace $NAMESPACE myfirstrelease ./mychart Check whether the ingress was successfully deployed by accessing the URL.","title":" Task 3: Upgrade the Chart"},{"location":"kubernetes-basics/helm/helm-charts/#task-4-overwrite-value-using-commandline-param","text":"An alternative way to set or overwrite values for charts we want to deploy is the --set name=value parameter. This parameter can be used when installing a chart as well as upgrading. Update the replica count of your nginx Deployment to 2 using --set name=value helm upgrade --namespace $NAMESPACE --set replicaCount = 2 myfirstrelease ./mychart","title":" Task 4: Overwrite value using commandline param"},{"location":"kubernetes-basics/helm/helm-charts/#task-5-valuesyaml","text":"Have a look at the values.yaml file in your chart and study all the possible configuration params introduced in a freshly created chart.","title":" Task 5: values.yaml"},{"location":"kubernetes-basics/helm/helm-charts/#task-6-uninstall-the-chart","text":"To remove an application, simply remove the Helm release with the following command: helm uninstall --namespace $NAMESPACE myfirstrelease Do this with our deployed release. With kubectl get pods --namespace $NAMESPACE you should no longer see your application Pod.","title":" Task 6: Uninstall the Chart"},{"location":"kubernetes-basics/helm/helm-charts/#further-reading","text":"For creating a nice README.md for your chart, have a look at this tool. Also check out the Artifact Hub for finding Helm charts. You can also publish your own charts there.","title":"Further Reading"},{"location":"kubernetes-basics/introduction/","text":"Introduction In this tutorial, we introduced the core concepts of Kubernetes. All instructions and resources used in this tutorial are for quick overview only, not detailed instructions. Please check the official documentation for more details. Kubernetes Docs Certifications You can also get certified by passing the following exams: CKA CKAD CKS Dashboard You can also use the Kubernetes Dashboard to manage your cluster. Dashboard Core idea With open-source software Kubernetes, you get a platform that can deploy your application in containers and run it simultaneously. For this reason, Kubernetes is also known as a container platform, or to use the term container as a service (CaaS). Depending on the configuration, the term Platform as a Service (PaaS) also applies. Container engine The underlying container engine of Kubernetes is mostly Docker. There are other container engines that can be used with Kubernetes, such as CRI-O. Docker was originally created to help developers test their applications in their continuous integration environments. Nowadays, system admins also use it. CRI-O doesn\u2019t exist as long as Docker does. It is a \u201clightweight container runtime for Kubernetes\u201d and is fully OCI-compliant . Overview Kubernetes consists of control plane and worker (minion, compute) nodes. Control plane and worker nodes The control plane components are the API server, the scheduler and the controller manager. The API server itself represents the management interface. The scheduler and the controller manager decide how applications should be deployed on the cluster. Additionally, the state and configuration of the cluster itself are controlled in the control plane components. Worker nodes are also known as compute nodes, application nodes or minions, and are responsible for running the container workload (applications). The control plane for the worker nodes is implemented in the control plane components. The hosts running these components were historically called masters. Containers and images he smallest entities in Kubernetes are Pods, which resemble your containerized application. Using container virtualization, processes on a Linux system can be isolated up to a level where only the predefined resources are available. Several containers can run on the same system without \u201cseeing\u201d each other (files, process IDs, network). One container should contain one application (web server, database, cache, etc.). It should be at least one part of the application, e.g. when running a multi-service middleware. In a container itself any process can be started that runs natively on your operating system. Containers are based on images. An image represents the file tree, which includes the binary, shared libraries and other files which are needed to run your application. A container image is typically built from a Containerfile or Dockerfile , which is a text file filled with instructions. The end result is a hierarchically layered binary construct. Depending on the backend, the implementation uses overlay or copy-on-write (COW) mechanisms to represent the image. Layer example for a Tomcat application: Base image (CentOS 7) Install Java Install Tomcat Install App The pre-built images under version control can be saved in an image registry and can then be used by the container platform. Namespaces Namespaces in Kubernetes represent a logical segregation of unique names for entities (Pods, Services, Deployments, ConfigMaps, etc.). Permissions and roles can be bound on a per-namespace basis. This way, a user can control his own resources inside a namespace. Note Some resources are valid cluster-wise and cannot be set and controlled on a namespace basis. Pods A Pod is the smallest entity in Kubernetes. It represents one instance of your running application process. The Pod consists of at least two containers, one for your application itself and another one as part of the Kubernetes design, to keep the network namespace. The so-called infrastructure container (or pause container) is therefore automatically added by Kubernetes. The application ports from inside the Pod are exposed via Services. Services A service represents a static endpoint for your application in the Pod. As a Pod and its IP address typically are considered dynamic, the IP address of the Service does not change when changing the application inside the Pod. If you scale up your Pods, you have an automatic internal load balancing towards all Pod IP addresses. There are different kinds of Services: ClusterIP : Default virtual IP address range NodePort : Same as ClusterIP plus open ports on the nodes LoadBalancer : An external load balancer is created, only works in cloud environments, e.g. AWS ELB ExternalName : A DNS entry is created, also only works in cloud environments A Service is unique inside a Namespace. Deployments Have a look at the official documentation . Volumes Have a look at the official documentation . Jobs Have a look at the official documentation .","title":"Introduction"},{"location":"kubernetes-basics/introduction/#introduction","text":"In this tutorial, we introduced the core concepts of Kubernetes. All instructions and resources used in this tutorial are for quick overview only, not detailed instructions. Please check the official documentation for more details. Kubernetes Docs Certifications You can also get certified by passing the following exams: CKA CKAD CKS Dashboard You can also use the Kubernetes Dashboard to manage your cluster. Dashboard","title":"Introduction"},{"location":"kubernetes-basics/introduction/#core-idea","text":"With open-source software Kubernetes, you get a platform that can deploy your application in containers and run it simultaneously. For this reason, Kubernetes is also known as a container platform, or to use the term container as a service (CaaS). Depending on the configuration, the term Platform as a Service (PaaS) also applies.","title":"Core idea"},{"location":"kubernetes-basics/introduction/#container-engine","text":"The underlying container engine of Kubernetes is mostly Docker. There are other container engines that can be used with Kubernetes, such as CRI-O. Docker was originally created to help developers test their applications in their continuous integration environments. Nowadays, system admins also use it. CRI-O doesn\u2019t exist as long as Docker does. It is a \u201clightweight container runtime for Kubernetes\u201d and is fully OCI-compliant .","title":"Container engine"},{"location":"kubernetes-basics/introduction/#overview","text":"Kubernetes consists of control plane and worker (minion, compute) nodes.","title":"Overview"},{"location":"kubernetes-basics/introduction/#control-plane-and-worker-nodes","text":"The control plane components are the API server, the scheduler and the controller manager. The API server itself represents the management interface. The scheduler and the controller manager decide how applications should be deployed on the cluster. Additionally, the state and configuration of the cluster itself are controlled in the control plane components. Worker nodes are also known as compute nodes, application nodes or minions, and are responsible for running the container workload (applications). The control plane for the worker nodes is implemented in the control plane components. The hosts running these components were historically called masters.","title":"Control plane and worker nodes"},{"location":"kubernetes-basics/introduction/#containers-and-images","text":"he smallest entities in Kubernetes are Pods, which resemble your containerized application. Using container virtualization, processes on a Linux system can be isolated up to a level where only the predefined resources are available. Several containers can run on the same system without \u201cseeing\u201d each other (files, process IDs, network). One container should contain one application (web server, database, cache, etc.). It should be at least one part of the application, e.g. when running a multi-service middleware. In a container itself any process can be started that runs natively on your operating system. Containers are based on images. An image represents the file tree, which includes the binary, shared libraries and other files which are needed to run your application. A container image is typically built from a Containerfile or Dockerfile , which is a text file filled with instructions. The end result is a hierarchically layered binary construct. Depending on the backend, the implementation uses overlay or copy-on-write (COW) mechanisms to represent the image. Layer example for a Tomcat application: Base image (CentOS 7) Install Java Install Tomcat Install App The pre-built images under version control can be saved in an image registry and can then be used by the container platform.","title":"Containers and images"},{"location":"kubernetes-basics/introduction/#namespaces","text":"Namespaces in Kubernetes represent a logical segregation of unique names for entities (Pods, Services, Deployments, ConfigMaps, etc.). Permissions and roles can be bound on a per-namespace basis. This way, a user can control his own resources inside a namespace. Note Some resources are valid cluster-wise and cannot be set and controlled on a namespace basis.","title":"Namespaces"},{"location":"kubernetes-basics/introduction/#pods","text":"A Pod is the smallest entity in Kubernetes. It represents one instance of your running application process. The Pod consists of at least two containers, one for your application itself and another one as part of the Kubernetes design, to keep the network namespace. The so-called infrastructure container (or pause container) is therefore automatically added by Kubernetes. The application ports from inside the Pod are exposed via Services.","title":"Pods"},{"location":"kubernetes-basics/introduction/#services","text":"A service represents a static endpoint for your application in the Pod. As a Pod and its IP address typically are considered dynamic, the IP address of the Service does not change when changing the application inside the Pod. If you scale up your Pods, you have an automatic internal load balancing towards all Pod IP addresses. There are different kinds of Services: ClusterIP : Default virtual IP address range NodePort : Same as ClusterIP plus open ports on the nodes LoadBalancer : An external load balancer is created, only works in cloud environments, e.g. AWS ELB ExternalName : A DNS entry is created, also only works in cloud environments A Service is unique inside a Namespace.","title":"Services"},{"location":"kubernetes-basics/introduction/#deployments","text":"Have a look at the official documentation .","title":"Deployments"},{"location":"kubernetes-basics/introduction/#volumes","text":"Have a look at the official documentation .","title":"Volumes"},{"location":"kubernetes-basics/introduction/#jobs","text":"Have a look at the official documentation .","title":"Jobs"},{"location":"kubernetes-basics/introduction/yaml/","text":"YAML YAML Ain\u2019t Markup Language (YAML) is a human-readable data-serialization language. YAML is not a programming language. It is mostly used for storing configuration information. Note Data serialization is the process of converting data objects, or object states present in complex data structures, into a stream of bytes for storage, transfer, and distribution in a form that can allow recovery of its original structure. As you will see a lot of YAML in our Kubernetes basics course, we want to make sure you can read and write YAML. If you are not yet familiar with YAML, this introduction is waiting for you. Otherwise, feel free to skip it or come back later if you meet some less familiar YAML stuff. This introduction is based on the YAML Tutorial from cloudbees.com . For more information about YAML, see the YAML website . Asimple file Let\u2019s look at a YAML file for a overview: --- foo : \"foo is not bar\" bar : \"bar is not foo\" pi : 3.14159 awesome : true kubernetes-birth-year : 2015 cloud-native : - scalable - dynamic - cloud - container kubernetes : version : \"1.22.0\" deployed : true applications : - name : \"My App\" location : \"public cloud\" The file starts with three dashes. These dashes indicate the start of a new YAML document. YAML supports multiple documents, and compliant parsers will recognize each set of dashes as the beginning of a new one. Then we see the construct that makes up most of a typical YAML document: a key-value pair. foo is a key that points to a string value: foo is not bar YAML knows four different data types: foo & bar are strings. pi is a floating-point number awesome is a boolean kubernetes-birth-year is an integer You can enclose strings in single or double-quotes or no quotes at all. YAML recognizes unquoted numerals as integers or floating point. The cloud-native item is an array with four elements, each denoted by an opening dash. The elements in cloud-native are indented with two spaces. Indentation is how YAML denotes nesting. The number of spaces can vary from file to file, but tabs are not allowed. Finally, kubernetes is a dictionary that contains a string version , a boolean deployed and an array applications where the item of the array contains two strings . YAML supports nesting of key-values, and mixing types. Indentation and Whitespace Whitespace is part of YAML\u2019s formatting. Unless otherwise indicated, newlines indicate the end of a field. You structure a YAML document with indentation. The indentation level can be one or more spaces. The specification forbids tabs because tools treat them differently. Comments Comments begin with a pound sign. They can appear after a document value or take up an entire line. --- # This is a full line comment foo : bar # this is a comment, too YAML data types Values in YAML\u2019s key-value pairs are scalar. They act like the scalar types in languages like Perl, Javascript, and Python. It\u2019s usually good enough to enclose strings in quotes, leave numbers unquoted, and let the parser figure it out. But that\u2019s only the tip of the iceberg. YAML is capable of a great deal more. Key-Value Pairs and Dictionaries The key-value is YAML\u2019s basic building block. Every item in a YAML document is a member of at least one dictionary. The key is always a string. The value is a scalar so that it can be any datatype. So, as we\u2019ve already seen, the value can be a string, a number, or another dictionary. Numeric types YAML recognizes numeric types. We saw floating point and integers above. YAML supports several other numeric types. An integer can be decimal, hexadecimal, or octal. --- foo : 12345 bar : 0x12d4 plop : 023332 YAML supports both fixed and exponential floating point numbers. --- foo : 1230.15 bar : 12.3015e+05 Finally, we can represent not-a-number (NAN) or infinity. --- foo : .inf bar : -.Inf plop : .NAN Foo is infinity. Bar is negative infinity, and plop is NAN. Strings YAML strings are Unicode. In most situations, you don\u2019t have to specify them in quotes. --- foo : this is a normal string But if we want escape sequences handled, we need to use double quotes. --- foo : \"this is not a normal string\\n\" bar : this is not a normal string\\n YAML processes the first value as ending with a carriage return and linefeed. Since the second value is not quoted, YAML treats the \\n as two characters. foo : this is not a normal string bar : this is not a normal string\\n YAML will not escape strings with single quotes, but the single quotes do avoid having string contents interpreted as document formatting. String values can span more than one line. With the fold (greater than) character, you can specify a string in a block. bar : > this is not a normal string it spans more than one line see? But it\u2019s interpreted without the newlines: bar : this is not a normal string it spans more than one line see? The block (pipe) character has a similar function, but YAML interprets the field exactly as is. --- bar : | this is not a normal string it spans more than one line see? So, we see the newlines where they are in the document. bar : this is not a normal string it spans more than one line see? Nulls You enter nulls with a tilde or the unquoted null string literal. --- foo : ~ bar : null Booleans YAML indicates boolean values with the keywords True, On and Yes for true. False is indicated with False, Off, or No. --- foo : True bar : False light : On TV : Off Arrays You can specify arrays or lists on a single line. --- items : [ 1 , 2 , 3 , 4 , 5 ] names : [ \"one\" , \"two\" , \"three\" , \"four\" ] Or, you can put them on multiple lines. --- items : - 1 - 2 - 3 - 4 - 5 names : - \"one\" - \"two\" - \"three\" - \"four\" The multiple line format is useful for lists that contain complex objects instead of scalars. --- items : - things : thing1 : huey things2 : dewey thing3 : louie - other things : key : value An array can contain any valid YAML value. The values in a list do not have to be the same type. Dictionaries We covered dictionaries above, but there\u2019s more to them. Like arrays, you can put dictionaries inline. We saw this format above. --- foo : { thing1 : huey , thing2 : louie , thing3 : dewey } We\u2019ve seen them span lines before. --- foo : bar bar : foo And, of course, they can be nested and hold any value. --- foo : bar : - bar - rab - plop","title":"YAML"},{"location":"kubernetes-basics/introduction/yaml/#yaml","text":"YAML Ain\u2019t Markup Language (YAML) is a human-readable data-serialization language. YAML is not a programming language. It is mostly used for storing configuration information. Note Data serialization is the process of converting data objects, or object states present in complex data structures, into a stream of bytes for storage, transfer, and distribution in a form that can allow recovery of its original structure. As you will see a lot of YAML in our Kubernetes basics course, we want to make sure you can read and write YAML. If you are not yet familiar with YAML, this introduction is waiting for you. Otherwise, feel free to skip it or come back later if you meet some less familiar YAML stuff. This introduction is based on the YAML Tutorial from cloudbees.com . For more information about YAML, see the YAML website .","title":"YAML"},{"location":"kubernetes-basics/introduction/yaml/#asimple-file","text":"Let\u2019s look at a YAML file for a overview: --- foo : \"foo is not bar\" bar : \"bar is not foo\" pi : 3.14159 awesome : true kubernetes-birth-year : 2015 cloud-native : - scalable - dynamic - cloud - container kubernetes : version : \"1.22.0\" deployed : true applications : - name : \"My App\" location : \"public cloud\" The file starts with three dashes. These dashes indicate the start of a new YAML document. YAML supports multiple documents, and compliant parsers will recognize each set of dashes as the beginning of a new one. Then we see the construct that makes up most of a typical YAML document: a key-value pair. foo is a key that points to a string value: foo is not bar YAML knows four different data types: foo & bar are strings. pi is a floating-point number awesome is a boolean kubernetes-birth-year is an integer You can enclose strings in single or double-quotes or no quotes at all. YAML recognizes unquoted numerals as integers or floating point. The cloud-native item is an array with four elements, each denoted by an opening dash. The elements in cloud-native are indented with two spaces. Indentation is how YAML denotes nesting. The number of spaces can vary from file to file, but tabs are not allowed. Finally, kubernetes is a dictionary that contains a string version , a boolean deployed and an array applications where the item of the array contains two strings . YAML supports nesting of key-values, and mixing types.","title":"Asimple file"},{"location":"kubernetes-basics/introduction/yaml/#indentation-and-whitespace","text":"Whitespace is part of YAML\u2019s formatting. Unless otherwise indicated, newlines indicate the end of a field. You structure a YAML document with indentation. The indentation level can be one or more spaces. The specification forbids tabs because tools treat them differently.","title":"Indentation and Whitespace"},{"location":"kubernetes-basics/introduction/yaml/#comments","text":"Comments begin with a pound sign. They can appear after a document value or take up an entire line. --- # This is a full line comment foo : bar # this is a comment, too","title":"Comments"},{"location":"kubernetes-basics/introduction/yaml/#yaml-data-types","text":"Values in YAML\u2019s key-value pairs are scalar. They act like the scalar types in languages like Perl, Javascript, and Python. It\u2019s usually good enough to enclose strings in quotes, leave numbers unquoted, and let the parser figure it out. But that\u2019s only the tip of the iceberg. YAML is capable of a great deal more.","title":"YAML data types"},{"location":"kubernetes-basics/introduction/yaml/#key-value-pairs-and-dictionaries","text":"The key-value is YAML\u2019s basic building block. Every item in a YAML document is a member of at least one dictionary. The key is always a string. The value is a scalar so that it can be any datatype. So, as we\u2019ve already seen, the value can be a string, a number, or another dictionary.","title":"Key-Value Pairs and Dictionaries"},{"location":"kubernetes-basics/introduction/yaml/#numeric-types","text":"YAML recognizes numeric types. We saw floating point and integers above. YAML supports several other numeric types. An integer can be decimal, hexadecimal, or octal. --- foo : 12345 bar : 0x12d4 plop : 023332 YAML supports both fixed and exponential floating point numbers. --- foo : 1230.15 bar : 12.3015e+05 Finally, we can represent not-a-number (NAN) or infinity. --- foo : .inf bar : -.Inf plop : .NAN Foo is infinity. Bar is negative infinity, and plop is NAN.","title":"Numeric types"},{"location":"kubernetes-basics/introduction/yaml/#strings","text":"YAML strings are Unicode. In most situations, you don\u2019t have to specify them in quotes. --- foo : this is a normal string But if we want escape sequences handled, we need to use double quotes. --- foo : \"this is not a normal string\\n\" bar : this is not a normal string\\n YAML processes the first value as ending with a carriage return and linefeed. Since the second value is not quoted, YAML treats the \\n as two characters. foo : this is not a normal string bar : this is not a normal string\\n YAML will not escape strings with single quotes, but the single quotes do avoid having string contents interpreted as document formatting. String values can span more than one line. With the fold (greater than) character, you can specify a string in a block. bar : > this is not a normal string it spans more than one line see? But it\u2019s interpreted without the newlines: bar : this is not a normal string it spans more than one line see? The block (pipe) character has a similar function, but YAML interprets the field exactly as is. --- bar : | this is not a normal string it spans more than one line see? So, we see the newlines where they are in the document. bar : this is not a normal string it spans more than one line see?","title":"Strings"},{"location":"kubernetes-basics/introduction/yaml/#nulls","text":"You enter nulls with a tilde or the unquoted null string literal. --- foo : ~ bar : null","title":"Nulls"},{"location":"kubernetes-basics/introduction/yaml/#booleans","text":"YAML indicates boolean values with the keywords True, On and Yes for true. False is indicated with False, Off, or No. --- foo : True bar : False light : On TV : Off","title":"Booleans"},{"location":"kubernetes-basics/introduction/yaml/#arrays","text":"You can specify arrays or lists on a single line. --- items : [ 1 , 2 , 3 , 4 , 5 ] names : [ \"one\" , \"two\" , \"three\" , \"four\" ] Or, you can put them on multiple lines. --- items : - 1 - 2 - 3 - 4 - 5 names : - \"one\" - \"two\" - \"three\" - \"four\" The multiple line format is useful for lists that contain complex objects instead of scalars. --- items : - things : thing1 : huey things2 : dewey thing3 : louie - other things : key : value An array can contain any valid YAML value. The values in a list do not have to be the same type.","title":"Arrays"},{"location":"kubernetes-basics/introduction/yaml/#dictionaries","text":"We covered dictionaries above, but there\u2019s more to them. Like arrays, you can put dictionaries inline. We saw this format above. --- foo : { thing1 : huey , thing2 : louie , thing3 : dewey } We\u2019ve seen them span lines before. --- foo : bar bar : foo And, of course, they can be nested and hold any value. --- foo : bar : - bar - rab - plop","title":"Dictionaries"},{"location":"kubernetes-basics/kustomize/","text":"Kustomize Environment Variables We are going to use some environment variables in this tutorial. Please make sure you have set them correctly. # check if the environment variables are set if not set them export NAMESPACE = <namespace> echo $NAMESPACE Kustomize is a tool to manage YAML configurations for Kubernetes objects in a declarative and reusable manner. In this lab, we will use Kustomize to deploy the same app for two different environments. Installation Kustomize can be used in two different ways: As a standalone kustomize binary, downloadable from here With the parameter --kustomize or -k in certain kubectl subcommands such as apply or create Note You might get a different behaviour depending on which variant you use. The reason for this is that the version built into kubectl is usually older than the standalone binary. Usage The main purpose of Kustomize is to build configurations from a predefined file structure (which will be introduced in the next section): kustomize build <path-to-kustomization-directory> The same can be achieved with kubectl: kubectl apply -k <path-to-kustomization-directory> The next step is to apply this configuration to the Kubernetes cluster: kustomize build <path-to-kustomization-directory> | kubectl apply -f - Or in one kubectl command with the parameter -k instead of -f : kubectl apply -k <path-to-kustomization-directory> Task 1 : Prepare a Kustomize config We are going to deploy a simple application: The Deployment starts an application based on nginx A Service exposes the Deployment The application will be deployed for two different example environments, integration and production Kustomize allows inheriting Kubernetes configurations. We are going to use this to create a base configuration and then override it for the different environments. Note that Kustomize does not use templating. Instead, smart patch and extension mechanisms are used on plain YAML manifests to keep things as simple as possible. File structure The structure of a Kustomize configuration typically looks like this: . \u251c\u2500\u2500 base \u2502 \u251c\u2500\u2500 deployment.yaml \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2514\u2500\u2500 service.yaml \u2514\u2500\u2500 overlays \u251c\u2500\u2500 production \u2502 \u251c\u2500\u2500 deployment-patch.yaml \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2514\u2500\u2500 service-patch.yaml \u2514\u2500\u2500 staging \u251c\u2500\u2500 deployment-patch.yaml \u251c\u2500\u2500 kustomization.yaml \u2514\u2500\u2500 service-patch.yaml Base Let\u2019s have a look at the base directory first which contains the base configuration. There\u2019s a deployment.yaml with the following content: apiVersion : apps/v1 kind : Deployment metadata : name : kustomize-app spec : selector : matchLabels : app : kustomize-app template : metadata : labels : app : kustomize-app spec : containers : - name : kustomize-app image : ghcr.io/natrongmbh/kubernetes-workshop-golog-test-webserver:latest env : - name : APPLICATION_NAME value : app-base command : - sh - -c - |- set -e /bin/echo \"My name is $APPLICATION_NAME\" /usr/local/bin/go ports : - name : http containerPort : 80 protocol : TCP There\u2019s also a Service for our Deployment in the corresponding base/service.yaml : apiVersion : v1 kind : Service metadata : name : kustomize-app spec : ports : - port : 80 targetPort : 80 selector : app : kustomize-app And there\u2019s an additional base/kustomization.yaml which is used to configure Kustomize: resources : - service.yaml - deployment.yaml It references the previous manifests service.yaml and deployment.yaml and makes them part of our base configuration. Overlays Now let\u2019s have a look at the other directory which is called overlays . It contains two subdirectories staging and production which both contain a kustomization.yaml with almost the same content. overlays/staging/kustomization.yaml : nameSuffix : -staging bases : - ../../base patchesStrategicMerge : - deployment-patch.yaml - service-patch.yaml overlays/production/kustomization.yaml : nameSuffix : -production bases : - ../../base patchesStrategicMerge : - deployment-patch.yaml - service-patch.yaml Only the first key nameSuffix differs. In both cases, the kustomization.yaml references our base configuration. However, the two directories contain two different deployment-patch.yaml files which patch the deployment.yaml from our base configuration. overlays/staging/deployment-patch.yaml : apiVersion : apps/v1 kind : Deployment metadata : name : kustomize-app spec : selector : matchLabels : app : kustomize-app-staging template : metadata : labels : app : kustomize-app-staging spec : containers : - name : kustomize-app env : - name : APPLICATION_NAME value : kustomize-app-staging overlays/production/deployment-patch.yaml : apiVersion : apps/v1 kind : Deployment metadata : name : kustomize-app spec : selector : matchLabels : app : kustomize-app-production template : metadata : labels : app : kustomize-app-production spec : containers : - name : kustomize-app env : - name : APPLICATION_NAME value : kustomize-app-production The main difference here is that the environment variable APPLICATION_NAME is set differently. The app label also differs because we are going to deploy both Deployments into the same Namespace. The same applies to our Service. It also comes in two customizations so that it matches the corresponding Deployment in the same Namespace. overlays/staging/service-patch.yaml : apiVersion : v1 kind : Service metadata : name : kustomize-app spec : selector : app : kustomize-app-staging overlays/production/service-patch.yaml : apiVersion : v1 kind : Service metadata : name : kustomize-app spec : selector : app : kustomize-app-production Info All files mentioned above are also directly accessible from GitHub. Prepare the files as described above in a local directory of your choice. Task 2 : Deploy with Kustomize We are now ready to deploy both apps for the two different environments. For simplicity, we will use the same Namespace. kubectl apply -k overlays/staging --namespace $NAMESPACE kubectl apply -k overlays/production --namespace $NAMESPACE As you can see, we now have two deployments and services deployed. Both of them use the same base configuration. However, they have a specific configuration on their own as well. Let\u2019s verify this. Our app writes a corresponding log entry that we can use for analysis: kubectl get pods --namespace $NAMESPACE kubectl logs <pod-name> --namespace $NAMESPACE Further Reading Kustomize has more features of which we just covered a couple. Please refer to the docs for more information. Kustomize documentation: https://kubernetes-sigs.github.io/kustomize/ API reference: https://kubernetes-sigs.github.io/kustomize/api-reference/ Another kustomization.yaml reference: https://kubectl.docs.kubernetes.io/pages/reference/kustomize.html Examples: https://github.com/kubernetes-sigs/kustomize/tree/master/examples","title":"Kustomize"},{"location":"kubernetes-basics/kustomize/#kustomize","text":"Environment Variables We are going to use some environment variables in this tutorial. Please make sure you have set them correctly. # check if the environment variables are set if not set them export NAMESPACE = <namespace> echo $NAMESPACE Kustomize is a tool to manage YAML configurations for Kubernetes objects in a declarative and reusable manner. In this lab, we will use Kustomize to deploy the same app for two different environments.","title":"Kustomize"},{"location":"kubernetes-basics/kustomize/#installation","text":"Kustomize can be used in two different ways: As a standalone kustomize binary, downloadable from here With the parameter --kustomize or -k in certain kubectl subcommands such as apply or create Note You might get a different behaviour depending on which variant you use. The reason for this is that the version built into kubectl is usually older than the standalone binary.","title":"Installation"},{"location":"kubernetes-basics/kustomize/#usage","text":"The main purpose of Kustomize is to build configurations from a predefined file structure (which will be introduced in the next section): kustomize build <path-to-kustomization-directory> The same can be achieved with kubectl: kubectl apply -k <path-to-kustomization-directory> The next step is to apply this configuration to the Kubernetes cluster: kustomize build <path-to-kustomization-directory> | kubectl apply -f - Or in one kubectl command with the parameter -k instead of -f : kubectl apply -k <path-to-kustomization-directory>","title":"Usage"},{"location":"kubernetes-basics/kustomize/#task-1-prepare-a-kustomize-config","text":"We are going to deploy a simple application: The Deployment starts an application based on nginx A Service exposes the Deployment The application will be deployed for two different example environments, integration and production Kustomize allows inheriting Kubernetes configurations. We are going to use this to create a base configuration and then override it for the different environments. Note that Kustomize does not use templating. Instead, smart patch and extension mechanisms are used on plain YAML manifests to keep things as simple as possible.","title":" Task 1: Prepare a Kustomize config"},{"location":"kubernetes-basics/kustomize/#file-structure","text":"The structure of a Kustomize configuration typically looks like this: . \u251c\u2500\u2500 base \u2502 \u251c\u2500\u2500 deployment.yaml \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2514\u2500\u2500 service.yaml \u2514\u2500\u2500 overlays \u251c\u2500\u2500 production \u2502 \u251c\u2500\u2500 deployment-patch.yaml \u2502 \u251c\u2500\u2500 kustomization.yaml \u2502 \u2514\u2500\u2500 service-patch.yaml \u2514\u2500\u2500 staging \u251c\u2500\u2500 deployment-patch.yaml \u251c\u2500\u2500 kustomization.yaml \u2514\u2500\u2500 service-patch.yaml","title":"File structure"},{"location":"kubernetes-basics/kustomize/#base","text":"Let\u2019s have a look at the base directory first which contains the base configuration. There\u2019s a deployment.yaml with the following content: apiVersion : apps/v1 kind : Deployment metadata : name : kustomize-app spec : selector : matchLabels : app : kustomize-app template : metadata : labels : app : kustomize-app spec : containers : - name : kustomize-app image : ghcr.io/natrongmbh/kubernetes-workshop-golog-test-webserver:latest env : - name : APPLICATION_NAME value : app-base command : - sh - -c - |- set -e /bin/echo \"My name is $APPLICATION_NAME\" /usr/local/bin/go ports : - name : http containerPort : 80 protocol : TCP There\u2019s also a Service for our Deployment in the corresponding base/service.yaml : apiVersion : v1 kind : Service metadata : name : kustomize-app spec : ports : - port : 80 targetPort : 80 selector : app : kustomize-app And there\u2019s an additional base/kustomization.yaml which is used to configure Kustomize: resources : - service.yaml - deployment.yaml It references the previous manifests service.yaml and deployment.yaml and makes them part of our base configuration.","title":"Base"},{"location":"kubernetes-basics/kustomize/#overlays","text":"Now let\u2019s have a look at the other directory which is called overlays . It contains two subdirectories staging and production which both contain a kustomization.yaml with almost the same content. overlays/staging/kustomization.yaml : nameSuffix : -staging bases : - ../../base patchesStrategicMerge : - deployment-patch.yaml - service-patch.yaml overlays/production/kustomization.yaml : nameSuffix : -production bases : - ../../base patchesStrategicMerge : - deployment-patch.yaml - service-patch.yaml Only the first key nameSuffix differs. In both cases, the kustomization.yaml references our base configuration. However, the two directories contain two different deployment-patch.yaml files which patch the deployment.yaml from our base configuration. overlays/staging/deployment-patch.yaml : apiVersion : apps/v1 kind : Deployment metadata : name : kustomize-app spec : selector : matchLabels : app : kustomize-app-staging template : metadata : labels : app : kustomize-app-staging spec : containers : - name : kustomize-app env : - name : APPLICATION_NAME value : kustomize-app-staging overlays/production/deployment-patch.yaml : apiVersion : apps/v1 kind : Deployment metadata : name : kustomize-app spec : selector : matchLabels : app : kustomize-app-production template : metadata : labels : app : kustomize-app-production spec : containers : - name : kustomize-app env : - name : APPLICATION_NAME value : kustomize-app-production The main difference here is that the environment variable APPLICATION_NAME is set differently. The app label also differs because we are going to deploy both Deployments into the same Namespace. The same applies to our Service. It also comes in two customizations so that it matches the corresponding Deployment in the same Namespace. overlays/staging/service-patch.yaml : apiVersion : v1 kind : Service metadata : name : kustomize-app spec : selector : app : kustomize-app-staging overlays/production/service-patch.yaml : apiVersion : v1 kind : Service metadata : name : kustomize-app spec : selector : app : kustomize-app-production Info All files mentioned above are also directly accessible from GitHub. Prepare the files as described above in a local directory of your choice.","title":"Overlays"},{"location":"kubernetes-basics/kustomize/#task-2-deploy-with-kustomize","text":"We are now ready to deploy both apps for the two different environments. For simplicity, we will use the same Namespace. kubectl apply -k overlays/staging --namespace $NAMESPACE kubectl apply -k overlays/production --namespace $NAMESPACE As you can see, we now have two deployments and services deployed. Both of them use the same base configuration. However, they have a specific configuration on their own as well. Let\u2019s verify this. Our app writes a corresponding log entry that we can use for analysis: kubectl get pods --namespace $NAMESPACE kubectl logs <pod-name> --namespace $NAMESPACE","title":" Task 2: Deploy with Kustomize"},{"location":"kubernetes-basics/kustomize/#further-reading","text":"Kustomize has more features of which we just covered a couple. Please refer to the docs for more information. Kustomize documentation: https://kubernetes-sigs.github.io/kustomize/ API reference: https://kubernetes-sigs.github.io/kustomize/api-reference/ Another kustomization.yaml reference: https://kubectl.docs.kubernetes.io/pages/reference/kustomize.html Examples: https://github.com/kubernetes-sigs/kustomize/tree/master/examples","title":"Further Reading"},{"location":"kubernetes-basics/security/","text":"Security There are many security concerns to take into account when deploying applications to a Kubernetes cluster. In this module, you'll learn the basics of how you can secure your applications.","title":"Security"},{"location":"kubernetes-basics/security/#security","text":"There are many security concerns to take into account when deploying applications to a Kubernetes cluster. In this module, you'll learn the basics of how you can secure your applications.","title":"Security"},{"location":"kubernetes-basics/security/network-policies/","text":"Network Policies One CNI function is the ability to enforce network policies and implement an in-cluster zero-trust container strategy. Network policies are a default Kubernetes object for controlling network traffic, but a CNI such as Cilium or Calico is required to enforce them. We will demonstrate traffic blocking with our simple app. Warning This section requires a CNI that supports network policies. The Flannel CNI does not support network policies which is currently the default CNI for a stepping stone cluster. Note If you are not yet familiar with Kubernetes Network Policies we suggest going to the Kubernetes Documentation . Task 1 : Deploy a simple frontend/backend application First we need a simple application to show the effects on Kubernetes network policies. Let\u2019s have a look at the following resource definitions: --- apiVersion : apps/v1 kind : Deployment metadata : name : frontend labels : app : frontend spec : replicas : 1 selector : matchLabels : app : frontend template : metadata : labels : app : frontend spec : containers : - name : frontend-container image : docker.io/byrnedo/alpine-curl:0.1.8 imagePullPolicy : IfNotPresent command : [ \"/bin/ash\" , \"-c\" , \"sleep 1000000000\" ] --- apiVersion : apps/v1 kind : Deployment metadata : name : not-frontend labels : app : not-frontend spec : replicas : 1 selector : matchLabels : app : not-frontend template : metadata : labels : app : not-frontend spec : containers : - name : not-frontend-container image : docker.io/byrnedo/alpine-curl:0.1.8 imagePullPolicy : IfNotPresent command : [ \"/bin/bash\" , \"-c\" , \"sleep 1000000000\" ] --- apiVersion : apps/v1 kind : Deployment metadata : name : backend labels : app : backend spec : replicas : 1 selector : matchLabels : app : backend template : metadata : labels : app : backend spec : containers : - name : backend-container env : - name : PORT value : \"8080\" ports : - containerPort : 8080 image : docker.io/cilium/json-mock:1.2 imagePullPolicy : IfNotPresent --- apiVersion : v1 kind : Service metadata : name : backend labels : app : backend spec : type : ClusterIP selector : app : backend ports : - name : http port : 8080 The application consists of two client deployments ( frontend and not-frontend ) and one backend deployment ( backend ). We are going to send requests from the frontend and not-frontend pods to the backend pod. Create a file policies-app.yaml with the above content. Deploy the app in a new namespace: export NAMESPACE = <username>-policies kubectl create namespace $NAMESPACE kubectl apply -f policies-app.yaml --namespace $NAMESPACE this gives you the following output: deployment.apps/frontend created deployment.apps/not-frontend created deployment.apps/backend created service/backend created Verify with the following command that everything is up and running: kubectl get pods --namespace $NAMESPACE Let us make life a bit easier by storing the pods name into an environment variable so we can reuse it later again: export FRONTEND = $( kubectl get pods -l app = frontend -o jsonpath = '{.items[0].metadata.name}' --namespace $NAMESPACE ) echo ${ FRONTEND } export NOT_FRONTEND = $( kubectl get pods -l app = not-frontend -o jsonpath = '{.items[0].metadata.name}' --namespace $NAMESPACE ) echo ${ NOT_FRONTEND } Task 2 : Verify that the frontend can access the backend Now we generate some traffic as a baseline test. kubectl exec -ti ${ FRONTEND } --namespace $NAMESPACE -- curl -I --connect-timeout 5 backend:8080 and kubectl exec -ti ${ NOT_FRONTEND } --namespace $NAMESPACE -- curl -I --connect-timeout 5 backend:8080 This will execute a simple curl call from the frontend and not-frondend application to the backend application: HTTP/1.1 200 OK X-Powered-By: Express Vary: Origin, Accept-Encoding Access-Control-Allow-Credentials: true Accept-Ranges: bytes Cache-Control: public, max-age=0 Last-Modified: Sat, 26 Oct 1985 08:15:00 GMT ETag: W/\"83d-7438674ba0\" Content-Type: text/html; charset=UTF-8 Content-Length: 2109 Date: Mon, 21 Nov 2022 13:00:59 GMT Connection: keep-alive HTTP/1.1 200 OK X-Powered-By: Express Vary: Origin, Accept-Encoding Access-Control-Allow-Credentials: true Accept-Ranges: bytes Cache-Control: public, max-age=0 Last-Modified: Sat, 26 Oct 1985 08:15:00 GMT ETag: W/\"83d-7438674ba0\" Content-Type: text/html; charset=UTF-8 Content-Length: 2109 Date: Mon, 21 Nov 2022 13:01:18 GMT Connection: keep-alive and we see, both applications can connect to the backend application. Until now ingress and egress policy enforcement are still disabled on all of our pods because no network policy has been imported yet selecting any of the pods. Let us change this. Task 3 : Deny traffic with a network policy We block traffic by applying a network policy. Create a file policies-backend-ingress-deny.yaml with the following content: --- kind : NetworkPolicy apiVersion : networking.k8s.io/v1 metadata : name : backend-ingress-deny spec : podSelector : matchLabels : app : backend policyTypes : - Ingress The policy will deny all ingress traffic as it is of type Ingress but specifies no allow rule, and will be applied to all pods with the app=backend label thanks to the podSelector. Ok, then let\u2019s create the policy with: kubectl apply -f policies-backend-ingress-deny.yaml --namespace $NAMESPACE and verify that the policy has been created: kubectl get networkpolicies --namespace $NAMESPACE which gives you the following output: NAME POD-SELECTOR AGE backend-ingress-deny app=backend 4s Task 4 : Verify that the frontend can no longer access the backend We can now execute the connectivity check again: kubectl exec -ti ${ FRONTEND } --namespace $NAMESPACE -- curl -I --connect-timeout 5 backend:8080 and kubectl exec -ti ${ NOT_FRONTEND } --namespace $NAMESPACE -- curl -I --connect-timeout 5 backend:8080 but this time you see that the frontend and not-frontend application cannot connect anymore to the backend: # Frontend curl: (28) Connection timed out after 5001 milliseconds command terminated with exit code 28 # Not Frontend curl: (28) Connection timed out after 5001 milliseconds command terminated with exit code 28 Further reading Check out the awesome Kubernetes Network Policy Receipes repository for a lot of examples on how to use Kubernetes Network Policies.","title":"Network Policies"},{"location":"kubernetes-basics/security/network-policies/#network-policies","text":"One CNI function is the ability to enforce network policies and implement an in-cluster zero-trust container strategy. Network policies are a default Kubernetes object for controlling network traffic, but a CNI such as Cilium or Calico is required to enforce them. We will demonstrate traffic blocking with our simple app. Warning This section requires a CNI that supports network policies. The Flannel CNI does not support network policies which is currently the default CNI for a stepping stone cluster. Note If you are not yet familiar with Kubernetes Network Policies we suggest going to the Kubernetes Documentation .","title":"Network Policies"},{"location":"kubernetes-basics/security/network-policies/#task-1-deploy-a-simple-frontendbackend-application","text":"First we need a simple application to show the effects on Kubernetes network policies. Let\u2019s have a look at the following resource definitions: --- apiVersion : apps/v1 kind : Deployment metadata : name : frontend labels : app : frontend spec : replicas : 1 selector : matchLabels : app : frontend template : metadata : labels : app : frontend spec : containers : - name : frontend-container image : docker.io/byrnedo/alpine-curl:0.1.8 imagePullPolicy : IfNotPresent command : [ \"/bin/ash\" , \"-c\" , \"sleep 1000000000\" ] --- apiVersion : apps/v1 kind : Deployment metadata : name : not-frontend labels : app : not-frontend spec : replicas : 1 selector : matchLabels : app : not-frontend template : metadata : labels : app : not-frontend spec : containers : - name : not-frontend-container image : docker.io/byrnedo/alpine-curl:0.1.8 imagePullPolicy : IfNotPresent command : [ \"/bin/bash\" , \"-c\" , \"sleep 1000000000\" ] --- apiVersion : apps/v1 kind : Deployment metadata : name : backend labels : app : backend spec : replicas : 1 selector : matchLabels : app : backend template : metadata : labels : app : backend spec : containers : - name : backend-container env : - name : PORT value : \"8080\" ports : - containerPort : 8080 image : docker.io/cilium/json-mock:1.2 imagePullPolicy : IfNotPresent --- apiVersion : v1 kind : Service metadata : name : backend labels : app : backend spec : type : ClusterIP selector : app : backend ports : - name : http port : 8080 The application consists of two client deployments ( frontend and not-frontend ) and one backend deployment ( backend ). We are going to send requests from the frontend and not-frontend pods to the backend pod. Create a file policies-app.yaml with the above content. Deploy the app in a new namespace: export NAMESPACE = <username>-policies kubectl create namespace $NAMESPACE kubectl apply -f policies-app.yaml --namespace $NAMESPACE this gives you the following output: deployment.apps/frontend created deployment.apps/not-frontend created deployment.apps/backend created service/backend created Verify with the following command that everything is up and running: kubectl get pods --namespace $NAMESPACE Let us make life a bit easier by storing the pods name into an environment variable so we can reuse it later again: export FRONTEND = $( kubectl get pods -l app = frontend -o jsonpath = '{.items[0].metadata.name}' --namespace $NAMESPACE ) echo ${ FRONTEND } export NOT_FRONTEND = $( kubectl get pods -l app = not-frontend -o jsonpath = '{.items[0].metadata.name}' --namespace $NAMESPACE ) echo ${ NOT_FRONTEND }","title":" Task 1: Deploy a simple frontend/backend application"},{"location":"kubernetes-basics/security/network-policies/#task-2-verify-that-the-frontend-can-access-the-backend","text":"Now we generate some traffic as a baseline test. kubectl exec -ti ${ FRONTEND } --namespace $NAMESPACE -- curl -I --connect-timeout 5 backend:8080 and kubectl exec -ti ${ NOT_FRONTEND } --namespace $NAMESPACE -- curl -I --connect-timeout 5 backend:8080 This will execute a simple curl call from the frontend and not-frondend application to the backend application: HTTP/1.1 200 OK X-Powered-By: Express Vary: Origin, Accept-Encoding Access-Control-Allow-Credentials: true Accept-Ranges: bytes Cache-Control: public, max-age=0 Last-Modified: Sat, 26 Oct 1985 08:15:00 GMT ETag: W/\"83d-7438674ba0\" Content-Type: text/html; charset=UTF-8 Content-Length: 2109 Date: Mon, 21 Nov 2022 13:00:59 GMT Connection: keep-alive HTTP/1.1 200 OK X-Powered-By: Express Vary: Origin, Accept-Encoding Access-Control-Allow-Credentials: true Accept-Ranges: bytes Cache-Control: public, max-age=0 Last-Modified: Sat, 26 Oct 1985 08:15:00 GMT ETag: W/\"83d-7438674ba0\" Content-Type: text/html; charset=UTF-8 Content-Length: 2109 Date: Mon, 21 Nov 2022 13:01:18 GMT Connection: keep-alive and we see, both applications can connect to the backend application. Until now ingress and egress policy enforcement are still disabled on all of our pods because no network policy has been imported yet selecting any of the pods. Let us change this.","title":" Task 2: Verify that the frontend can access the backend"},{"location":"kubernetes-basics/security/network-policies/#task-3-deny-traffic-with-a-network-policy","text":"We block traffic by applying a network policy. Create a file policies-backend-ingress-deny.yaml with the following content: --- kind : NetworkPolicy apiVersion : networking.k8s.io/v1 metadata : name : backend-ingress-deny spec : podSelector : matchLabels : app : backend policyTypes : - Ingress The policy will deny all ingress traffic as it is of type Ingress but specifies no allow rule, and will be applied to all pods with the app=backend label thanks to the podSelector. Ok, then let\u2019s create the policy with: kubectl apply -f policies-backend-ingress-deny.yaml --namespace $NAMESPACE and verify that the policy has been created: kubectl get networkpolicies --namespace $NAMESPACE which gives you the following output: NAME POD-SELECTOR AGE backend-ingress-deny app=backend 4s","title":" Task 3: Deny traffic with a network policy"},{"location":"kubernetes-basics/security/network-policies/#task-4-verify-that-the-frontend-can-no-longer-access-the-backend","text":"We can now execute the connectivity check again: kubectl exec -ti ${ FRONTEND } --namespace $NAMESPACE -- curl -I --connect-timeout 5 backend:8080 and kubectl exec -ti ${ NOT_FRONTEND } --namespace $NAMESPACE -- curl -I --connect-timeout 5 backend:8080 but this time you see that the frontend and not-frontend application cannot connect anymore to the backend: # Frontend curl: (28) Connection timed out after 5001 milliseconds command terminated with exit code 28 # Not Frontend curl: (28) Connection timed out after 5001 milliseconds command terminated with exit code 28","title":" Task 4: Verify that the frontend can no longer access the backend"},{"location":"kubernetes-basics/security/network-policies/#further-reading","text":"Check out the awesome Kubernetes Network Policy Receipes repository for a lot of examples on how to use Kubernetes Network Policies.","title":"Further reading"},{"location":"kubernetes-basics/security/rbac/","text":"RBAC Environment Variables We are going to use some environment variables in this tutorial. Please make sure you have set them correctly. # check if the environment variables are set if not set them export NAMESPACE = <namespace> echo $NAMESPACE Role-based access control (RBAC) is a method of regulating access to computer or network resources based on the roles of individual users within your organization. RBAC allows management of users and roles, where a role is a collection of permissions to access resources. Users can be assigned to multiple roles and permissions can be reused across multiple roles. Read more about RBAC in the Kubernetes documentation . Task 1 : Create a Service Account A Service Account is an account that is used by applications to interact with the Kubernetes cluster. It is assigned a set of credentials that can be used to authenticate to the cluster. The credentials are stored as a Secret in the Kubernetes API. In this task, you will create a Service Account named app kubectl create serviceaccount app --namespace $NAMESPACE Task 2 : Create a Role A Role is a collection of permissions that can be assigned to a Service Account. In this task, you will create a Role named app that allows the Service Account to read the Pod resource in the current namespace. kubectl create role app --verb = get --verb = list --verb = watch --resource = pods --namespace $NAMESPACE Task 3 : Create a Role Binding A Role Binding is a link between a Role and a Service Account. In this task, you will create a Role Binding that links the Role app to the Service Account app . kubectl create rolebinding app --role = app --serviceaccount = ${ NAMESPACE } :app --namespace $NAMESPACE Task 4 : Verify the Role Binding In this task, you will verify that the Role Binding is created and that the Service Account is linked to the Role. kubectl get rolebinding app --namespace $NAMESPACE Task 5 : Create a Pod In this task, you will create a Pod that uses the Service Account app to access the Kubernetes API. kubectl apply -f - <<EOF apiVersion: v1 kind: Pod metadata: name: app namespace: $NAMESPACE spec: serviceAccountName: app containers: - name: app image: yauritux/busybox-curl command: [\"sh\", \"-c\", \"sleep 3600\"] EOF Task 6 : Verify the Pod In this task, you will verify that the Pod is created and that it is running. kubectl get pod app --namespace $NAMESPACE Task 7 : Access the Kubernetes API In this task, you will access the Kubernetes API from within the Pod and verify that the Service Account has the correct permissions. First we need the Service Account token. # export the whole output to TOKEN export TOKEN = $( kubectl exec app --namespace $NAMESPACE -- cat /var/run/secrets/kubernetes.io/serviceaccount/token ) Then we can curl the Kubernetes API over the pod's network. kubectl exec app --namespace $NAMESPACE -- curl -ks --header \"Authorization: Bearer $TOKEN \" https://kubernetes.default.svc/api/v1/namespaces/ $NAMESPACE /pods This should return a list of Pods in the current namespace in JSON format. Task 8 : Delete the Pod In this task, you will delete the Pod. kubectl delete pod app --namespace $NAMESPACE","title":"RBAC"},{"location":"kubernetes-basics/security/rbac/#rbac","text":"Environment Variables We are going to use some environment variables in this tutorial. Please make sure you have set them correctly. # check if the environment variables are set if not set them export NAMESPACE = <namespace> echo $NAMESPACE Role-based access control (RBAC) is a method of regulating access to computer or network resources based on the roles of individual users within your organization. RBAC allows management of users and roles, where a role is a collection of permissions to access resources. Users can be assigned to multiple roles and permissions can be reused across multiple roles. Read more about RBAC in the Kubernetes documentation .","title":"RBAC"},{"location":"kubernetes-basics/security/rbac/#task-1-create-a-service-account","text":"A Service Account is an account that is used by applications to interact with the Kubernetes cluster. It is assigned a set of credentials that can be used to authenticate to the cluster. The credentials are stored as a Secret in the Kubernetes API. In this task, you will create a Service Account named app kubectl create serviceaccount app --namespace $NAMESPACE","title":" Task 1: Create a Service Account"},{"location":"kubernetes-basics/security/rbac/#task-2-create-a-role","text":"A Role is a collection of permissions that can be assigned to a Service Account. In this task, you will create a Role named app that allows the Service Account to read the Pod resource in the current namespace. kubectl create role app --verb = get --verb = list --verb = watch --resource = pods --namespace $NAMESPACE","title":" Task 2: Create a Role"},{"location":"kubernetes-basics/security/rbac/#task-3-create-a-role-binding","text":"A Role Binding is a link between a Role and a Service Account. In this task, you will create a Role Binding that links the Role app to the Service Account app . kubectl create rolebinding app --role = app --serviceaccount = ${ NAMESPACE } :app --namespace $NAMESPACE","title":" Task 3: Create a Role Binding"},{"location":"kubernetes-basics/security/rbac/#task-4-verify-the-role-binding","text":"In this task, you will verify that the Role Binding is created and that the Service Account is linked to the Role. kubectl get rolebinding app --namespace $NAMESPACE","title":" Task 4: Verify the Role Binding"},{"location":"kubernetes-basics/security/rbac/#task-5-create-a-pod","text":"In this task, you will create a Pod that uses the Service Account app to access the Kubernetes API. kubectl apply -f - <<EOF apiVersion: v1 kind: Pod metadata: name: app namespace: $NAMESPACE spec: serviceAccountName: app containers: - name: app image: yauritux/busybox-curl command: [\"sh\", \"-c\", \"sleep 3600\"] EOF","title":" Task 5: Create a Pod"},{"location":"kubernetes-basics/security/rbac/#task-6-verify-the-pod","text":"In this task, you will verify that the Pod is created and that it is running. kubectl get pod app --namespace $NAMESPACE","title":" Task 6: Verify the Pod"},{"location":"kubernetes-basics/security/rbac/#task-7-access-the-kubernetes-api","text":"In this task, you will access the Kubernetes API from within the Pod and verify that the Service Account has the correct permissions. First we need the Service Account token. # export the whole output to TOKEN export TOKEN = $( kubectl exec app --namespace $NAMESPACE -- cat /var/run/secrets/kubernetes.io/serviceaccount/token ) Then we can curl the Kubernetes API over the pod's network. kubectl exec app --namespace $NAMESPACE -- curl -ks --header \"Authorization: Bearer $TOKEN \" https://kubernetes.default.svc/api/v1/namespaces/ $NAMESPACE /pods This should return a list of Pods in the current namespace in JSON format.","title":" Task 7: Access the Kubernetes API"},{"location":"kubernetes-basics/security/rbac/#task-8-delete-the-pod","text":"In this task, you will delete the Pod. kubectl delete pod app --namespace $NAMESPACE","title":" Task 8: Delete the Pod"},{"location":"kubernetes-basics/security/security-contexts/","text":"Security Contexts Environment Variables We are going to use some environment variables in this tutorial. Please make sure you have set them correctly. # check if the environment variables are set if not set them export NAMESPACE = <namespace> echo $NAMESPACE In the concept of security context for a pod or container, there are severals thing to consider: Access control SElinux Running privileged or unprivileged workload Linux capabilities AppArmor Seccomp In this tutorial you will learn where to configure and how to use some of these types. Task 1 : Access Control Create a new pod access-pod.yaml by using this example: apiVersion : v1 kind : Pod metadata : name : security-context-demo spec : securityContext : runAsUser : 1000 runAsGroup : 3000 fsGroup : 2000 volumes : - name : sec-ctx-vol emptyDir : {} containers : - name : sec-ctx-demo image : busybox:1.28 command : [ \"sh\" , \"-c\" , \"sleep 1h\" ] volumeMounts : - name : sec-ctx-vol mountPath : /data/demo securityContext : allowPrivilegeEscalation : false Apply the file: kubectl apply -f access-pod.yaml --namespace $NAMESPACE You can see the different value entries in the \u2018securityContext\u2019 section, let\u2019s figure how what do they do. So create the pod and connect into the shell: kubectl exec -it security-context-demo --namespace $NAMESPACE -- sh In the container run ps to get a list of all running processes. The output shows, that the processes are running with the user 1000 , which is the value from runAsUser : / $ ps PID USER TIME COMMAND 1 1000 0:00 sleep 1h 6 1000 0:00 sh 12 1000 0:00 ps Now navigate to the directory /data and list the content. As you can see the emptyDir has been mounted with the group ID of 2000 , which is the value of the fsGroup field. /data $ ls -lah total 0 drwxr-xr-x 3 root root 18 Nov 21 13:12 . drwxr-xr-x 1 root root 63 Nov 21 13:12 .. drwxrwsrwx 2 root 2000 6 Nov 21 13:12 demo Go into the dir demo and create a file: /data $ cd demo/ /data/demo $ echo hello > demofile /data/demo $ ls -lah total 4 drwxrwsrwx 2 root 2000 22 Nov 21 13:15 . drwxr-xr-x 3 root root 18 Nov 21 13:12 .. -rw-r--r-- 1 1000 2000 6 Nov 21 13:15 demofile List the content with ls -lah again and see, that demofile has the group ID 2000 , which is the value fsGroup as well. Run the last command id here and check the output: /data/demo $ id uid=1000 gid=3000 groups=2000 The shown group ID of the user is 3000 , from the field runAsGroup . If the field would be empty the user would have 0 (root) and every process would be able to go with files which are owned by the root (0) group. /data/demo $ exit Check out the documentation at kubernetes.io for more information about Security Context .","title":"Security Contexts"},{"location":"kubernetes-basics/security/security-contexts/#security-contexts","text":"Environment Variables We are going to use some environment variables in this tutorial. Please make sure you have set them correctly. # check if the environment variables are set if not set them export NAMESPACE = <namespace> echo $NAMESPACE In the concept of security context for a pod or container, there are severals thing to consider: Access control SElinux Running privileged or unprivileged workload Linux capabilities AppArmor Seccomp In this tutorial you will learn where to configure and how to use some of these types.","title":"Security Contexts"},{"location":"kubernetes-basics/security/security-contexts/#task-1-access-control","text":"Create a new pod access-pod.yaml by using this example: apiVersion : v1 kind : Pod metadata : name : security-context-demo spec : securityContext : runAsUser : 1000 runAsGroup : 3000 fsGroup : 2000 volumes : - name : sec-ctx-vol emptyDir : {} containers : - name : sec-ctx-demo image : busybox:1.28 command : [ \"sh\" , \"-c\" , \"sleep 1h\" ] volumeMounts : - name : sec-ctx-vol mountPath : /data/demo securityContext : allowPrivilegeEscalation : false Apply the file: kubectl apply -f access-pod.yaml --namespace $NAMESPACE You can see the different value entries in the \u2018securityContext\u2019 section, let\u2019s figure how what do they do. So create the pod and connect into the shell: kubectl exec -it security-context-demo --namespace $NAMESPACE -- sh In the container run ps to get a list of all running processes. The output shows, that the processes are running with the user 1000 , which is the value from runAsUser : / $ ps PID USER TIME COMMAND 1 1000 0:00 sleep 1h 6 1000 0:00 sh 12 1000 0:00 ps Now navigate to the directory /data and list the content. As you can see the emptyDir has been mounted with the group ID of 2000 , which is the value of the fsGroup field. /data $ ls -lah total 0 drwxr-xr-x 3 root root 18 Nov 21 13:12 . drwxr-xr-x 1 root root 63 Nov 21 13:12 .. drwxrwsrwx 2 root 2000 6 Nov 21 13:12 demo Go into the dir demo and create a file: /data $ cd demo/ /data/demo $ echo hello > demofile /data/demo $ ls -lah total 4 drwxrwsrwx 2 root 2000 22 Nov 21 13:15 . drwxr-xr-x 3 root root 18 Nov 21 13:12 .. -rw-r--r-- 1 1000 2000 6 Nov 21 13:15 demofile List the content with ls -lah again and see, that demofile has the group ID 2000 , which is the value fsGroup as well. Run the last command id here and check the output: /data/demo $ id uid=1000 gid=3000 groups=2000 The shown group ID of the user is 3000 , from the field runAsGroup . If the field would be empty the user would have 0 (root) and every process would be able to go with files which are owned by the root (0) group. /data/demo $ exit Check out the documentation at kubernetes.io for more information about Security Context .","title":" Task 1: Access Control"},{"location":"materials/","text":"Materials Some of the materials used in the course are available here. kubectl cheat sheet See all useful commands for kubectl in one place. Get it presentation The presentation used in the course. Get it Stepping Stone Wiki Head over to the Stepping Stone Wiki to find more information about the cluster setup. Take me Kubernetes Documentation The official Kubernetes documentation is a great resource to learn more about Kubernetes. Take me Kubernetes API Reference A reference for the Kubernetes API. Show me Troubleshooting Kubernetes Deployments A guide to troubleshoot Kubernetes deployments. Show me Official Kubectl Cheat Sheet","title":"Materials"},{"location":"materials/#materials","text":"Some of the materials used in the course are available here. kubectl cheat sheet See all useful commands for kubectl in one place. Get it presentation The presentation used in the course. Get it Stepping Stone Wiki Head over to the Stepping Stone Wiki to find more information about the cluster setup. Take me Kubernetes Documentation The official Kubernetes documentation is a great resource to learn more about Kubernetes. Take me Kubernetes API Reference A reference for the Kubernetes API. Show me Troubleshooting Kubernetes Deployments A guide to troubleshoot Kubernetes deployments. Show me Official Kubectl Cheat Sheet","title":"Materials"},{"location":"materials/credits/","text":"Credits Several parts of this guide are based on the course of Acend. We would like to shout out to them for their great work. You can find the original course here: Kubernetes Basics Training Also multiple parts of this guide are based on the course of the Kubernetes Documentation . License This guide is licensed under the Creative Commons Attribution-ShareAlike 4.0 International License .","title":"Credits"},{"location":"materials/credits/#credits","text":"Several parts of this guide are based on the course of Acend. We would like to shout out to them for their great work. You can find the original course here: Kubernetes Basics Training Also multiple parts of this guide are based on the course of the Kubernetes Documentation .","title":"Credits"},{"location":"materials/credits/#license","text":"This guide is licensed under the Creative Commons Attribution-ShareAlike 4.0 International License .","title":"License"},{"location":"materials/gitlab/","text":"Gitlab","title":"Gitlab"},{"location":"materials/gitlab/#gitlab","text":"","title":"Gitlab"},{"location":"monitoring/","text":"Monitoring Monitoring is a key part of any production system. It allows you to keep track of the health of your system and to identify problems before they become critical. Kubernetes provides a number of tools that can help you monitor your system. You can also checkout the Client Tools page for a list of local tools, which can help you monitor your cluster. Kubernetes Dashboard Kubernetes Dashboard The Kubernetes Dashboard is a web-based Kubernetes user interface. You can use the Kubernetes Dashboard to deploy containerized applications to a Kubernetes cluster, troubleshoot your containerized application, and manage the cluster resources. You can use the Kubernetes Dashboard to get an overview of applications running on your cluster, as well as for creating or modifying individual Kubernetes resources (such as Deployments, Jobs, DaemonSets, etc). For example, you can scale a Deployment, initiate a rolling update, restart a pod or deploy new applications using a deploy wizard. The Dashboard also provides information on the state of Kubernetes resources in your cluster and on any errors that may have occurred. Metrics Server The Kubernetes Dashboard UI (in particular, the graphs on the Cluster page) displays resource usage data (CPU and memory) of your cluster's nodes. The resource usage data is retrieved by the Kubernetes Dashboard from the Metrics Server. The Metrics Server is not deployed by default in Kubernetes. See the Metrics Server documentation to learn how to deploy the Metrics Server. You have to redeploy the Kubernetes Dashboard with additional permissions to allow it to access the Metrics Server. Kube Prometheus Stack The kube-prometheus-stack is a collection of community-curated Helm charts that deploy the core components of the Prometheus monitoring system. The kube-prometheus-stack Helm chart deploys the following components: Prometheus Operator Prometheus Alertmanager Prometheus Node Exporter kube-state-metrics Grafana Installation To install the kube-prometheus-stack Helm chart, run the following command: helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update kubectl create namespace monitoring helm install monitoring prometheus-community/kube-prometheus-stack -n monitoring kubectl get pods -n monitoring -w Have a look at the values.yaml file to see all the configuration options. The default configuration should be fine for most use cases, but you can add ingress rules to access the services from outside the cluster. Accessing the Grafana Dashboard To access the Grafana dashboard, run the following command: kubectl port-forward svc/monitoring-grafana 3000 :80 -n monitoring Then, open the following URL in your browser: http://localhost:3000 Search for the monitoring-grafana secret and base64 decode the admin-user and admin-password values. kubectl get secret monitoring-grafana -n monitoring -o jsonpath = \"{.data.admin-user}\" | base64 --decode ; echo kubectl get secret monitoring-grafana -n monitoring -o jsonpath = \"{.data.admin-password}\" | base64 --decode ; echo Note The default username is admin and the default password is prom-operator . Accessing the Prometheus Dashboard To access the Prometheus dashboard, run the following command: kubectl port-forward svc/monitoring-kube-prometheus-prometheus 9090 :9090 -n monitoring Then, open the following URL in your browser: http://localhost:9090 Loki Loki Loki is a horizontally-scalable, highly-available, multi-tenant log aggregation system inspired by Prometheus. It is designed to be very cost effective and easy to operate. It does not index the contents of the logs, but rather a set of labels for each log stream.","title":"Monitoring"},{"location":"monitoring/#monitoring","text":"Monitoring is a key part of any production system. It allows you to keep track of the health of your system and to identify problems before they become critical. Kubernetes provides a number of tools that can help you monitor your system. You can also checkout the Client Tools page for a list of local tools, which can help you monitor your cluster.","title":"Monitoring"},{"location":"monitoring/#kubernetes-dashboard","text":"Kubernetes Dashboard The Kubernetes Dashboard is a web-based Kubernetes user interface. You can use the Kubernetes Dashboard to deploy containerized applications to a Kubernetes cluster, troubleshoot your containerized application, and manage the cluster resources. You can use the Kubernetes Dashboard to get an overview of applications running on your cluster, as well as for creating or modifying individual Kubernetes resources (such as Deployments, Jobs, DaemonSets, etc). For example, you can scale a Deployment, initiate a rolling update, restart a pod or deploy new applications using a deploy wizard. The Dashboard also provides information on the state of Kubernetes resources in your cluster and on any errors that may have occurred.","title":"Kubernetes Dashboard"},{"location":"monitoring/#metrics-server","text":"The Kubernetes Dashboard UI (in particular, the graphs on the Cluster page) displays resource usage data (CPU and memory) of your cluster's nodes. The resource usage data is retrieved by the Kubernetes Dashboard from the Metrics Server. The Metrics Server is not deployed by default in Kubernetes. See the Metrics Server documentation to learn how to deploy the Metrics Server. You have to redeploy the Kubernetes Dashboard with additional permissions to allow it to access the Metrics Server.","title":"Metrics Server"},{"location":"monitoring/#kube-prometheus-stack","text":"The kube-prometheus-stack is a collection of community-curated Helm charts that deploy the core components of the Prometheus monitoring system. The kube-prometheus-stack Helm chart deploys the following components: Prometheus Operator Prometheus Alertmanager Prometheus Node Exporter kube-state-metrics Grafana","title":"Kube Prometheus Stack"},{"location":"monitoring/#installation","text":"To install the kube-prometheus-stack Helm chart, run the following command: helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update kubectl create namespace monitoring helm install monitoring prometheus-community/kube-prometheus-stack -n monitoring kubectl get pods -n monitoring -w Have a look at the values.yaml file to see all the configuration options. The default configuration should be fine for most use cases, but you can add ingress rules to access the services from outside the cluster.","title":"Installation"},{"location":"monitoring/#accessing-the-grafana-dashboard","text":"To access the Grafana dashboard, run the following command: kubectl port-forward svc/monitoring-grafana 3000 :80 -n monitoring Then, open the following URL in your browser: http://localhost:3000 Search for the monitoring-grafana secret and base64 decode the admin-user and admin-password values. kubectl get secret monitoring-grafana -n monitoring -o jsonpath = \"{.data.admin-user}\" | base64 --decode ; echo kubectl get secret monitoring-grafana -n monitoring -o jsonpath = \"{.data.admin-password}\" | base64 --decode ; echo Note The default username is admin and the default password is prom-operator .","title":"Accessing the Grafana Dashboard"},{"location":"monitoring/#accessing-the-prometheus-dashboard","text":"To access the Prometheus dashboard, run the following command: kubectl port-forward svc/monitoring-kube-prometheus-prometheus 9090 :9090 -n monitoring Then, open the following URL in your browser: http://localhost:9090","title":"Accessing the Prometheus Dashboard"},{"location":"monitoring/#loki","text":"Loki Loki is a horizontally-scalable, highly-available, multi-tenant log aggregation system inspired by Prometheus. It is designed to be very cost effective and easy to operate. It does not index the contents of the logs, but rather a set of labels for each log stream.","title":"Loki"},{"location":"setup/client-setup/","text":"Client Setup Command Line The kubectl command is the primary command line tool for interacting with the Kubernetes API. Before you can use kubectl , you need to install it. It is available for Linux, macOS, and Windows. Official documentation: https://kubernetes.io/docs/tasks/tools/install-kubectl/","title":"Client Setup"},{"location":"setup/client-setup/#client-setup","text":"","title":"Client Setup"},{"location":"setup/client-setup/#command-line","text":"The kubectl command is the primary command line tool for interacting with the Kubernetes API. Before you can use kubectl , you need to install it. It is available for Linux, macOS, and Windows. Official documentation: https://kubernetes.io/docs/tasks/tools/install-kubectl/","title":"Command Line"},{"location":"setup/client-setup/linux/","text":"Linux If installing from the official package repository doesn't work (or requires a specific version), you can download the static binaries and place them in the following paths: ~/bin File mode The binary kubectl has to be executable. chmod +x ~/bin/kubectl Add to PATH On Linux, you can add the ~/bin directory to your PATH environment variable to make the kubectl command available to all users on the system. If kubectl is placed in a different directory, you can change the path to that directory. export PATH = $PATH :~/bin Autocomplete On most Linux distributions, you have to install the bash-completion package to enable autocompletion. Debian/Ubuntu sudo apt-get install bash-completion CentOS/RHEL sudo yum install bash-completion Fedora sudo dnf install bash-completion Bash To enable bash autocompletion, add the following to your ~/.bashrc file: source < ( kubectl completion bash ) Zsh To enable zsh autocompletion, add the following to your ~/.zshrc file: source < ( kubectl completion zsh ) Verify Verify that the kubectl command is available at the Verify page.","title":"Linux"},{"location":"setup/client-setup/linux/#linux","text":"If installing from the official package repository doesn't work (or requires a specific version), you can download the static binaries and place them in the following paths: ~/bin","title":"Linux"},{"location":"setup/client-setup/linux/#file-mode","text":"The binary kubectl has to be executable. chmod +x ~/bin/kubectl","title":"File mode"},{"location":"setup/client-setup/linux/#add-to-path","text":"On Linux, you can add the ~/bin directory to your PATH environment variable to make the kubectl command available to all users on the system. If kubectl is placed in a different directory, you can change the path to that directory. export PATH = $PATH :~/bin","title":"Add to PATH"},{"location":"setup/client-setup/linux/#autocomplete","text":"On most Linux distributions, you have to install the bash-completion package to enable autocompletion. Debian/Ubuntu sudo apt-get install bash-completion CentOS/RHEL sudo yum install bash-completion Fedora sudo dnf install bash-completion","title":"Autocomplete"},{"location":"setup/client-setup/linux/#bash","text":"To enable bash autocompletion, add the following to your ~/.bashrc file: source < ( kubectl completion bash )","title":"Bash"},{"location":"setup/client-setup/linux/#zsh","text":"To enable zsh autocompletion, add the following to your ~/.zshrc file: source < ( kubectl completion zsh )","title":"Zsh"},{"location":"setup/client-setup/linux/#verify","text":"Verify that the kubectl command is available at the Verify page.","title":"Verify"},{"location":"setup/client-setup/macos/","text":"Mac OS If installing from the official package repository doesn't work (or requires a specific version), you can download the static binaries and place them in the following paths: ~/bin File mode The binary kubectl has to be executable. chmod +x ~/bin/kubectl Add to PATH On Linux, you can add the ~/bin directory to your PATH environment variable to make the kubectl command available to all users on the system. If kubectl is placed in a different directory, you can change the path to that directory. export PATH = $PATH :~/bin Homebrew If you are using Homebrew , you can install kubectl with the following command: brew install kubectl Autocomplete Zsh To enable zsh autocompletion, add the following to your ~/.zshrc file: source < ( kubectl completion zsh ) Verify Verify that the kubectl command is available at the Verify page.","title":"Mac OS"},{"location":"setup/client-setup/macos/#mac-os","text":"If installing from the official package repository doesn't work (or requires a specific version), you can download the static binaries and place them in the following paths: ~/bin","title":"Mac OS"},{"location":"setup/client-setup/macos/#file-mode","text":"The binary kubectl has to be executable. chmod +x ~/bin/kubectl","title":"File mode"},{"location":"setup/client-setup/macos/#add-to-path","text":"On Linux, you can add the ~/bin directory to your PATH environment variable to make the kubectl command available to all users on the system. If kubectl is placed in a different directory, you can change the path to that directory. export PATH = $PATH :~/bin","title":"Add to PATH"},{"location":"setup/client-setup/macos/#homebrew","text":"If you are using Homebrew , you can install kubectl with the following command: brew install kubectl","title":"Homebrew"},{"location":"setup/client-setup/macos/#autocomplete","text":"","title":"Autocomplete"},{"location":"setup/client-setup/macos/#zsh","text":"To enable zsh autocompletion, add the following to your ~/.zshrc file: source < ( kubectl completion zsh )","title":"Zsh"},{"location":"setup/client-setup/macos/#verify","text":"Verify that the kubectl command is available at the Verify page.","title":"Verify"},{"location":"setup/client-setup/verify/","text":"Verify Verify that the kubectl command is available by running the following command: kubectl version --client Make sure that the version of the client is the same or higher than the version of the cluster. Client Version: version.Info { Major: \"1\" , Minor: \"22\" , GitVersion: \"v1.22.0\" , GitCommit: \"cde122dc4477e5e9c5f8833d2fb01c8807a0a2b1\" , GitTreeState: \"clean\" , BuildDate: \"2021-06-17T20:20:38Z\" , GoVersion: \"go1.16.5\" , Compiler: \"gc\" , Platform: \"linux/amd64\" } Tools You can have a look at the Tools page to see what tools are available to help you manage your Kubernetes cluster. Next steps The kubectl has many commands and options. Check them out with kubectl --help or kubectl <command> --help . Now that you have installed kubectl , you can continue with the Kubernetes Basics tutorial.","title":"Verify"},{"location":"setup/client-setup/verify/#verify","text":"Verify that the kubectl command is available by running the following command: kubectl version --client Make sure that the version of the client is the same or higher than the version of the cluster. Client Version: version.Info { Major: \"1\" , Minor: \"22\" , GitVersion: \"v1.22.0\" , GitCommit: \"cde122dc4477e5e9c5f8833d2fb01c8807a0a2b1\" , GitTreeState: \"clean\" , BuildDate: \"2021-06-17T20:20:38Z\" , GoVersion: \"go1.16.5\" , Compiler: \"gc\" , Platform: \"linux/amd64\" }","title":"Verify"},{"location":"setup/client-setup/verify/#tools","text":"You can have a look at the Tools page to see what tools are available to help you manage your Kubernetes cluster.","title":"Tools"},{"location":"setup/client-setup/verify/#next-steps","text":"The kubectl has many commands and options. Check them out with kubectl --help or kubectl <command> --help . Now that you have installed kubectl , you can continue with the Kubernetes Basics tutorial.","title":"Next steps"},{"location":"setup/client-setup/windows/","text":"Windows If installing from the official package repository doesn't work (or requires a specific version), you can download the static binaries and place them in the following paths: C :\\ Kubernetes \\ In Windows, the PATH environment variable can be used to add the kubectl command to the system path. Windows 10 Windows 11 Quickstart for Windows Copy the kubectl binary directly to the C:\\Windows directory. Verify Verify that the kubectl command is available at the Verify page.","title":"Windows"},{"location":"setup/client-setup/windows/#windows","text":"If installing from the official package repository doesn't work (or requires a specific version), you can download the static binaries and place them in the following paths: C :\\ Kubernetes \\ In Windows, the PATH environment variable can be used to add the kubectl command to the system path. Windows 10 Windows 11 Quickstart for Windows Copy the kubectl binary directly to the C:\\Windows directory.","title":"Windows"},{"location":"setup/client-setup/windows/#verify","text":"Verify that the kubectl command is available at the Verify page.","title":"Verify"},{"location":"setup/tools/","text":"Tools There are several tools that can help you administer your Kubernetes cluster. This section lists some of the most common tools. pv-migrate pv-migrate is a tool that can help you migrate your persistent volumes from one persistent volume claim to another. kubectx kubectx is a tool that can help you switch between clusters and namespaces. OpenLens OpenLens is a tool that can help you manage your Kubernetes clusters with a graphical user interface. minikube To install minikube you need docker or podman installed on your machine. minikube is a tool that can help you run a single-node Kubernetes cluster locally. GitBash GitBash is a tool that can help you run bash commands on Windows. Kubeshark Kubeshark provides deep visibility and monitoring of all API traffic and payloads going in, out and across containers and pods inside a Kubernetes cluster.","title":"Tools"},{"location":"setup/tools/#tools","text":"There are several tools that can help you administer your Kubernetes cluster. This section lists some of the most common tools.","title":"Tools"},{"location":"setup/tools/#pv-migrate","text":"pv-migrate is a tool that can help you migrate your persistent volumes from one persistent volume claim to another.","title":"pv-migrate"},{"location":"setup/tools/#kubectx","text":"kubectx is a tool that can help you switch between clusters and namespaces.","title":"kubectx"},{"location":"setup/tools/#openlens","text":"OpenLens is a tool that can help you manage your Kubernetes clusters with a graphical user interface.","title":"OpenLens"},{"location":"setup/tools/#minikube","text":"To install minikube you need docker or podman installed on your machine. minikube is a tool that can help you run a single-node Kubernetes cluster locally.","title":"minikube"},{"location":"setup/tools/#gitbash","text":"GitBash is a tool that can help you run bash commands on Windows.","title":"GitBash"},{"location":"setup/tools/#kubeshark","text":"Kubeshark provides deep visibility and monitoring of all API traffic and payloads going in, out and across containers and pods inside a Kubernetes cluster.","title":"Kubeshark"}]}